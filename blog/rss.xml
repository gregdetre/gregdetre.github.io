<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Greg Detre</title>
    <link>https://www.gregdetre.com/blog/rss.xml</link>
    <description>Greg Detre's personal website</description>
    <atom:link href="https://www.gregdetre.com/blog/rss.xml" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>en</language>
    <lastBuildDate>Sun, 17 Aug 2025 16:10:44 +0300</lastBuildDate>
    <item>
      <title>Advice to a younger self</title>
      <link>https://www.gregdetre.com/blog/advice-to-a-younger-self/</link>
      <description>**Honestly doing the exercise is far more interesting than reading other people's answers.** What would you tell your younger self about procrastination, balance, and learning?</description>
      <content:encoded><![CDATA[<h3 id="think-back-to-what-you-were-doing-5-10-15-years-ago-really-visualise-it-what-advice-would-you-give-your-younger-self-is-it-consistent-honestly-doing-the-exercise-is-far-more-interesting-than-reading-other-peoples-answers">"Think back to what you were doing 5, 10, 15+ years ago. Really visualise it. What advice would you give your younger self? Is it consistent? Honestly doing the exercise is far more interesting than reading other people’s answers."</h3>
<p><em>(<a href="https://twitter.com/naval/status/914189738453622784">from Naval Ravikant</a>)</em></p>
<ul>
<li>
<p>Drink more water when you go out boozing.</p>
</li>
<li>
<p>Procrastination is an unwillingness to sit with your own discomfort, and willpower is a mood/energy state.</p>
</li>
<li>
<p>Your environment and home situation both affect how well you recharge. Invest in them.</p>
</li>
<li>
<p>Improve your posture and tense your core to stay awake when sleepy in a lecture.</p>
</li>
<li>
<p>You feel way better and more energised when you exercise more and feel part of a community.</p>
</li>
<li>
<p>Learn as much maths and technical stuff as you can when you’re young. Don’t just read textbooks and audit the classes - you only learn if you do the exercises.</p>
</li>
<li>
<p>You are not invincible. You need balance in your life to be able to continue to work well and stay happy.</p>
</li>
<li>
<p>Switch the channel on your inner monologue when it’s not serving you.</p>
</li>
<li>
<p>Regular meditation gives you just a little more chance to notice your inner monologue, and to choose what you say and how you respond in the moment.</p>
</li>
<li>
<p>It’s better to be happy and effective than right. You'll never change someone's mind if they're feeling angry or threatened.</p>
</li>
<li>
<p>Practice saying the hard things earlier.</p>
</li>
</ul>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/advice-to-a-younger-self/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>Fahrenheit 9-11</title>
      <link>https://www.gregdetre.com/blog/fahrenheit-911/</link>
      <description>Moore's *Fahrenheit 9/11* could swing the election, but raises questions about whether **subjective journalism** succeeds where objective reporting has failed to expose uncomfortable truths.</description>
      <content:encoded><![CDATA[<p>I went to see this and thought it was great. I wandered out wondering how anyone could vote Bush after having seen it. It seems quite possible that if enough people see this, it could swing the election. But Moore's journalism is regrettably far from impeccable.  </p>
<p><a href="http://www2.blogger.com/%20http://www.moorewatch.com/">http://www.moorewatch.com/</a> </p>
<p>Having said that, the Left needs someone vocal, persuasive and with wide reach. I'm not sure that I agree with this quote, but I am starting to agree that what objective journalism there is has failed to get the message across.  </p>
<p>"Some people will say that words like scum and rotten are wrong for Objective Journalism - which is true, but they miss the point," wrote Thompson. "It was the built-in blind spots of the Objective rules and dogma that allowed Nixon to slither into the White House in the first place. He looked so good on paper that you could almost vote for him sight unseen. He seemed so all-American, so much like Horatio Alger, that he was able to slip through the cracks of Objective Journalism. You had to get Subjective to see Nixon clearly, and the shock of recognition was often painful."  </p>
<p><a href="http://www.google.com/search?hl=en&amp;lr=&amp;amp;amp;ie=UTF-8&amp;q=michael+moore+journalism">Googling for Michael Moore</a> </p>
<p><a href="http://www.jessicaswell.com/Life-Page01.htm">[broken link]</a></p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/fahrenheit-911/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>File-sharing</title>
      <link>https://www.gregdetre.com/blog/file-sharing/</link>
      <description>**File-sharing is stealing, plain and simple** — masked by anonymity, mass participation, and spurious justifications that ignore the industry's survival needs.</description>
      <content:encoded><![CDATA[<p>file-sharing is stealing, plain and simple, authenticised, masked, justified and guilt-free, authenticised by its seeming anonymity, the fact that genuinely millions of people are doing, the bullshit myth that the recording companies are somehow evil, the refusal to consider that the industry cannot survive if no one pays for music, spurious arguments about borrowing your neighbour's tools, and because it has a nice windows interface  </p>
<p>Plato was clearly wrong when he said that no man would willingly do evil</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/file-sharing/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>Metaphors for depression, clean language, and the DSM</title>
      <link>https://www.gregdetre.com/blog/metaphors-for-depression-clean-language-and-the-dsm/</link>
      <description>Could we diagnose depression by the metaphors people use to describe it? *Black veil*, *gravity*, *suffocation* - each might reveal different underlying patterns requiring different treatments.</description>
      <content:encoded><![CDATA[<div class="toc">
<ul>
<li><a href="#postscript-a-testable-hypothesis-treatment-idea">Postscript: a testable hypothesis &amp; treatment idea</a></li>
</ul>
</div>
<p>I've used a lot of metaphors for depression to myself, to help me understand what I'm feeling. And to remember later how it was - perhaps to exorcise it, to capture it, and to study it. And because I often want to write when I'm depressed, and the depression crowds out almost everything else.</p>
<p>It's a black veil, making it hard to take in a deep breath of clean air, masking all the colour, making it hard to even imagine colour.</p>
<p>It's a friction, a downwards. I feel like an ungainly bird trying to lift off, flailing and thrashing hard to heave off against the heaviness. I look at the others, aloft, buoyed by thermals, only needing a lazy flap here or there. They can't imagine how much more work it is to lift off than stay up.</p>
<p>When I'm still functioning in the face of the depression, I feel like I'm flying inches above the treetops. It takes only a slight drop in the wind and then I'm being snagged by a stray branch, stumbling and wheeling awkwardly, feeling the darkness of the forest yawning and gaping underneath me.</p>
<p>The physics of it makes me feel like a rocket yearning for escape velocity. On the ground the gravity is strongest, and from a standing start it's going to take a truly concerted burst of effort to make any headway - even a gigantic plume of flame barely seems enough to shrug off the inertia. But eventually, gradually, there is motion. But is it enough? Are you going to run out of go before you run out of slow? It feels like there's so so much more stop to start with. And every time you stall, defeat surrounds and clothes and clings to you, like raindrops in a drizzle.</p>
<p>But there are moments during the ascent that feel like suddenly bursting through the clouds, when thrust exceeds gravity and it feels like the acceleration could be endless. Smiles and good fortune are in arm's reach in every direction, like reassuring rounded pebbles on a British beach. Treasure and relive those moments, and they will help sustain you through the difficult times.</p>
<p>I suppose the message from the physics of a rocket is to keep on afterburnering upwards as fast as you can, that a small constant effort is never going to be enough to achieve escape velocity. This is why the NHS NICE guidelines suggest that you combine both intensive CBT <em>and</em> antidepressants for more serious or long-lasting depression. You need as much acceleration as soon as possible. And my hunch is that the different components have different timecourses, e.g. exercise gives you a quick boost, medication takes a few weeks to really kick in, and the cognitive interventions probably have the longest-term effects.</p>
<p>Suffocation and colour and friction and gravity. Intensity and activation and expansion.</p>
<h2 id="postscript-a-testable-hypothesis-treatment-idea">Postscript: a testable hypothesis &amp; treatment idea</h2>
<p>One further thought.</p>
<p>Of course, there are other metaphors that people have used. Could there be something to the different kinds of metaphors that people use?</p>
<p>After all, we try to characterise individual symptoms as precisely as we can, and to pay attention to how they cluster, as our means of individuating and categorising diseases. Could metaphor be a way to characterise the phenomenological symptoms, as a clue to what's underlying them?</p>
<p>There's evidence to suggest that depression isn't a unitary condition. This might explain why different people respond differently to medication.</p>
<p>Could we use something like <a href="https://en.wikipedia.org/wiki/Clean_Language">clean language</a> to gather people's metaphors for their depression, characterise the kinds of metaphors used, and then attempt a systematisation and diagnosis of depression by metaphor?</p>
<p>Or better still, try and map the system of metaphors to the patterns we see from our more traditional evidence-gathering, with brain imaging, response to medication, in symptoms, etc? <em>"Ah, heavy-veil, with family history, but no mania - try Sertraline until your sleep symptoms improve."</em> Oh, you're getting <em>"Black-dog with occasional mania and increased schmorbito-florbital activity - stop ruminating, and eat fewer hamburgers."</em>.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/metaphors-for-depression-clean-language-and-the-dsm/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>Get your users to tell you what they really want</title>
      <link>https://www.gregdetre.com/blog/get-your-users-to-tell-you-what-they-really-want/</link>
      <description>A simple feedback box at the end of learning sessions generated **5 times more user suggestions** than a prominent red UserVoice button that users had learned to ignore.</description>
      <content:encoded><![CDATA[<p>We're building <a href="http://www.memrise.com/">a site that we want people to love using</a>, so we want as much feedback on it as we can get. From the get-go, we've had a big red <a href="http://www.uservoice.com/">UserVoice 'Feedback'</a> button on the lefthand site of every page. But only a tiny proportion of our users ever suggested, voted or commented on an idea.</p>
<p><img alt="" src="/img/blog/uservoice_box.png"/></p>
<ul>
<li>
<p>The red Feedback button was ubiquitous, but I bet that made it effectively invisible. Our feedback box is a straightforward text box that gets presented opportunely and prominently at the end of every learning session, at a moment when people will be most likely to have something they want to tell us.  </p>
</li>
<li>
<p>We get an email every time someone drops in a suggestion, with the user's email address, so we can respond quickly to them individually. We very much want to follow up with people that have made the effort to give us feedback.  </p>
</li>
</ul>
<p>I'd say we're getting at least 5 times as many suggestions as we were.  </p>
<p>On the downside, we don't have a nice communal forum any more that allows people to vote or comment on one another's ideas - we might do something about that in the future.</p>
<p>Here's what I wish UserVoice would do:  </p>
<ul>
<li>
<p>Make it much easier for people to suggest new ideas. Even if it were to bring down the overall quality, I think an increased volume of raw responses would be of greater value.  </p>
</li>
<li>
<p>Charge straightforwardly based on the number of new suggestions. This would set up the right incentives for UserVoice to make it really easy for users to make new suggestions. N.B. I don't have a problem with paying - I just don't want to be forced to start out with a $100/mo plan when we have a tiny userbase in order to get features that I consider essential to the user experience.</p>
</li>
<li>
<p>Corollary: Don't withhold important features like single sign-on and white labeling the design for the exorbitant options.  </p>
</li>
</ul>
<p>[1] We compared UserVoice and GetSatisfaction pretty closely, and they both hike the prices if you want to be able to transfer login status across. In fact, GetSatisfaction didn't (at least at the time) allow anonymous suggestions, which felt like a huge barrier to entry to new submissions - this was what convinced me to try UserVoice in the first place.</p>
<p>[2] Hmm - perhaps this <a href="http://dustincurtis.com/you_should_follow_me_on_twitter.htm">should be a command</a>, e.g. 'Tell us what we can do to improve things'?</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/get-your-users-to-tell-you-what-they-really-want/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>A letter to a prospective grad student</title>
      <link>https://www.gregdetre.com/blog/a-letter-to-a-prospective-grad-student/</link>
      <description>A PhD can be wonderful when you're excited about it, but **don't expect the idiosyncrasies of my experience to apply closely to anyone else**.</description>
      <content:encoded><![CDATA[<p>Preface: I wrote this to a friend asking me for advice about whether to embark on a science PhD. At the time of writing I still had more than a year to go - so I could see the summit in the distance, but I was feeling grim about the steep icewall I had to climb to get there.</p>
<p>In retrospect, I think a lot about Jeff Bezos' advice: don't be proud of your talents - be proud of the things you really worked hard to achieve. For this reason, I'm more proud of (and glad about) my PhD than anything else I've yet done.</p>
<p>----</p>
<p>It's hard for me to summarize my thoughts on grad school, perhaps because it varied so much in so many ways. Grad school was wonderful when I was excited about it - for the first few years, there was literally nothing I wanted to do more than talk and think and write and program lab stuff. Every week was filled with new ideas, a sense of progress and discovery, and I bounded into the lab every morning.</p>
<p>I don't know what changed exactly, but at some point, I started to really lose enthusiasm. I'm perenially stymied by an inability to understand the source of my own motivations, and to make sense of my own emotions. So I don't really feel like I understand why the joy started to fade. Perhaps because I worked for years on ambitious experiments that didn't work out. Because I'd been in one place for years. Because I'm a little flighty. Because I thrive in a more competitive or fast-moving jobs. Because really I love AI and computers a little more than brains. Because I wanted to be my own boss. Because I lost confidence. Because I need to feel part of a team working towards a common goal. Because I needed more inter-personal contact with a range of different people. Because I'm not temperamentally suited to be a scientist. Because I need to be in a city. Because I felt obliged to finish it, after investing so much into it, long after I would have left a normal job. Because the specialization necessary can come to seem like a straitjacket. Because I got obsessed with new ideas. I don't know.</p>
<p>It seems to me that a PhD is the right move if one loves what one's doing, and one wants to be an academic. Of course, you can't know for sure in advance that both of those are true. But if you think they might be, then go for it! While I think we have some things in common, I don't expect the idiosyncracies of my experiences to apply closely to anyone else, so don't look too closely for parallels to yourself in my issues above.</p>
<p>Right now, starting a company feels like the job I've been looking for my whole life, but I wouldn't have the wherewithal to do it unless I'd been through the last few years.</p>
<p>I don't know where your path will lead. Like me, I think you get excited about a lot of things, and could happily set off in many different directions, including becoming a great and happy scientist.</p>
<p>This email doesn't really answer any of your questions. I'm sorry about that - I just don't want to give advice one way or the other, because I think you'll make the right choices without my advice, and because you'll make whatever choices you make into the right ones. You are a lucky guy, in this (technical) sense - http://gregdetre.blogspot.com/2009/10/i-dont-believe-in-luck.html</p>
<p>:)</p>
<p>Keep me posted.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/a-letter-to-a-prospective-grad-student/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>Philosophy as debugging</title>
      <link>https://www.gregdetre.com/blog/philosophy-as-debugging/</link>
      <description>Philosophical argument resembles debugging code: you modularize problems, test cases, and hunt for the source of error—often finding wrongful initialization or overlooked implications.</description>
      <content:encoded><![CDATA[<p>Philosophical argument is kind of like debugging a program. You try and zero in on the source of the error, which is why you try and modularise the argument, provide test cases, see where you agree and disagree, and often it comes down to wrongful initialisation, a step accidentally added or omitted, or a failure to see the implications of some interaction you'd never fully considered.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/philosophy-as-debugging/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>Paul Graham, Joel Spolsky and Steve Yegge and the Law of Increasing Returns</title>
      <link>https://www.gregdetre.com/blog/paul-graham-joel-spolsky-and-steve-yegge-and-the-law-of-increasing-returns/</link>
      <description>Three influential tech writers share a crucial insight: the **Law of Increasing Returns** means truly smart people, powerful tools, and quality environments deliver exponentially greater value than their mediocre counterparts.</description>
      <content:encoded><![CDATA[<p>I've read almost everything these three guys (<a href="http://www.paulgraham.com/">PG</a>, <a href="http://www.joelonsoftware.com/">JS</a>, <a href="http://steve-yegge.blogspot.com/">SY</a>) have written. I think it's because I get an unshakable feeling of <em>rightness</em> and convergence when I read their stuff that I've been trying to pin down. Some fairly obvious commonalities between them include:</p>
<ul>
<li>
<p>They have <a href="http://www.imdb.com/title/tt0371724/quotes">brains the size of planets</a>, write really well and have <a href="http://www.cabochon.com/%7Estevey/blog-rants/blog-ancient-perl.html">strong views about programming languages</a>.</p>
</li>
<li>
<p>They feel similarly about <a href="http://gregdetre.blogspot.com/2006/04/procrastination.html">procrastination</a>, that programmers need <a href="http://www.joelonsoftware.com/articles/BionicOffice.html">nice work environments</a>, <a href="http://www.paulgraham.com/gh.html">good tools</a> and <a href="http://www.joelonsoftware.com/navLinks/fog0000000262.html">their own office</a>, and about the evils of <a href="http://www.joelonsoftware.com/articles/fog0000000022.html">task-switching</a>.</p>
</li>
<li>
<p><a href="http://www.paulgraham.com/avg.html">PG</a> &amp; <a href="http://www.cabochon.com/%7Estevey/blog-rants/scheming-is-believing.html">SY</a> are both <a href="http://www.c2.com/cgi/wiki?SmugLispWeenie">smug lisp weenies</a>, and make pretty convincing cases that <a href="http://www.cabochon.com/%7Estevey/blog-rants/lisp-wins.html">Lisp is the way forward</a> and although JS <a href="http://discuss.fogcreek.com/newyork/default.asp?cmd=show&amp;ixPost=1998">isn't as convinced</a>, he may be <a href="http://lemonodor.com/archives/001274.html">changing his mind</a></p>
</li>
<li>
<p>PG &amp; JS are both successful entrepreneurs, though they <a href="http://webdevs.blogspot.com/2006/01/10-differences-between-joel-spolsky.html">don't always agree on the details</a>.</p>
</li>
</ul>
<p>But most of all, I think the key tenet that binds them together is an awareness of the Law of Increasing Returns. They each buy into the idea that:</p>
<ul>
<li>
<p>a really smart person</p>
</li>
<li>
<p>a powerful programming language</p>
</li>
<li>
<p>a beautifully-architected office</p>
</li>
<li>
<p>an uninterrupted 3-day period</p>
</li>
</ul>
<p>is worth 10</p>
<ul>
<li>
<p>Joes</p>
</li>
<li>
<p>Blubs</p>
</li>
<li>
<p>cubicles</p>
</li>
<li>
<p>half-hour slots between errands.</p>
</li>
</ul>
<p>PG's essay on <a href="http://www.paulgraham.com/taste.html">taste</a> is perhaps the most ardent tribute to the Law of Increasing Returns. He catalogues the hallmarks of good design, and though he doesn't say it, the key point of all this is that they add non-linearly. I'm still thinking about this.</p>
<p>He doesn't say much about how one can hone one's taste. I think there's a Vonnegut quote to the effect that the only way to learn to tell good painting from bad is to look at thousands and thousands of good ones, and it will become obvious to you.</p>
<hr/>
<p>Interestingly, while I was trawling for links for this essay, I noticed that the three of them read each other:</p>
<ul>
<li>
<p>Joel uses <a href="http://joel.reddit.com/">Reddit</a>, a startup that grew out of <a href="http://ycombinator.com/">Y Combinator</a>. Uppublish_date: hah! The first time I went to Joel's reddit page, I saw a link to <a href="http://steve-yegge.blogspot.com/2006/04/lisp-is-not-acceptable-lisp.html">Stevey's blog rants</a></p>
</li>
<li>
<p>PG actually appears in JS's <a href="http://www.joelonsoftware.com/items/2005/11/07.html">documentary</a></p>
</li>
<li>
<p>Stevey's <a href="http://www.cabochon.com/%7Estevey/blog-rants/bob-paradox.html">Being the averagest</a> post was inspired directly by Paul Graham's <a href="http://www.paulgraham.com/avg.html">Beating the averages</a></p>
</li>
</ul>
<p>I feel a little less clever now that it's clear that a lot of other people are reading all of them too:</p>
<ul>
<li>
<p><a href="http://webdevs.blogspot.com/2006/01/10-differences-between-joel-spolsky.html">Direct comparison</a> between PG and JS</p>
</li>
<li>
<p><a href="http://www.google.com/search?q=%22paul+graham%22+%22joel+spolsky%22+%22steve+yegge%22&amp;start=0&amp;ie=utf-8&amp;amp;oe=utf-8&amp;client=firefox-a&amp;rls=org.mozilla:en-US:official">Google search for all three names</a></p>
</li>
</ul>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/paul-graham-joel-spolsky-and-steve-yegge-and-the-law-of-increasing-returns/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>Email is the mind-killer</title>
      <link>https://www.gregdetre.com/blog/email-is-the-mind-killer/</link>
      <description>A *Dune*-inspired meditation on conquering email overwhelm transforms Frank Herbert's iconic litany against fear into a mantra for digital mindfulness.</description>
      <content:encoded><![CDATA[<p>I must not email.  </p>
<p>Email is the mind-killer.  </p>
<p>Email is the little-death that brings total obliteration.  </p>
<p>I will face my email.  </p>
<p>I will permit it to pass over me and through me.  </p>
<p>And when it has gone past I will turn the inner eye to see its path.  </p>
<p>Where the email has gone there will be nothing.  </p>
<p>Only I will remain.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/email-is-the-mind-killer/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>Alleviating tinnitus, and the shape of the auditory phenomenological landscape</title>
      <link>https://www.gregdetre.com/blog/alleviating-tinnitus-and-the-shape-of-the-auditory-phenomenological-landscape/</link>
      <description>Could **psychological interference** from carefully tuned external sounds disrupt the brain's perception of tinnitus, creating a kind of neural "noise cancellation" effect?</description>
      <content:encoded><![CDATA[<p><a href="http://en.wikipedia.org/wiki/Tinnitus">Tinnitus</a> is a chronic condition where you hear a ringing in your ears - for acute sufferers it can be very loud and never stops. This is tortuously unpleasant.  </p>
<p>I wonder if this has been tried - could you alleviate the symptoms of tinnitus by playing in a kind of psychologically out-of-phase sound, to cancel the ringing sound experienced? Of course, the normal physics of waves and phases won't hold true here, since the perceived sound isn't 'real' (i.e. external, based on moving currents of air).  </p>
<p>However, I wonder if there might be psychological rules about sounds where some external sound of the right characteristics might cause a kind of neural interference, and disrupt the perception of the tinnitus sound.  </p>
<p>One could imagine having a tinnitus subject navigate with gradient descent through a space of auditory parameters, rating the subjective intensity of the tinnitus sounds while listening to different external sounds. Eventually, you might find a point in the auditory parameter landscape where the tinnitus wasn't too annoying. With enough participants, you might learn something interesting about the shape of that landscape, and about the phenomenology (and neural representations) of audition.  </p>
<p>In principle, I suppose, one could do this with non-tinnitus sufferers, but I'm assuming that the tinnitus sounds are constant and so would provide a fixed point of comparison.  </p>
<p>UPDATE: hah! It looks like someone's trying to do something a little akin, though it uses a more physiological than phenomenological mechanism. I wonder what made them pick a low hum? See <a href="http://www.scientificamerican.com/podcast/episode.cfm?id=teen-inventors-fight-tinnitus-09-09-28%20">Teen inventors fight tinnitus</a></p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/alleviating-tinnitus-and-the-shape-of-the-auditory-phenomenological-landscape/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>Dropbocumentation</title>
      <link>https://www.gregdetre.com/blog/dropbocumentation/</link>
      <description>Every time a programmer goes away, infrastructure they know best breaks—that's **Murphy's Algorithm**. How do you optimize wiki 'edit' conversion rates to capture knowledge while it's fresh?</description>
      <content:encoded><![CDATA[<p>Every time a programmer goes away for a few days, a piece of infrastructure they know best breaks. That's just Murphy's Algorithm.  </p>
<p>If they they had only written a 100-word overview with some examples, that would have saved someone else a painstaking day figuring out how things should work, why they suddenly don't, and righting the world once more.  </p>
<p>How do you make it likely that everyone writes down what they know while it's still fresh? Think of edits as conversions (in the analytics sense) - our funnel stretches from signup to viewing to editing, and we want to maximize the number of edits.  </p>
<p>How do we optimize the 'edit' conversion rate for a wiki?  </p>
<ul>
<li>
<p><em>Editing should happen in the same mode as viewing.</em> If you have to click 'edit', then wait for a page refresh, then scroll down inside a teeny textbox in a browser, then hit save to see your changes ... those steps create a barrier to entry. The conversion rate of views to edits will drop dramatically. Typos, inaccuracies, inscrutabilities and out-of-datenesses will accumulate.</p>
</li>
<li>
<p><em>It needs to be as available as possible.</em> All and only your team can access and contribute to it, even if they're on a different computer or offline.</p>
</li>
<li>
<p><em>Consolidate everything in one or two searchable places.</em> When it's hard to find something, you won't want to start looking. If you have to search one by one through a wiki, your email, a bug tracker, the version control commit log and comments in the codebase, you'll end up just tapping someone on the shoulder - the knowledge will never get planted in a way that it can grow.</p>
</li>
<li>
<p><em>No special knowledge.</em> Wiki markups are confusing and confusable. WYSYWYG editors are a good start - but editing text in most browser textboxes feels like typing with chopsticks. And proprietary document formats are opaque and constricting.</p>
</li>
<li>
<p><em>No barrier to exit.</em> I want to be able to easily (and ideally automatically) grab a dump of all our documentation, both as a backup and as an export.</p>
</li>
</ul>
<p>After reviewing these possibilities over and over, these are the best solutions I've come up with for Memrise:  </p>
<ul>
<li>
<p><em>A few monolithic Google Docs.</em> This has worked reasonably well, except that Google Docs still falters in an unwieldy and buggy way when dealing with even medium-sized documents. Boooo!</p>
</li>
<li>
<p><em>Etherpad clone</em>. They seem pretty expensive for multi-user monthly subscriptions, and seem weak at linking and searching. Plus, they don't work offline, and I don't trust the companies behind them to be around in 5 years' time.</p>
</li>
<li>
<p><em>Text files in Dropbox</em>. The main downside to this is that you can't easily inter-link text files, and they lack formatting which makes them ugly to read. But they have no barriers to entry whatsoever.</p>
</li>
</ul>
<p>In an ideal world, someone would build a nice (optionally hosted?) wiki solution pulling and formatting Dropbox text files as webpages to give you the best of both worlds, perhaps combined with a few desktop apps and extensions to make offline viewing editing more pleasant.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/dropbocumentation/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>Evangelizing Emacs (in terms of economics)</title>
      <link>https://www.gregdetre.com/blog/evangelizing-emacs-in-terms-of-economics/</link>
      <description>Emacs has a **huge barrier to entry**, but its keyboard shortcuts and customizations provide enormous *economies of scale* once mastered.</description>
      <content:encoded><![CDATA[<p>As may have become apparent, I harbour somewhat fanatically evangelical feelings about Emacs, though i'm very aware of its failings and idiosyncracies too.  </p>
<p>Let me try my pitch for why you should learn Emacs in terms of economics. As a computer user, you want the biggest bang for your buck, the most reward for effort spent mastering a tool - one way to ensure this is for that tool to be useful in a variety of situations. Keyboard shortcuts, customizations, regular expressions and the like all provide enormous economies of scale. That is, they provide value, but they have a high marginal cost initially. However, the more you use them, that cost gets amortized, and so the more you profit from them.  </p>
<p>Anyway, Emacs has a huge huge barrier to entry. Getting the hang of it is a pain, mainly because it has a huge amount of jargon and all of its keyboard shortcuts seem to involve 3 keys. The good news is that with a little effort, you can change them all to something a little more reasonable. Furthermore, once you've got the hang of a few, then you get to reuse them for almost everything you do.  </p>
<p>Perhaps I should just quote Neal Stephenson, who <a href="http://project.cyberpunk.ru/lib/in_the_beginning_was_the_command_line/">said it best</a> a long time ago:  </p>
<p>"I use Emacs, which might be thought of as a thermonuclear word processor. It was created by Richard Stallman; enough said. It is written in Lisp, which is the only computer language that is beautiful. It is colossal, and yet it only edits straight ASCII text files, which is to say, no fonts, no boldface, no underlining. In other words, the engineer-hours that, in the case of Microsoft Word, were devoted to features like mail merge, and the ability to embed feature-length motion pictures in corporate memoranda, were, in the case of Emacs, focused with maniacal intensity on the deceptively simple-seeming problem of editing text. If you are a professional writer--i.e., if someone else is getting paid to worry about how your words are formatted and printed--Emacs outshines all other editing software in approximately the same way that the noonday sun does the stars. It is not just bigger and brighter; it simply makes everything else vanish. For page layout and printing you can use TeX: a vast corpus of typesetting lore written in C and also available on the Net for free."</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/evangelizing-emacs-in-terms-of-economics/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>Hot tips from the Viennese recliner</title>
      <link>https://www.gregdetre.com/blog/hot-tips-from-the-viennese-recliner/</link>
      <description><![CDATA[<p>drown out the voices in your head. talk gibberish. drive THEM insane. after all, we still have control of our mouths</p>]]></description>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/hot-tips-from-the-viennese-recliner/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>Memories slipping through your fingers like sand...</title>
      <link>https://www.gregdetre.com/blog/memories-slipping-through-your-fingers-like-sand/</link>
      <description>You don't remember life as a continuous stream—you remember **episodes**. The secret to slowing time's passage lies in creating salient experiences that serve as vivid bookmarks of each era.</description>
      <content:encoded><![CDATA[<p>the way to slow down experience is to have salient experiences as bookmarks and embodiments of an era for you - you don't remember a stream, even though that's how you experience, but rather you remember episodes</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/memories-slipping-through-your-fingers-like-sand/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>Scroogled</title>
      <link>https://www.gregdetre.com/blog/scroogled/</link>
      <description>Cory Doctorow's dystopian fiction explores **'What if Google were evil?'** — a compelling thought experiment about privacy erosion and surveillance power.</description>
      <content:encoded><![CDATA[<p>Cory Doctorow has a nice, dystopian piece imagining <a href="http://www.radaronline.com/from-the-magazine/2007/09/google_fiction_evil_dangerous_surveillance_control_1.php">'What if Google were evil?'</a>. It also provides a little ammo to discharge in the direction of people who ask what good people should have to fear about the burgeoning infringements of our privacy. This is a tricky subject for me, since I spend much more time dreaming of cool uses for <a href="http://gregdetre.blogspot.com/search?q=collaborative+filtering">collaborative filtering</a> than dwelling on the chills I get from abuses of people's private data.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/scroogled/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>Auto-links</title>
      <link>https://www.gregdetre.com/blog/auto-links/</link>
      <description>What if wikis could automatically create links as you type, without CamelCase or brackets? Auto-links detect existing page names and transform them into links instantly, creating **lazy serendipity**.</description>
      <content:encoded><![CDATA[<p>Most wikis require you to perform one of two contortions to create a link:  </p>
<ul>
<li>
<p>Use CamelCase. Much like a camel, this is robust, but tiring to finger.</p>
</li>
<li>
<p>Wrap things in ["symbols that are hard to type"].</p>
</li>
</ul>
<p>In both cases, you need to know in advance that you plan to create a link, and be enough of a disciplined philistine to overcome the effort and overlook the ugliness.  </p>
<p>Auto-links are the solution [1] - here's how they work. Say you create a page called 'Camel case'. Now, type Camel case anywhere else, and that 'Camel case' text will be turned into an auto-link as you go. In other words, the wiki notices that you've typed the name of an existing page in the midst of your text, and automatically creates a link for you. If you go back and edit the text, the link goes away. [2]  </p>
<p>Links between pages become evident to readers without any extra effort on the part of the writer. If I type 'MySQL' and an auto-link appears, it's easy to see that a relevant page about it already exists.  </p>
<p>Having used <a href="http://www.youtube.com/watch?v=vGqaSzD-FTE#t=2m11s">such a system</a> for a long time, I have come to appreciate the tiny flash of satisfaction at seeing a link appear with no extra effort, confirming that the page does indeed exist [3], and making navigation while editing a breeze. Pages that I wrote years ago are now festooned with links to pages that were created long afterwards. Indeed, the most satisfying feeling of all is when an auto-link pops up to a page I'd forgotten I wrote. Lazy serendipity!  </p>
<p>[1] see <a href="http://faculty.psy.ohio-state.edu/sederberg/">Per Sederberg</a>'s implementation in <a href="http://www.youtube.com/watch?v=vGqaSzD-FTE#t=2m11s">Emacs Freex</a> mode, though we called them 'implicit links' back then  </p>
<p>[2] To do this the way God intended requires running a regex containing all the pagetitles in your wiki over what you type on every keystroke - this is very nearly instantaneous for even 10k documents.  </p>
<p>[3] For extra points, allow pages to have multiple aliases, so that (for instance) 'database', 'databases' and 'MySQL' all point to the same page.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/auto-links/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>What's blowin' in the wind?</title>
      <link>https://www.gregdetre.com/blog/whats-blowin-in-the-wind/</link>
      <description>What if we could identify musical early adopters through data, then track what they're listening to now to predict next year's hits?</description>
      <content:encoded><![CDATA[<p>[Thanks to Stephen Hartley-Brewer for the kernel of this idea]  </p>
<p><a href="http://bluehoneybear.blogspot.com/">My brother is my musical weather vane</a>, my song-canary who knows what's good long before the rest of the world has cottoned on, and points out the things I'll like. I treasure his advice, partly because it's good, and partly because it comes from him.  </p>
<p>But just for badness, pretend you don't have a brother with his ear to the ground. Instead, you have a big computer that you've fed the listening habits of the entire world. Ask it two questions:  </p>
<p>1) who is listening to music that's hugely popular (or that I like) a year ago?  </p>
<p><em>Those are your early adopters.</em> </p>
<p>2) what are they listening to now?  </p>
<p>Throw in some <a href="http://gregdetre.blogspot.com/2007/04/collaborative-filtering-and-how-its.html">collaborative filtering</a> to tailor the recommendations for me.  </p>
<p>Are iTunes or Last.fm doing this to predict who's going to be big next year?  </p>
<p>Are the record labels doing this to predict where they should put their marketing money?  </p>
<p>Can bands buy this information to figure out which users to send promo albums to?  </p>
<p>If there was a prediction market for music, could my brother get paid to tell people what he likes and dislikes?</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/whats-blowin-in-the-wind/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>Procrastination</title>
      <link>https://www.gregdetre.com/blog/procrastination/</link>
      <description>Reading about avoiding procrastination is among the *best* ways to procrastinate—but what if you could harness this tendency as a force for good?</description>
      <content:encoded><![CDATA[<p>Reading about how to avoid procrastinating is amongst my very favourite ways of procrastinating. It's a lot easier than whatever you're supposed to be doing, and neatly neutralises the guilt that you'd otherwise feel with a seductive promise that in the long run, this will prove to be the most useful hour you've ever spent.  </p>
<p>There appear to be at least two main schools of thought regarding procrastination. There are certainly those who treat it as an evil that can be combatted, either head-on or deviously, but there are also those that embrace some degree of procrastination in the service of sifting project-wheat from errand-chaff.  </p>
<p>There are people who spring out of bed at <a href="http://www.stevepavlina.com/blog/2005/05/how-to-become-an-early-riser/">5am</a>, chanting 'get thee behind me, Slashdot', who are all too willing to tell you how to 'maximise your productivity'. <a href="http://www.stevepavlina.com/">Steve Pavlina</a>'s intoxicating account of how he ostensibly <a href="http://http//www.stevepavlina.com/articles/do-it-now.htm">graduated from college in CS in three semesters</a> is the best example of this. Look how easy life is if you don't waste any time whatsoever, he whispers to you. He's either making it all up, or a superman, but he does tell an <a href="http://www.stevepavlina.com/blog/2005/06/the-meaning-of-life-intro/">interesting story</a>. And his <a href="http://www.stevepavlina.com/blog/2005/10/polyphasic-sleep/">polyphasic sleep</a> experiment is worth a read.  </p>
<p>Then there's this bit of mental judo for using procrastination as a force for good. Basically, the idea behind <a href="http://www-csli.stanford.edu/%7Ejohn/procrastination.html">'structured procrastination'</a> is this:  </p>
<p>"Procrastinators seldom do absolutely nothing; they do marginally useful things, like gardening or sharpening pencils or making a diagram of how they will reorganize their files when they get around to it. Why does the procrastinator do these things? Because they are a way of not doing something more important. If all the procrastinator had left to do was to sharpen some pencils, no force on earth could get him do it. However, the procrastinator can be motivated to do difficult, timely and important tasks, as long as these tasks are a way of not doing something more important."  </p>
<p>Continue procrastinating. But instead of reading <a href="http://www.theonion.com/">The Onion</a>, procrastinate by doing something you'll have to do eventually. This may not be the thing you should be doing most of all, but it's better than nothing. And it won't feel as much like work, because you still get to feel that you're avoiding the thing you're not supposed to be avoiding. Everyone's a winner.  </p>
<p>In opposition to this idea, <a href="http://www.paulgraham.com/index.html">Paul Graham</a> argues that there are <a href="http://www.paulgraham.com/procrastination.html">good and bad forms of procrastination</a>:  </p>
<p>"There are three variants of procrastination, depending on what you do instead of working on something: you could work on (a) nothing, (b) something less important, or (c) something more important. That last type, I'd argue, is good procrastination."  </p>
<p>He and <a href="http://www.joelonsoftware.com/">Joel Spolsky</a> are in remarkably close agreement on this (and <a href="/blog/paul-graham-joel-spolsky-and-steve-yegge-and-the-law-of-increasing-returns/">related issues</a>). Difficult and important things, like research, need big chunks of time and get completely minced by interruptions and any kind of task-switching. If blowing off a few errands means that you don't get knocked out of the zone, and work solidly on a hard problem for three days straight, then that's the way to be. And often, the things that you're procrastinating about will disappear of their own accord - that's a sure sign they weren't that important to begin with.  </p>
<p>I think there's a final point to remember about procrastinators, as people. It is possible to be very successful and still <a href="http://www.joelonsoftware.com/articles/fog0000000339.html">procrastinate horrendously</a>. For this to work, you need constructive panic. People who constructively panic thrill a little in the throes of that total focus you get when you realize that you have no time left to waste. You have exactly as much time remaining as you need to get things done, assuming you sleep as little as humanly possible, and view the whole world through a hole the size of a pinprick with the unblinking eye of your deadline staring back at you. Procrastination brought you here, and constructive panic will get you out.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/procrastination/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>Presentations at BarCamp Tampa 2015</title>
      <link>https://www.gregdetre.com/blog/barcamp-tampa-2015/</link>
      <description>Two presentations from BarCamp Tampa 2015: mastering Python unit testing for better code, and **killing your crusty old PHP system** through seamless replatforming.</description>
      <content:encoded><![CDATA[<p><a href="/wp-content/uploads/2015/10/presentation-barcamp-tampa-python-151017.pdf">Unit testing in Python  - better code faster</a>.</p>
<p><a href="/wp-content/uploads/2015/10/presentation-barcamp-tampa-replatforming-1510172.pdf">The best way to kill and bury your crusty old PHP system</a> - replatforming a legacy system without your users noticing.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/barcamp-tampa-2015/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>It has to be easy, and worth it, for you to add tags</title>
      <link>https://www.gregdetre.com/blog/it-has-to-be-easy-and-worth-it-for-you-to-add-tags/</link>
      <description>Tagging lets you assign things to multiple categories, reducing cognitive cost—but only if it's **effortless** to add tags and **reliable** for finding things later.</description>
      <content:encoded><![CDATA[<p>Whoever adopted the idea that "there's a place for everything, and everything in its place" when it came to organizing files and ideas on a computer suffered from a failure of imagination. Or maybe they were just over-wedded to the desktop and filing cabinet metaphors. Fortunately, the idea of 'tagging' (or 'labels' in Google's parlance) blew that whole banal tidiness away. In short, tagging lets you assign things to multiple categories, or if you prefer, put things in multiple places. Rashmi describes this well - tagging is popular because there's a <a href="http://www.rashmisinha.com/archives/05_09/tagging-cognitive.html">lower cognitive cost</a> when you can put things in multiple categories, rather than having to decide on just one.</p>
<p>We've only just started to scratch the surface of how categorization schemes could work. I'm going to propose a few ways in which things might grow from here, focusing on the restricted case where you're tagging your own files privately, ignoring all the interesting goodness that happens when those tags are available to others, <a href="http://del.icio.us/">delicious</a>-style.</p>
<p>N.B. I'm going to use the term 'category' rather than 'tag', since it's easier to think of things belonging to categories than being labelled with a tag. The key notion is that things can belong to multiple categories simultaneously.</p>
<h3 id="the-more-tags-the-better">The more tags the better</h3>
<p><a href="http://207.22.26.166/">Jon Udell</a> has a great post on <a href="http://webservices.xml.com/pub/a/ws/2002/01/01/topic_map.html">building up a taxonomy of categories by hand</a>, starting with a smallish corpus of documents, and just letting the taxonomy emerge, combined with a little judicious weeding. The dataset he has in mind is pretty small, and so he's aiming for 15-40 categories. The kinds of datasets I have in mind are much larger.</p>
<p>For instance, I have a few thousand text files with notes on topics ranging from Ubuntu troubleshooting to the symptoms of schizophrenia to my travel arrangements for the summer. I could maybe try and shoe-horn things into a few tens of categories, with each category holding many items, and each item belonging to maybe one or two categories. But I very quickly found this to be unsatisfying. We want to be able to differentiate things more finely than that. For instance, how would I categorize a document containing hotel bookings in Florence last summer for the <a href="http://www.humanbrainmapping.org/florence2006">HBM conference</a>? Just by 'travel'? Or also 'Florence', 'conference', 'hotel', 'HBM', and '2007'. Remember the argument about lower cognitive cost though - it's much less effort just to include all those categories. If I do that, I'll end up with many hundreds or even thousands of categories, some of which will have tens or hundreds of members and some of which might only have one or two members. I think one might raise two main objections to this approach:</p>
<ul>
<li>
<p>Can you really be bothered to add a bunch of categories each time you write something?</p>
</li>
<li>
<p>How do you begin to find anything now? Sometimes filtering by a category doesn't help because it returns way too many members, and sometimes it doesn't help because it returns hardly any. Where's Goldilocks when you need her?</p>
</li>
</ul>
<p>I'll address these in turn.</p>
<h3 id="can-you-be-bothered-to-add-a-bunch-of-categories-each-time">Can you be bothered to add a bunch of categories each time?</h3>
<p>People are lazy. Any system that requires people to be assiduous book-keepers while they're writing is doomed. Dave Winer talks about how he should be categorizing all his posts, and yet he doesn't do it - and this makes him feel <a href="http://archive.scripting.com/2005/01/29#guiltAboutCategories">guilty</a>. He knows that he won't be able to trust the categories to find that thing later. The value of the whole system has dropped. Squirrels wouldn't go to the effort of hoarding nuts for the winter if they knew that they wouldn't remember where those nuts are when they need them. So what's the point of hoarding nuts any more? All of a sudden, the system has broken down. We need to find a way to make the system less brittle.</p>
<p>Let's look at Dave Winer's guilty confession a little more closely:</p>
<blockquote>
<p>"I have a very easy category routing system built-in to my blogging software. To route an item to a category, I just right-click and choose a category from a hierarchy of menus. I can't imagine that it could be easier. Yet I don't do it."</p>
</blockquote>
<p>If you ask me, that's not easy enough. Navigating hierarchical menus with a mouse is slow and distracting. Blogger does it right - there's a 'labels' text box that you can tab to, into which you can write a comma-delimited list of tags. As you type, it auto-suggests - pressing 'return' fills in the rest of the tag and puts a comma and space after for you. So that's step 1.</p>
<p>But it should be even easier. What <em>should</em> happen is that the machine should automatically throw up a list of tags that it thinks might be appropriate for this post. It should put the ones it's most confident about to the left, and less confident ones to the right, with the cursor positioned at the end to make it easy for the user to delete false positives and add new categories it missed. And if you're feeling lazy, then you can just accept the machine's suggestions without glancing at them. The cost of a false positive is low, so it'll deliberately suggest too many. This brings us neatly to our second concern.</p>
<h3 id="but-then-how-do-you-find-anything">But then how do you find anything?</h3>
<p>So now every document belongs to a bajillion categories, none of which is particularly useful on its own. But a conjunction of categories should narrow things down nicely. If I'm trying to find that hotel booking in Florence, I don't have to worry about remembering whether it's tagged with 'travel', 'hotel', 'Florence', '2007' or 'HBM conference', since it's tagged with all of them. So I'll try filtering by the conjunction of 'hotel'+'Florence'+'2007' and that'll probably winnow things down sufficiently for me to pick the file out manually (see also: <a href="http://gregdetre.blogspot.com/2007/04/make-tags-not-trees-filesystem-idea.html">make tags not trees</a>). .</p>
<p>But maybe we never made a 'Florence' category. It seems like such a natural cue to use now, but at the time, 'Florence' didn't spring to mind as a salient category, despite our liberal categorizing policy. If the system auto-completes in a handy way, we'd already know this, and our fingers would already be backspacing and trying 'Italy' or 'HBM conference'. There are many points of failure, but there are also many points of entry. If we make it easy enough to cue for conjunctions of categories, then there's a very low cognitive cost to having to backtrack once or twice, since our brain effortlessly supplies us with so many possible cues to use.</p>
<p>We could make things even less brittle in lots and lots of ways. Perhaps the system notices that only one item in the whole database is tagged with 'Florence', so it's probably too restrictive a category. No matter. It could just ignore 'Florence', or suggest that we omit 'Florence' from our search. Better still, and less intrusively, it could now grep through all the files that match one or more of the tags to see if 'Florence' appears in the text, and automatically suggest any matches as partial matches.</p>
<h3 id="conclusions">Conclusions</h3>
<p>I keep coming back to the same feeling - for the most part, people don't write notes because they don't think they'll be able to find those notes later when they need them - so why bother writing the notes in the first place?</p>
<p>All of these suggestions are geared towards:</p>
<ul>
<li>
<p>Reducing the cognitive cost at both writing and retrieval. If it's less effort, you'll feel less lazy about adding category metadata.</p>
</li>
<li>
<p>Making the system less brittle, so that if you <em>were</em> lazy about your category metadata, you still have a good chance of finding things later. This is the key to ensuring that you don't end up losing faith and give up on writing things down in a structured way altogether.</p>
</li>
</ul>
<p>Taken together, I hope that it will become easier to categorize your notes in a way that helps you find them later, which is going to make you much more likely to write them down in the first place.</p>
<hr/>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/it-has-to-be-easy-and-worth-it-for-you-to-add-tags/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>The Turing tournament - a proposal for a reformulation of the Turing Test</title>
      <link>https://www.gregdetre.com/blog/the-turing-tournament-a-proposal-for-a-reformulation-of-the-turing-test/</link>
      <description>What if we replaced the binary pass/fail Turing Test with a competitive tournament where players devise rules for each other, creating a **richer, multi-dimensional measure of intelligence**?</description>
      <content:encoded><![CDATA[<ol>
<li>
<p>Introduction</p>
</li>
<li>
<p>Describing the Turing Tournament</p>
</li>
<li>
<p>Comparing the Turing Test and the Turing Tournament</p>
</li>
<li>
<p>Devising new rules, and non-linguistic competitors</p>
</li>
<li>
<p>But is it intelligent?</p>
</li>
</ol>
<hr/>
<blockquote>
<p><em>MH: Are you a computer?</em></p>
<p><em>Dell: Nope.</em></p>
<p><em>MH: You'd be surprised how many fall for that one.</em></p>
<p><em>Dell: Not me.</em></p>
<p><em>——</em></p>
<p><em>MH: What's fifty-six times thirty-three?</em></p>
<p><em>Dell: One thousand eight hundred forty-eight.</em></p>
<p><em>MH: You're pretty fast!</em></p>
<p><em>Dell: Those are my favorite numbers.</em></p>
<p>— from <a href="http://home.sprynet.com/%7Eowl1/turing.htm">http://home.sprynet.com/~owl1/turing.htm</a></p>
</blockquote>
<hr/>
<h3 id="introduction">Introduction</h3>
<p>The <a href="http://cogprints.org/499/00/turing.html">Turing Test</a> was designed to be an operational test of whether a machine can think. In <a href="http://www.eecs.harvard.edu/shieber/">Stuart Shieber</a>'s <a href="http://www.amazon.com/gp/product/0262692937/002-6025796-7553627?v=glance&amp;n=283155">words</a>:</p>
<blockquote>
<p><em>"How do you test if something is a meter long? You compare it with an object postulated to be a meter long. If the two are indistinguishable with regard to the pertinent property, their length, then you can conclude that the tested object is the given length. Now, how do you tell if something is intelligent? You compare it with an entity postulated to be intelligent. If the two are indistinguishable with regard to the pertinent properties, then you can conclude that the tested entity is intelligent (pg 1)."</em></p>
</blockquote>
<p>In order for a machine to be deemed intelligent according to the Turing Test, we would determine whether human judges could reliably distinguish a human from the machine after some lengthy text-only conversation. I don't think a machine is going to pass it any time soon, and when it does, it'll be pretty self-evident that we're dealing with a machine that can think.</p>
<p>Anyone who disagrees that a full and proper Turing Test is a stringent enough test of intelligence should read <a href="http://www.ulg.ac.be/cogsci/rfrench.html">Robert French</a>'s <a href="http://www.google.com/url?sa=t&amp;ct=res&amp;cd=1&amp;url=http%3A%2F%2Fwww.ulg.ac.be%2Fcogsci%2Frfrench%2Fturing.pdf&amp;ei=4xlCRNfiEayMaJSd8ZcE&amp;sig2=TIdl4ow8Yn-kxhPVAVHuzg">excellent discussion</a> of the kinds of very human and culturally rooted subcognitive processes that would have to going on in the machine in order for it to pass. His criticism is that the Turing Test "provides a guarantee not of intelligence but of culturally-oriented <em>human</em> intelligence", i.e. that it sets the bar too high, or too narrowly. This is a subtler variant of the obvious point that human beings who don't speak English would fail a Turing Test with English-speaking judges. In other words, the Turing Test is a necessary but not sufficient test of intelligence, because you would have to have a certain subcognitive makeup in order to pass it, on top of being intelligent.</p>
<p>The beautiful thing about the Turing Test is that there's nothing about it that's specific to machines. Indeed, Turing's original idea for the Imitation Game, as he termed it, was based on a parlour game where the judge attempted to distinguish male from female players. This essay is an attempt to broaden the scope of the Turing Test from being a binary and culturally-rooted test of human intelligence to something vaguer and less unidimensional.</p>
<p>Let's make this idea somewhat more concrete, and considerably more vivid. Imagine that a small, slimy green-headed alien lands on your lawn right now, travelling in a spaceship the size of a Buick. Assume that the alien demonstrates its extraterrestrial credentials to your satisfaction by whisking you to its home planet and back before breakfast. It bats away the impact of a few .357 rounds with its forcefield and patiently replicates household objects for your amusement. It would seem niggardly to refuse a being that has mastered faster-than-light travel the ascription of intelligence when most humans can't tie their shoelaces in the morning without a dose of caffeine. So we might be moved to patch the Turing Test in some <em>ad hoc</em> manner to read:</p>
<blockquote>
<p>"Any entity that cannot be reliably distinguished from a human after a lengthy text-only conversation, OR that has mastered faster-than-light travel and can withstand a .357 round at close up, can be considered to be intelligent."</p>
</blockquote>
<p>It's clear that this lacks the pithy generality of Turing's original formulation, and we'd have to do quite a lot more work to restrict the scope of the above to exclude asteroids. Perhaps over time, our super-intelligent alien will learn to speak English with a flawless cockney accent, and will pass the standard Turing Test, rendering this discussion moot. But in the meantime, before it has learned to speak a human language, we are faced with a manifestly intelligent being that fails our gold standard test for intelligence. The background aim of this whole essay will be to consider a new version of the Turing Test that overcomes the inherent human- and language-specific parochialism of the original. That way, our intelligent alien might pass, without having to learn to speak English with a cockney accent.</p>
<p>Along the way, it may be that our reformulated test provides a more constructive goal and yardstick by which to direct and evaluate progress in AI research than the standard Turing Test. Perhaps its primary limitation is that it's difficult to restrict the difficulty or scope without losing everything that's interesting about the test. And since even our current best efforts are a long way from success, the gradient of improvement is almost flat in every direction, making it difficult to discern when progress is being made in the right direction. This makes it difficult for machines to bootstrap themselves by training against each other, requiring lots of labour-intensive profiling against humans. Finally, the current test is very language-orientated, and undesirably emphasizes domain knowledge,</p>
<h3 id="describing-the-turing-tournament">Describing the Turing Tournament</h3>
<p>I'll term this new version of the Turing Test the 'Turing Tournament', to reflect its competitive round robin form. Like the original Turing Test, the Turing Tournament will not yield a definitive, objective yes/no answer, but rather a ranking of the entrants, where the human players provide a benchmark. A lot of the details I'm proposing will probably need considerable refinement. Here are the organizing principles of a Turing Tournament:</p>
<ul>
<li>
<p>The organizers of each tournament decide what the domain of play will be, e.g. a chessboard, text, a paint program, a 3D virtual reality environment, binary numbers, or some multidimensional analogue stimuli.</p>
</li>
<li>
<p>Every 'player' (within which I'm subsuming both human and machine variants) is competing in a round robin competition, and will play every other player twice, once as the 'teacher' and once as the 'student'.</p>
</li>
<li>
<p>Every bout will have two players, a teacher and a student. Play proceeds in turns, with the teacher going first. Play terminates when the allotted time has been exceeded, or when some terminating criterion specified by the teacher has been satisfied. Neither player will know the identity of the other player.</p>
</li>
<li>
<p>Before the bouts begin, every player is given access to the domain of play so that they can construct their own set of rules that will operate when they are the teachers in a bout.</p>
</li>
<li>
<p>The organizers of each tournament determine the scoring for bouts that terminate relative to bouts whose time elapses. We will consider some possible scoring systems <a href="http://doom.princeton.edu/www/blog/TuringTournament.html#scoring">later</a>.</p>
</li>
</ul>
<p>These sound like strange rules. What kind of games could be played? Why does each teacher get to set their own rules? Do teachers get rewarded or punished if a student is able to reach criterion for their bout?</p>
<p>I think the easiest way to illustrate what I have in mind is with a concrete example. Imagine the following scenario:</p>
<ul>
<li>
<p>A big room with lots of people sitting at computers. The people are the human players. The machine players are sitting inside a big server at the back of the room.</p>
</li>
<li>
<p>The domain for this competition is a Go board, a 19x19 checkerboard with black and white pieces. Although all bouts in this tournament will take place on a Go board, the rules and goals of each bout will be up the teacher of that bout.</p>
</li>
<li>
<p>Let us peer over the shoulder of a human player, currently in the role of student, trying to determine what the rules of the bout are, and play so that the bout terminates before running out of time. Neither we nor they know whether the other player is human or machine.</p>
</li>
<li>
<p>The board is blank initially.</p>
</li>
<li>
<p>As always, the teacher makes the first move. They place a horizontal line of 19 black pieces in the bottom row of the board.</p>
</li>
<li>
<p>Now it is the student's turn. They have no idea how the bout is scored, what the aim is, what constitutes a legal move, how many moves there will be or whether there will be multiple sub-bouts. All of that is up to the teacher.</p>
<p>Working on the assumption that the teacher wants the student to play white, the student lays down a single white piece in the top left corner.</p>
</li>
<li>
<p>The teacher removes the white piece, and replaces it with a horizontal row of white pieces just above the existing horizontal black line, and another horizontal row of black pieces above that. So now there are three rows of pieces filling up from the bottom of the board: black, white and then black.</p>
</li>
<li>
<p>The student decides that the removal of their white piece in the corner was a signal that its future moves should consist of placing an entire row of pieces on the board at a time. The student tries placing an entire row of white pieces in the top row of the board.</p>
</li>
<li>
<p>The teacher again removes all the student's pieces, and replaces them with another row of white pieces and another row of black pieces. The bottom of the board consists of black, white, black, white and black stripes.</p>
</li>
<li>
<p>The student reasons that its next move should be to place a row of white pieces above the most recent row of black pieces to continue the stripy pattern.</p>
</li>
<li>
<p>Gratifyingly, the teacher leaves the row of white pieces in place, and adds a black row above it, as expected.</p>
</li>
<li>
<p>The two players continue to take turns until all but the top row has been filled with alternating black and white rows.</p>
<p>Now, it is once more the teacher's turn, and the student wonders whether the last row will be filled in. Instead, the board blanks again, and the teacher places a vertical column of white pieces on the right hand side.</p>
</li>
<li>
<p>The student tries tentatively to place an adjacent column of black pieces, deciding that this sub-bout involves creating black and white vertical stripes, with the black and white players reversed.</p>
</li>
<li>
<p>As it turns out, this assumption appears to be correct, since the teacher does not remove the student's pieces, and together they quickly build up an alternating vertical stripe that moves leftwards.</p>
</li>
<li>
<p>When only the last column remains to be filled in by the teacher, the bout has reached criterion, and the student moves on to the next bout, with a different player.</p>
</li>
<li>
<p>Upon inspecting the scores later, our human player (the 'student' in the bout just described) finds that they had scored highly on that bout, but not as high as some. Some of the machine players had failed to see a pattern at all, and had been putting pieces down more or less at random. These players did the worst, since the scoring for this tournament is a function of the total number of turns taken to finish the bout, as well as the number of errors made by the student. (need to clarify???)</p>
<p>Like our hero, the best players at this bout had also quickly deduced that the pattern involved stripes. Their extra insight came after a few turns, where they tried placing multiple stripes down at once. As it turns out, there was nothing in the rules set by the teacher prohibiting this, and so they finished more quickly, earning a higher score.</p>
<p>It seems reasonable to imagine that most humans would quickly figure out the stripy pattern, and some would eventually think to lay down multiple stripes at a time. Might a machine? Perhaps soon.</p>
</li>
</ul>
<p>This is intended as a toy example. The rules of the bout are pretty simple, but I think they would discriminate somewhat between intelligent and not-so-intelligent players. The key point to note is that each player would play twice against every other player, once as the teacher and once as the student playing within the teacher's rules. Perhaps some bouts are too hard, and some are too easy. But en masse, the rankings should discriminate quite finely between players, even between human players. The exact details of the scoring, especially how teachers are scored, and how teachers pre-specify their rules, are clearly going to be crucial. It will suffice for now to say that students should probably get points for satisfying the criterion of a bout quickly, and teachers should be rewarded for devising discriminative games, that is, games that only intelligent players can solve. I will defer further discussion of these topics until <a href="http://doom.princeton.edu/www/blog/TuringTournament.html#scoring">later</a>.</p>
<h3 id="comparing-the-turing-test-and-the-turing-tournament">Comparing the Turing Test and the Turing Tournament</h3>
<p>In discussing the idea of an Inverted Turing Test (more <a href="http://doom.princeton.edu/www/blog/TuringTournament.html#inverted">below</a>) Robert French <a href="http://psycprints.ecs.soton.ac.uk/archive/00000531/">states</a> that:</p>
<blockquote>
<p><em>"All variations of the original Turing Test, at least all of those of which I am currently aware, that attempt to make it more powerful, more subtle, or more sensitive can, in fact, be done within the framework of the original Turing Test."</em></p>
</blockquote>
<p>Is the same true of the Turing Tournament? I think the answer is both yes and no. In fact, you could think of a Turing Tournament as a kind of generalization of the Turing Test. That is, the original Turing Test could be treated (more or less) as a Turing Tournament where the domain of play is restricted to text, and the bouts terminate if the teachers/judges are satisfied they are talking to a human. It wouldn't be quite the same, since here the players would double up as judges and the judges would double up as players. In other words, the machines would also themselves be making judgements about the humanness of both each other and the humans - an 'Inverted Turing Test'. In its current formulation, where every player plays every other player as both teacher and student (i.e. judge and player), a Turing Tournament would really be a strange hybrid of both the Inverted and standard Turing Tests.</p>
<p>The idea of an Inverted Turing Test has been <a href="https://web.archive.org/web/20071101055928/http://psycprints.ecs.soton.ac.uk/archive/00000506/">proposed</a> before:</p>
<blockquote>
<p><em>"Instead of evaluating a system's ability to deceive people, we should test to see if a system ascribes intelligence to others in the same way that people do ... by building a test that puts the system in the role of the observer ... [A] system passes [this Inverted Turing Test] if it is itself unable to distinguish between two humans, or between a human and a machine that can pass the normal Turing Test, but which can discriminate between a human and a machine that can be told apart by a normal Turing Test with a human observer."</em></p>
</blockquote>
<p>French ingeniously <a href="http://psycprints.ecs.soton.ac.uk/archive/00000531/">showed</a> that this Inverted Turing Test could be simulated within a standard (if somewhat convoluted) Turing Test. In contrast, it seems clear that an unrestricted Turing Tournament could not be fully simulated by a Turing Test because the potential domains of play are limitless. So although one might imagine instantiating the Go domain by communicating using grid references within a standard Turing test, it seems clear that there would be no way to run a domain of play such as a 3D virtual reality environment within a standard Turing Test using text alone. The principle advantage of widening the domain of play from text-only in this way is to allow players to pass some kinds of Turing Tournaments without speaking English, or any language at all. As a result, it seems reasonable to think of the Turing Tournament as (more or less) a superset of the Turing Test, or if the reader prefers, at least a redescription of it with unrestricted domains of play. I find this Ouroborean quality quite pleasing. [is it really ouroborean???] Either way, we can agree that most of the original Test's merits and stringency should still be present in the Tournament version, depending on the way a particular Tournament's domain of play and restrictions are set up. This does raise the important question of whether a Tournament victory would be as convincing a demonstration of intelligence as a victory in a standard Turing Test - I will return to this <a href="http://doom.princeton.edu/www/blog/TuringTournament.html#convincing">below</a>.</p>
<p>French also shows that the Inverted Turing Test could be passed by a simple and mindless program that would take advantage of the very subcognitive demands that make the original test so parochial and difficult to pass. In short, the machine could have a pre-prepared list of questions that have been shown to weed out machines in the past, such as</p>
<blockquote>
<p><em>"On a scale of 0 (completely implausible) to 10 (completely plausible), please rate:</em></p>
</blockquote>
<ul>
<li>
<p><em>'Flugblogs' as a name Kellogg's would give to a new breakfast cereal.</em></p>
</li>
<li>
<p><em>'Flugblogs' as the name of a new computer company.</em></p>
</li>
<li>
<p><em>'Flugblogs' as the name of big, air-filled bags worn on the feet and used to walk on water.</em></p>
</li>
<li>
<p><em>'Flugly' as the name a student might give its favorite teddy bear.</em></p>
</li>
<li>
<p><em>'Flugly' as the surname of a bank accountant in a W.C. Fields movie.</em></p>
</li>
<li>
<p><em>'Flugly' as the surname of a glamorous female movie star."</em></p>
</li>
</ul>
<p>By pre-testing lots of humans and machines to figure out what kinds of things people say, and machines fail to say, a simple but well-prepared machine could draw up a 'Human Subcognitive Profile'. By comparing this to the responses of players, it would be an extremely effective judge in an Inverted Turing Test. There are two reasons why this strategy would not work in a Turing Tournament:</p>
<blockquote>
<p>a) In the above specification, none of the players know which domain they will be playing in until the competition begins officially (after which the designer is barred from tweaking his machine). As a result, it would be impossible for the designer to create Human Subcognitive Profiles for every possible domain that the machine might find itself playing in a Tournament.</p>
<p>This same effect could perhaps be wrought in a standard Test by restricting the domain of conversation, but not telling the players before the competition begins what that domain will be.</p>
<p>b) In order to be successful, players have to be good as both teachers and students. As mentioned above, this is akin to holding both a standard and Inverted Turing Test. Even if the domain was known in advance, and even if it was possible to draw up a Human Subcognitive Profile for that domain somehow, such a machine would be exposed as a student.</p>
</blockquote>
<p>Lastly, <a href="http://www.ulg.aec.be/cogsci/rfrench/turing.pdf">French asks</a> whether the standard Turing Test might be modified to forbid the kind of subcognitive questions that underly its cultural and species-specific parochialism. He concludes that the kinds of questions that probe "intelligence <em>in general</em> ... are the very questions that will allow us, unfailingly, to unmask the computer".</p>
<p>He may well be right. However, it may be that moving out of the text domain will dramatically reduce the scope of possible subcognitive shibboleths that human teachers could employ. Having said that, there will still be many possibilities for rules that would place human student-players at a big advantage. For instance, in the case of the Go domain, a cunning human teacher could choose to play by the rules of Connect4, which other humans might be much quicker to fathom. In the case of some kind of Photoshop canvas domain, humans could spell out words cursively, outwitting even the most seasoned OCR software. If there's any kind of free-text involved, any of the subcognitive tricks designed for the standard Turing Test might be employed. In the case of a 3D virtual environment, human student-players will have a huge edge, though perhaps 2D or high-D worlds would level the playing field. One might hope that imaginative specification of domains could minimize such advantages, and that after 10 years of such competitions, machine programmers will almost certainly know to build in pre-loaded expert knowledge of Connect4, for instance, but the problem will clearly still remain.</p>
<blockquote>
<p>[N.B. In order to ensure that the scales aren't conversely weighted too heavily <em>against</em> human players, it seems reasonable to allow all human players the use of a laptop throughout the Tournament.]</p>
</blockquote>
<p>Maybe instead we should accept the possibility of subcognitive shibboleths, and embrace their utility instead as a means of cataloguing different kinds of conceptual schemes. There is a presumption inherent in the standard Turing Test that smartness can be measured on a one-dimensional continuum ranging from rocks to rocket scientists. In the case of the aliens that have travelled 4 million light years in a space ship built out of genetically-engineered quantum nanobits and powered by fermented mango juice, we could be pretty sure they're intelligent, even if they were never to get the hang of English. It's just that their conceptual schemes are different. In this case, we may find that there are cases where they think more like machines than like humans. Or possibly more like dolphins, or African grey parrots, or white mice or marmosets. If we're able to set up a domain in a Tournament that everyone can play in, then we can expect that human student-players may not necessarily come out on top in all respects, even within the animal kingdom. We will <a href="http://doom.princeton.edu/www/blog/TuringTournament.html#cultures">return</a> to this idea when we discuss Turing Tournaments between groups of individuals.</p>
<h3 id="devising-new-rules-and-non-linguistic-competitors">Devising new rules, and non-linguistic competitors</h3>
<p>Besides extending the domain of play beyond text, the principle innovation of the Turing Tournament is in casting every player as both student and teacher.</p>
<p>It is clear enough what is required of the student player. When the bout begins, they have some idea of the kinds of interactions, puzzles and patterns that the domain presents. By interacting with the teacher player, they have to somehow fathom what the aim (i.e. terminating criterion) of the current bout is, and attempt to satisfy that. It might involve placing pieces on the board in some complex pattern, learning the structure of a maze, guessing at the next number in a sequence or optimizing some noisy function. Depending on the tournament, they may or may not be given feedback after each move:</p>
<ul>
<li>
<p>If they're given a running score, they can attempt to learn how to maximise that reward.</p>
</li>
<li>
<p>If no reward is given, but the teacher corrects incorrect moves, then the learning by imitation can be seen as a kind of supervised mapping or reconstructive learning problem.</p>
</li>
<li>
<p>There may even be cases where no feedback is given whatsoever, such as when the bout requires the student to guess the next number in some sequence.</p>
</li>
</ul>
<p>It is the teacher's job to come up with new and inventive rules for bouts that challenge the student-players, and also to perhaps lead the student in the right direction. For the Tournament to work as intended, teachers should be intending to come up with the most discriminative bout rules they can.</p>
<p>Getting the incentive structure for the teachers right is therefore key. I expect that early scoring structures will contain loopholes that ingenious machine designers can exploit, but that over time, scoring structures that serve their purpose in a robust way will emerge. If our goal is to discriminate humans from machines, then this simple scoring system may work:</p>
<ul>
<li>
<p>If the student 'wins' (i.e. satisfies the terminating criterion) a bout, whether human or machine, then they get a point, otherwise they get nothing.</p>
</li>
<li>
<p>If a human student wins a bout, then the teacher gets a point, otherwise they get nothing</p>
</li>
<li>
<p>If a machine student wins a bout, then the teacher loses a point, otherwise they get nothing.</p>
</li>
<li>
<p>Total player score: the sum of the player's scores as a student and their scores as a teacher</p>
<p>[There may need to be some weighting/normalization if the number of human and machine players is unequal.]</p>
</li>
</ul>
<p>In effect, we're rewarding players that seem human, and can devise rules that discriminate whether other players are human. This Tournament setup is the combo standard/Inverse Turing Test described above, that would not differ all that wildly in principle from the standard Turing Test if played in a text domain. Such Tournaments would encourage the kinds of subcognitive or culturally-rooted human-parochialism that we're trying to avoid.</p>
<p>Perhaps this more general scheme will work instead:</p>
<ul>
<li>
<p>If the student wins, whether human or machine, then they get a point, otherwise they get nothing.</p>
</li>
<li>
<p>To calculate the student's score: at the end of all the bouts, count the number of bouts that each player won as a student. Calculate the mean number of bouts won. For each student, subtract this mean value from the number of bouts they won. This will mean that a very average player will have zero student points, a good player will have a positive number of points, and a poor player will actually have negative student points.</p>
<p>In other words:</p>
<blockquote>
<p>c_m = sigma_n^N( W_nm ) - sigma_n^N( sigma_m^N( W_nm )) / 2N</p>
</blockquote>
<p>where:</p>
<blockquote>
<p>c_m = the overall student score for player m</p>
<p>N = the number of players</p>
<p>W_nm = 1 if student m won in their bout with teacher n</p>
<p>W_nm = 0 if student m lost in their bout with teacher n</p>
</blockquote>
</li>
<li>
<p>To calculate the teacher's score: if the student wins a bout, then add the student's student score (which may be negative) to the teacher's teacher score. If the student loses the bout, then the teacher gets nothing.</p>
<p>In other words:</p>
<blockquote>
<p>p_n = sigma_m^N( W_nm * c_m )</p>
</blockquote>
<p>where:</p>
<blockquote>
<p>p_n = the overall teacher score for player n</p>
<p>N = the number of players</p>
<p>W_nm = 1 if student m won in their bout with teacher n</p>
<p>W_nm = 0 if student m lost in their bout with teacher n</p>
<p>[It may be that W_nm should be -1 if the student lost]</p>
</blockquote>
</li>
<li>
<p>Total player score: the sum of the player's teacher score and student score</p>
<p>[There may need be some normalization to ensure that the teacher and student scores are weighted equally.]</p>
</li>
</ul>
<p>What's the point of all this complexity? If you're a teacher, then you do best if you can design your rules such that only above-average players (whether human or machine) win in your bouts. There's actually a penalty if you make your rules so easy that everyone can figure them out, and you'll get zero points if no one can figure them out at all. When you're a student, you want to be as smart as you can, and when you're a teacher, you want to be as discriminative as you can. En masse, the community of competitors are striving to do as well as they can and to evaluate each other as well as they can.</p>
<p>Inventively devising rules to favour intelligent over non-intelligent participants requires sufficient representational power to understand, let alone manipulate, your own rules, a rich theory of mind, as well as a generative good taste. Consider a Tournament played in the simple domain consisting solely of <a href="http://www.cogsci.indiana.edu/lap.html">letterstring analogy problems</a>, where the student is faced with problems such as:</p>
<blockquote>
<p><em>"I change abc into abd. Can you 'do the same thing' to ijk?"</em></p>
</blockquote>
<p>or in non-linguistic terms:</p>
<blockquote>
<p>abc —&gt; abd; ijk —&gt; ?</p>
</blockquote>
<p>Reasonable responses include ijl, ijd, or even abd.</p>
<p>Let us imagine that a player as cunning as <a href="http://www.cogs.indiana.edu/people/homepages/hofstadter.html">Douglas Hofstadter</a> has devised the following problem:</p>
<blockquote>
<p>abc —&gt; abd; mrrjjj —&gt; ?</p>
</blockquote>
<p>Peer at this for a moment - you won't appreciate that this is somewhat fiendish unless you <a href="http://www.cogsci.indiana.edu/fcca.html">try it for a while</a> yourself. Any ideas?</p>
<p>There's no obvious pattern to the letters chosen on the right hand side, so mrrkkk seems kind of lame, and abd always feels lame. Well, how about if you try this one first:</p>
<blockquote>
<p>abc —&gt; abd; abbccc —&gt; ?</p>
</blockquote>
<p>Though your first thought may have been abbddd, doesn't abbdddd seem so much nicer? It's as though the successorship sequence of letters needs to be reflected in the increasing length of the letter groups (to use the <a href="http://www.cogsci.indiana.edu/index.html">FARG's</a> terminology). Now, let us turn back to:</p>
<blockquote>
<p>abc —&gt; abd; mrrjjj —&gt; ?</p>
</blockquote>
<p>Doesn't mrrjjjj seem like a nice, reasonable solution now? Would you have considered it so nice before the previous example. Probably almost as nice. Would you have thought of it on your own, without the previous example? Probably, given some head-scratching.</p>
<p>The point of this digression is to point out how an imaginative teacher can guide, plant ideas, manipulate, prime, coax and lead the student by example in such a way that an intelligent player would almost certainly get the right answer, but there are almost no <a href="http://www.cogsci.indiana.edu/copycat.html">extant</a> machines that would stand a chance. Besides having the sheer representational flexibility to deal with even barebones analogies such as the one above, a really intelligent player would be using the first few turns to gauge the teacher, get a sense of what kinds of solutions are admissible, and would probably be relying on <a href="http://www.ux1.eiu.edu/%7Ecfbxb/class/1900/prag/grice.htm">Gricean maxims</a> wherever possible.</p>
<p>What if your alien doesn't know anything about Gricean maxims? Or doesn't understand concepts like tournaments, rules, intelligence, machines, scores or games? We've finished outlining how a Tournament might be run that might require less domain knowledge and linguistic ability than the standard Turing test. But one striking pragmatic problem remains, which becomes apteacher when we consider our newly-arrived green visitor. If the alien doesn't speak English, how are we going to explain the idea of the Turing Tournament to him so that he can participate?</p>
<p>Following <a href="http://web.media.mit.edu/%7Eminsky/papers/AlienIntelligence.html">Minsky</a>, I think that we will be able to converse with aliens to some degree, provided they are motivated to cooperate, because we'll both think in similar ways in spite of our different origins. Every evolving intelligence operates within spatial and temporal constraints, suffers from a scarcity of resources (and presumably, competition), must develop symbols and rules, and must have thought about computation and machine learning in order to be able build spaceships. Perhaps notions of games, intelligence, scores and tournaments are only relevant in a society of individual entities that compete with each other for resources, and that maybe a hive mind or single monolithic being or other <a href="http://www.amazon.co.uk/exec/obidos/ASIN/1857988078/202-0063501-3220649">unimaginable entity</a> might not need such concepts. In that case, you wouldn't have any more luck using the standard Turing Test on such a being.</p>
<p>Will we have much more luck with machines? Not unless we start small. At the moment, the state of the art in artificial intelligence wouldn't do very well in most of the domains we've discussed, and would struggle especially when trying to generate new rules of its own. Sadly, very few researchers have focused on generative heuristics to <a href="http://doom.princeton.edu/www/outbursts/copycat/curiousmachines_analogymaking_copycat.pdf">curiously</a> <a href="http://en.wikipedia.org/wiki/Discovery_system">discover</a> things that are interesting simply for their own sake, such as <a href="http://en.wikipedia.org/wiki/Doug_Lenat">Lenat</a>'s <a href="http://en.wikipedia.org/wiki/Automated_Mathematician">Automated Mathematician</a> that sought interesting mathematical concepts. In order to stand a chance in a Turing Tournament, much work needs to be done on curiously discovering interesting things that could serve as the basis for a rule set in a new domain. Good, that is, discriminative, rule sets for a Turing Tournament bout might consist of a difficult but ultimately guessable sequence of numbers based on a funny arithmetical pattern, or the kind of letterstring analogy problem that elicits an 'aha'. Better still, teacher players that can lead an intelligent student player down a suggestive road towards the terminating criterion through tutorial or warm-up sub-bout problems will be at a tremendous advantage, where half the problem for the student consists of figuring out what their goal is supposed to be.</p>
<h3 id="but-is-it-intelligent">But is it intelligent?</h3>
<p>Let us recall Shieber's pithy test for intelligence:</p>
<blockquote>
<p><em>"Now, how do you tell if something is intelligent? You compare it with an entity postulated to be intelligent."</em></p>
</blockquote>
<p>We've replaced that with an intellectual obstacle course. As teachers devising rules for their bouts, we are effectively asking players to define their own micro-test of intelligence (since being able to do this is surely a sign of good taste?). They must then be able to convey the parameters of that test such that other intelligent student players can figure out how to pass it, perhaps by creating lead-up sub-bouts, internalizing what the student player is probably thinking and so guiding the student players' intuitions in the right direction. Finally, as students, the players must demonstrate in turn that they can flexibly assimilate what their goal should be, and then be able to get to it.</p>
<p>So although we might imagine some narrow machines that could best humans in certain kinds of puzzles or computation, but it seems less likely that a brute force machine player would also do well on Bongard problems, letterstring analogies, or be able to devise ingenious, fun and discriminative rules for bouts. This new generative aspect is intended to tap into the kind of creative, generative, playful, inventive or aesthetic faculty that humans display, as well as the ability to form a rich internal model of the student player's state of confusion, and guide them towards a solution. In this respect, it borrows the idea of a dialog or gentle interrogation from the original Test, but allows for the translation of that dialog to new domains.</p>
<p>Bringing this back to Turing's original question, we can finally ask, 'if a machine were to score higher than some of the humans in a Turing Tournament, would we definitely be willing to call it intelligent?' The answer could depend on a few factors:</p>
<ul>
<li>
<p>Let us assume that the Tournament is well-planned, that the human competitors are well-chosen, that no independent experts can find any scoring loopholes or weaknesses in the organization of the Tournament, and that the result is replicable. If any of these conditions are not met, we will not consider the Tournament to be well-run.</p>
</li>
<li>
<p>If the domain is too restrictive, then there may be a dearth of interesting rule sets that can be devised. In this case, a good player won't do much better than a poor player, and this wouldn't be an interesting result.</p>
</li>
<li>
<p>Even if the domain is a rich one, such as letterstring analogy problems, it could be that a highly specialized program like Copycat could outperform many humans. Unless the success is relatively domain-general, then you've shown what you probably knew already, i.e. the machine is exhibiting some domain-specific proto-intelligence.</p>
</li>
<li>
<p>At that point, we would probably want to analyze the machine's performance. Did it do better as a teacher or student? Was it simply very good at certain kinds of problems? Was there some simple trick to its way of devising problems which, once exposed, would clue in future human players in a rerun of the Tournament?</p>
</li>
<li>
<p>Could it pass a standard Turing Test?</p>
</li>
</ul>
<p>Let us imagine that a machine is designed which is a poor teacher player, but a good student player, particularly in a couple of abstract limited-interaction domains like letter strings, number sequences, Go boards and cryptography, but that it can't pass the Turing Test. Is it intelligent? Somewhat? We've forfeited the no-frills and no-free-parameters yes/no answer that you get from a Turingf Test, but we now have a much richer set of data with which to try and place this machine in the space of all possible minds. We have a more finely-graded multi-dimensional scale. Our machines can bootstrap themselves by competing amongst themselves without human intervention - specialist teacher machines that are good at discovering generative heuristics can be used to train specialist student machines that are good at problem solving, and vice versa. So in forfeiting our neat yes/no answer, we've gained a great deal.</p>
<p>Perhaps most importantly for the field of AI, we can now attempt to scale the enormous subcognitive iceberg of the mind <em>incrementally</em>, using ever more complex Turing Tournaments as yardsticks. In time, perhaps this will lead back towards the Turing Test as the final summit.</p>
<hr/>
<p>see also: <a href="http://doom.princeton.edu/www/blog/AlienIntelligenceLinks.html">AlienIntelligenceLinks</a></p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/the-turing-tournament-a-proposal-for-a-reformulation-of-the-turing-test/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>When I am famous, I will decline interviews</title>
      <link>https://www.gregdetre.com/blog/when-i-am-famous-i-will-decline-interviews/</link>
      <description>Famous actors' glossy interviews fill the author with *existential dread*—conversations stripped of context, distilled by strangers into caricature.</description>
      <content:encoded><![CDATA[<p>Reading the 5-page staged and glossy magazine interview in a hotel room with a famous actor has always filled me with a peculiar kind of existential dread. There's something a little horrifying about an hour of conversation in cold type, bereft of the intonation, expression, context and rapport that make anything one says out loud bearable. And at the end of it all, to be distilled, distorted, interpreted and weighed by the pen of a stranger... Who could have the strength of character to read about but not become their own caricature?  </p>
<p>In contrast, the last page of the Sunday Times magazine features 'a life in the day of' a happy array of personalities and professions. I like the concreteness of a single day as a window into someone else's micro challenges and achievements. I realize that these days are probably fictionalized composites - but fiction makes for a sweet, concentrated and memorable pill. And at the end of it, there is no distillation, no weighing - just the reality of a daily rhythm.  </p>
<p>When I am famous, I will decline interviews.  </p>
<p>P.S. That said, I still remember being stopped in my tracks when a fashion photographer relative asked me sweetly 'what did you today?' in the midst of my PhD. My day had consisted of:  </p>
<ul>
<li>
<p>2 hours debugging a misplaced comma</p>
</li>
<li>
<p>so that I could finish the 3-day long project of rearchitecting my non-parametric statistics to work across-subjects</p>
</li>
<li>
<p>in order to get a better sense of whether results from the latest in a long line of experiments were actually better than chance</p>
</li>
<li>
<p>so that we could tell whether reminding people and distracting them at the same time was causing them to forget</p>
</li>
<li>
<p>to test our computational theory that half-remembering a memory actually weakens it</p>
</li>
<li>
<p>which would have deep implications for our understanding how the brain learns and self-organizes</p>
</li>
</ul>
<p>But really, I'd been comma-hunting, and it seemed hard to fit that into a the kind of response usually expected from 'what did you do today?'.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/when-i-am-famous-i-will-decline-interviews/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>Excretation over</title>
      <link>https://www.gregdetre.com/blog/excretation-over/</link>
      <description><![CDATA[<p>I just handed in my dissertation draft - <em>Weakening memories by half-remembering them</em>. This was the sunrise that birthed it (inspired by <a href="http://neurofoolishmusings.blogspot.com/2008/03/ah-theres-thesis-sun.html">neurotomfoolery</a>).</p>
<p><a href="/wp-content/uploads/2010/03/img_07412.jpg"><img alt="" src="http://blog.gregdetre.co.uk/wp-content/uploads/2010/03/img_07412.jpg?w=300"/></a></p>
<p>Mewling infant.</p>
<p><a href="/wp-content/uploads/2010/03/img_0744.jpg"><img alt="" src="http://blog.gregdetre.co.uk/wp-content/uploads/2010/03/img_0744.jpg?w=300"/></a></p>]]></description>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/excretation-over/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>Bruce Blumberg on curiosity</title>
      <link>https://www.gregdetre.com/blog/bruce-blumberg-on-curiosity/</link>
      <description>Is curiosity a single adaptive specialization or a collection of mechanisms working together? Bruce Blumberg poses this fundamental question about the nature of human curiosity.</description>
      <content:encoded><![CDATA[<p><a href="https://www.linkedin.com/in/bruce-blumberg-8a627120/">Bruce Blumberg</a> - question to think about: Is curiosity a "thing", that is, an adaptive specialization, or a collection of mechanisms that together produce what we call curiosity?</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/bruce-blumberg-on-curiosity/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>Create something shareable</title>
      <link>https://www.gregdetre.com/blog/create-something-shareable/</link>
      <description>**Create something shareable** - add to the sum total of human knowledge by building something useful, trustworthy, and worth the effort for others to discover rather than recreate themselves.</description>
      <content:encoded><![CDATA[<p>My friend <a href="http://www.jperla.com/blog/">Joseph Perla</a> is taking a year off from his undergraduate degree. Joe has created a rare opportunity for himself to spend his time any way he pleases. To be honest, I'm a little jealous of him - for a long time, I'd dreamed of living alone for a few months in a sunny, comfortable cave with high-speed internet access near a supermarket and a pub, just to think and read and write and code.  </p>
<p>We talked about how to fill this period meaningfully. It would be pretty easy to fill a year reading RSS feeds and masturbating, though this would leave you with little to show for your time. I want to have some more substantive impact on the world around me. To be able to look back on my time spent with pride. To do something worthwhile. Joe and I talked about how to decide what projects would be worthwhile, and this was the metric I suggested:  </p>
<p>Create something shareable.  </p>
<p>Add to the sum total of human knowledge. Create something that others can use.  </p>
<p>To be shareable, it has to be useful. It has to be trustworthy - if it's flawed/buggy, or its conclusions are ill-founded then it could actually be of negative value. It's probably going to require some effort, since if it was easy and quick, then it would be easier for other people to generate it anew than to internalize your solution. It probably needs to synthesize or improve upon what already exists, maybe surprising us or overturning existing intuitions.  </p>
<p>The internet provides the medium for distribution, advertisement and collaboration. Commoditized hardware, open source software and freely available information provide the tools and raw materials.  </p>
<p>This validates a whole host of online activities. <a href="http://krugman.blogs.nytimes.com/">Write carefully considered opinion pieces</a>. <a href="http://dilbertblog.typepad.com/">Build up a corpus of entertaining posts to create an online persona</a>. <a href="http://kottke.org/">Gather interesting tidbits</a>. <a href="http://www.gapingvoid.com/">Develop your art in whatever form it appears</a>. Maintain an existing piece of open source software or <a href="http://apsy.gse.uni-magdeburg.de/main/index.psp?page=hanke/debian&amp;lang=en">create a Debian package</a>. Write a new application or library. Edit wikipedia articles. Release your photos under a Creative Commons license. <a href="http://www.cscs.umich.edu/%7Ecrshalizi/notebooks/">Put your notes online</a>. <a href="http://www.deadinsect.co.uk/2007/09/jellyfish-costume.html">Provide instructions for building cool things</a>. Share your tools. Run a controlled experiment <a href="http://www.sethroberts.net/">on yourself</a>. Curate a dataset. Write a scientific paper and <a href="https://compmem.princeton.edu/psych_review_07/">make it easy for others to replicate it</a>. [These links are just some of my favourite examples of each activity].</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/create-something-shareable/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>Losing an old friend - a paean to a laptop</title>
      <link>https://www.gregdetre.com/blog/losing-an-old-friend/</link>
      <description>A dying ThinkPad forces a choice between the **MacBook Air's** sleek reliability and the **Lenovo X300's** Linux freedom—beauty versus ideology in laptop form.</description>
      <content:encoded><![CDATA[<p>My trusty, indomitable <a href="http://www-307.ibm.com/pc/support/site.wss/MIGR-46451.html">Thinkpad T40</a> is showing telltale signs of domitability. I will feel its loss like the loss of an arm I'm using to cling to a mountain. Unfortunately, my Thinkpad crashes about once a day - hangs rigidly from a carefully-tied noose, requiring a hard reboot. I could send it off again to Lenovo, but it felt like a hasty hemispherectomy with a blunt knife the last time I needed to do that.  </p>
<p>In pondering the purchase of my next outboard brain, I'm torn between the <a href="http://shop.lenovo.com/SEUILibrary/controller/e/web/LenovoPortal/en_US/catalog.workflow:category.details?current-catalog-id=12F0696583E04D86B9B79B0FEC01C087&amp;current-category-id=AB685843BDD4412BB8FAB17D26FADACF">Lenovo X300</a> and the <a href="http://www.apple.com/macbookair/">MacBook Air</a>. If I refrain from superfluous eating or personal grooming, I think I can stretch my budget to $2200, which has to include warranty, spare power cable, external DVD drive etc. [Right now, the Lenovo X300 only exists in a flash drive model for $3000, but let's assume that if I pray and masturbate hard enough, they'll introduce a cheaper model with a standard hard disk].  </p>
<p>Let's start with the MacBook Air pros. It's more beautiful than a prone supermodel, and almost as slim and weightless. Mac OS X probably works more reliably than anything else on the market, and with Parallels/Boot Camp, I should be pretty much covered for most eventualities. Keynote - nuff said. I don't need a DVD drive more than once every few months, and I can live reluctantly with the ports it offers, plus some dongles.  </p>
<p>On the con side, I don't want to reward Steve Jobs' deviously effective strategy of locking Apple consumers into an all-Apple world. I hate barriers to exit in consumer products. I think open solutions foster greater innovation in the long run. And I resent proprietary software like I resent being told what (not) to do.  </p>
<p>There's little to argue in favor of the Thinkpad specifically. I like its distinctiveness, its nipple mouse, and the fact that it has two mouse buttons and pageup/down keys.  </p>
<p>Furthermore, I really like linux - I like that things are free in every sense. I love Debian-based package management - being able to automatically install and update everything with a single command or click is so weirdly, futuristically better than the Mac or Windows approach of downloading each application manually, each of which has its own update software - so much better that non-linux users literally don't seem to believe what they're missing. Finally, I love KDE. I like the fact that I have keyboard shortcuts for everything - I wear it like I wear my 5-year old walking boots that have moulded perfectly to my feet. I especially like focus-follows-mouse and being able to effortlessly move and resize windows by holding down Alt. These alone are very nearly enough to tie me to linux indefinitely.  </p>
<p>But linux lets me down. My wireless is better than before, but it's still a little drunk and a little deaf. I lose minutes a day to niggles and imperfections (e.g. USB, sleep and projectors). I've never quite had 3D video stuff working, so I have to jump through fiery hoops to run my experiment presentation software (PyEPL). YouTube and video are pretty hit and miss. And, even after using Open Office for 2 years, I still don't like it as much as Microsoft Office.  </p>
<p>So I'm left trading off my preference for linux's user interface, package management and freeness against beautiful hardware and reliability. Perhaps I'm getting old and impatient, but the reliability is really the kicker here.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/losing-an-old-friend/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>Jimmy Carr</title>
      <link>https://www.gregdetre.com/blog/jimmy-carr/</link>
      <description>Comedian Jimmy Carr poses a provocative theological question that challenges traditional Christian beliefs about divine hierarchy and special status.</description>
      <content:encoded><![CDATA[<p>"If we're all god's children, what's so special about Jesus?"</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/jimmy-carr/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>Master Turkers</title>
      <link>https://www.gregdetre.com/blog/master-turkers/</link>
      <description>What if Amazon's Mechanical Turk could identify **Master Turkers** capable of higher-skilled work like research, programming, or creative tasks—transforming micro-work into meaningful portfolio building?</description>
      <content:encoded><![CDATA[<p>Amazon's Mechanical Turk is an amazing service where one can create a simple task that can be micro-out-sourced to many people over the web, each of whom performs a small parcel of it. For instance, if you wanted 1000 people to highlight faces in photographs, think of synonyms for words, or provide from-the-hip feedback on your website, Mechanical Turk is ideal.  </p>
<p>For reasons that are unclear to me, people seem to be willing to work for far below a minimum wage performing pretty dull tasks. As an experimental psychologist, I'm torn between feelings of data lust at the number of participants I could thus thriftily recruit, and concern about the quality of their data. What kind of person is willing to engage in dull tasks that must feel meaningless from a worm's eye view? Where's the incentive to do a good job?  </p>
<p>It seems to me that there might be a market for tasks that require more effort, skill or thought, for which one would like to be able to cherry pick the participants. For this to work, you'd need a rich reputation scheme to Mechanical Turk, to pick out the Master Turks.  </p>
<p>I'm picturing myself in holidays as an undergraduate. If someone was willing to pay more £10/hour (roughly what I was earning as a medical secretary), I (or my more talented peers) would have happily:  </p>
<ul>
<li>
<p>researched historical facts for a novel</p>
</li>
<li>
<p>proofread a doctoral thesis</p>
</li>
<li>
<p>helped with market research for a business plan</p>
</li>
<li>
<p>written a catchy jingle</p>
</li>
<li>
<p>filmed a youtube video using your product</p>
</li>
<li>
<p>written a program to generate verbal reasoning or arithmetic questions for an exam</p>
</li>
<li>
<p>provided summaries of white papers</p>
</li>
</ul>
<p>You could imagine non-fixed-rate payment schemes, e.g.  </p>
<ul>
<li>
<p>a competition where the best submissions divide the spoils</p>
</li>
<li>
<p>an auction, so that more enjoyable tasks would be bid lower</p>
</li>
</ul>
<p>And, deliciously, you could create a meta peer-review system where other Master Turkers' task is to rate the submissions you've received.  </p>
<p>Stack Overflow is going to transform the programming job market by making answering people's questions satisfying, and then providing a metric of someone's expertise that will help them land a job.  </p>
<p>There have been many precedents of this kind of idea, but it seems strange that none of them have taken off. This feels like a way to demonstrate one's abilities on potentially interesting tasks that would provide a portfolio of work to supplement a job application.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/master-turkers/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>Blogging with Wordpress and Emacs</title>
      <link>https://www.gregdetre.com/blog/blogging-with-wordpress-and-emacs/</link>
      <description>A simple workflow combining Emacs, Python, and WordPress lets you write posts in your favorite editor and publish with a single command: `M-x wordpress-publish-this-file`.</description>
      <content:encoded><![CDATA[<p>When it comes to tools, I am a hedgehog rather than a fox. I like to have a small number of tools, and to know them well.</p>
<p>I recently resolved to start writing again. But I decided that I needed to sharpen my pencils first.</p>
<p>I have <a href="/2007/09/12/entangling-the-ground-and-cloud/">plans on how publishing and sharing should work</a>. Grand plans. Too grand, perhaps.</p>
<p>So for now, I wrote something simple for myself. Now I can type away, press buttons... publish.</p>
<p>If you like Emacs, Python and Wordpress, this might be interesting to you too. If not, it certainly won't be.</p>
<p><a href="https://github.com/gregdetre/wordpress-python-emacs">wordpress-python-emacs GitHub repository</a></p>
<p>Most of the work is being done by this great <a href="https://github.com/maxcutler/python-wordpress-xmlrpc">Python/Wordpress</a> library. Thank you.</p>
<p>I wrote some simple Python scripts. One grabs all my existing blog posts. One looks through their titles, and checks them against the filename to see if this is a new post.</p>
<p>And then there's a very simple Emacs function that calls them to save/publish the current text file.</p>
<p>I could add more things: deleting posts, or a proper workflow for moving from draft to published. Maybe later.</p>
<p>I wrote this post, then hit M-x wordpress-publish-this-file.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/blogging-with-wordpress-and-emacs/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>Breaking the seal</title>
      <link>https://www.gregdetre.com/blog/breaking-the-seal/</link>
      <description>After years of procrastination, a simple context shift finally unlocked the floodgates: *"Writing's like peeing - once you break the seal, the words just spill forth all evening."*</description>
      <content:encoded><![CDATA[<p>I've wanted to try daily writing for an impossibly long time, but the first words didn't want to be dragged out.  </p>
<p>In my case, they were unstoppered by a day in London. I pinballed from train platforms to coffee shops, oblivious bustle all around me, far away from the furrowed-browed finger-pecking at Memrise HQ. That context-shift provided a firebreak from the quotidian, and I was finally in the mood to mentally roll up my sleeves and rub my hands together.  </p>
<p>Writing's like peeing - once you break the seal, the words just spill forth all evening.  </p>
<p>I was able to decant a dozen half-thoughts that I queued up like toy soldiers, to be birthed one by one over the following week. It's now rather fun to receive a blog post from my previous self every day.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/breaking-the-seal/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>Neuroscience notes (OBSOLETE)</title>
      <link>https://www.gregdetre.com/blog/neuroscience-notes/</link>
      <description>PhD qualifying exam notes transformed into a **wiki-style neuroscience resource** with hyperlinked brain regions, tracts, and disorders—though some pages remain sparse in this *gargantuan task*.</description>
      <content:encoded><![CDATA[<p><em>[I've taken these down, because they weren't good enough, and there are so many better resources now.]</em></p>
<p>The neuroscience notes for my PhD qualifying exam are now online as <a href="/notes/neuro_generals/">browsable individual webpages</a> (originally written with <a href="http://www.mwolson.org/projects/EmacsMuse.html">Emacs Muse</a>).</p>
<p>The notes are in wiki form - in other words, I tried to give every brain region, tract, disorder, function and topic its own page. Emacs Muse automatically created the hyperlinks as I was typing (thanks to <a href="https://psychology.as.virginia.edu/people/sederberg">Per Sederberg</a>'s implicit linking patch. In reality, many of the pages are missing or sparse, since this is a pretty gargantuan task. There's also the possibility that things are inaccurate. For instance, I'm pretty sure there's some confusion regarding the nucleus reticularis vs nucleus reticularis pontis oralis, since I didn't initially realize that they're distinct brain regions with similar names...  </p>
<p>Anyway, you're welcome to use, modify and distribute these in any way you'd like, though I'd appreciate a shout-out if you do. Consider them to be released with a <a href="http://creativecommons.org/licenses/by-sa/3.0/">Creative Commons Attribution 3.0 license</a>, though I've been lazy and not included the license file. I'd doubly appreciate hearing about any flaws or confusions you find. I'm still working on these, and so I may one day release an updated version.  </p>
<p>Obviously, to some degree this is reinventing the wheel. Most of these notes are from <a href="http://www.amazon.com/Principles-Neural-Science-Eric-Kandel/dp/0838577016">Kandel &amp; Schwarz</a> (4th edition), and you can find most of this stuff online, e.g. the wikipedia has some good pages on the <a href="http://en.wikipedia.org/wiki/Brain">brain</a>, though the usual caveats apply. I also learned a lot about the pros and cons of wikifying knowledge along the way, but that's the topic of another post.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/neuroscience-notes/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>Pluviocabulary (rain words), Memrise-style</title>
      <link>https://www.gregdetre.com/blog/pluviocabulary-rain-words-memrise-style/</link>
      <description>From *petrichor* (the smell of first rain) to *tirl* (rain's sound on rooftops), exploring the rich vocabulary of precipitation reveals language as nuanced as weather itself.</description>
      <content:encoded><![CDATA[<p>I got distracted by the <a href="http://www.nytimes.com/interactive/2011/04/03/opinion/20110403_schott.html">NY Times' Pluviocabulary list</a>, and found myself <a href="http://www.memrise.com/cave/?iset=pluviocabulary-new-york-times-schott">playing with my new words</a> - just thought I'd share. I particularly enjoyed being reminded of <a href="http://www.memrise.com/item/40834/petrichor-the-smell-accompanying-the-first-rain-af/?set=pluviocabulary-new-york-times-schott">petrichor</a>, and learning <a href="http://www.memrise.com/mem/91054/oh-no-i-got-caught-in-a-flaught/">flaught</a> and <a href="http://www.memrise.com/item/40852/tirl-the-sound-of-rain-on-a-roof/?set=pluviocabulary-new-york-times-schott">tirl</a>.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/pluviocabulary-rain-words-memrise-style/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>Eroding our minds</title>
      <link>https://www.gregdetre.com/blog/eroding-our-minds/</link>
      <description>Advertising works by exploiting **cognitive fluency**—the more familiar something feels, the more we like it. *Like the banks of a river worn smooth by ceaseless flow, advertising erodes our minds.*</description>
      <content:encoded><![CDATA[<p>I said that I thought <a href="http://gregdetre.blogspot.com/2010/10/i-refuse-to-consider-advertising-based.html">"there's something irresponsible about making money from advertising"</a>.  </p>
<p><a href="http://pulchrifex.wordpress.com/">Matt Weber</a> was right to point out that although people hate the idea of targeted ads, they can be genuinely useful. Though I don't think a very large proportion of the available advertising real estate offers the possibility for really great targeting.  </p>
<p>[Of course, good advertising can be an art form in itself. And by funding most of our software and reading materials, advertising adds tremendous value to our lives.]  </p>
<p>But even on the internet, most advertising still feels as though it's about <em>increasing our familiarity with the brand</em>.  </p>
<p>Think of advertising in terms of <em><a href="http://www.boston.com/bostonglobe/ideas/articles/2010/01/31/easy__true/?page=full">cognitive fluency</a></em>, i.e. how easy we find something to process. There are lots of ways to make something fluent - make it easy to read, easy to pronounce, write it in a simple font, or in high contrast.  </p>
<p>Things that are fluent (easy to process) get processed faster. We tend to like fluent things better, find fluent statements more valid. We think companies with fluent names are more valuable.  </p>
<p>Advertisers have (implicitly) known this for a long time. By incessantly dinging our minds with an advert over and over, we are gently having that brand branded upon our minds, making it easier to process, more familiar, and making us unwittingly and unjustifiedly like it more. Like the banks of a river worn smooth by the ceaseless flow, advertising erodes our minds.  </p>
<blockquote>
<p><a href="http://www.metafilter.com/95152/Userdriven-discontent#3256046"><em>If you are not paying for it, you're not the customer; you're the product being sold.</em></a></p>
</blockquote>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/eroding-our-minds/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>We should de-stigmatise suicide, our innermost freedom and right</title>
      <link>https://www.gregdetre.com/blog/we-should-de-stigmatise-suicide-our-innermost-freedom-and-right/</link>
      <description>My greatest fear has never been dying—it's not being able to die if I want to.</description>
      <content:encoded><![CDATA[<p>My greatest fear has never been dying. It is and remains not being able to die if I want to.</p>
<p>I fear torture. I fear solitary confinement. I fear dementia. I fear the slow and painful breakdown of my body. I fear dependence and becoming a burden. I fear debilitating depression.</p>
<p>What is there to fear about death, nothingness itself? Maybe an end to joys, a sense of opportunity costs and failed potential, the pain our loved ones might feel at losing us... but these are the preoccupations of the living, not the non-existent.</p>
<p>As a society, we should support people's right to die at a time and in a manner of their choosing. We should try and help them make the decision - we should, of course, encourage them not to die, especially if there is hope for a good life, and provide assistance in building back up towards that. We should create checks and balances to avoid people being manipulated or coerced into choosing to die. We should provide support, both psychological and financial for those they leave behind. We should, in short, legalise suicide.</p>
<p>A leap further, we should <em>de-stigmatise</em> suicide. This is not to say that people should be told to die any more than people should be told to change their gender - only that they should be able to, be educated and supported in deciding to, and not stigmatised if they do.</p>
<p>The freedom to die is our innermost freedom.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/we-should-de-stigmatise-suicide-our-innermost-freedom-and-right/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>Soft and hard skills</title>
      <link>https://www.gregdetre.com/blog/soft-and-hard-skills/</link>
      <description>After years focused on management over coding, wrestling with TensorFlow raises a familiar question: **was this always this hard, or have I gone soft?**</description>
      <content:encoded><![CDATA[<p>PUBLISHED https://medium.com/@gregdetre/getting-soft-or-just-soft-skills-fd73146fe477#.pbmzfexl9</p>
<p>I'm sitting with a glass of red wine and a full-screen Jupyter Notebook window full of TensorFlow code open. Now, it may be the red wine, but it's just plain difficult to get things to work.</p>
<p>After more than a PhD's worth of wrestling with code and algorithms, I felt up to the challenge. But in between, there's been a few years where the emphasis has been on management rather than code.</p>
<p>So now, I find myself wondering whether it was always this hard, or whether my focus on soft rather than hard skills has left me, well, soft.</p>
<p>After a few months of cognitive behavioural therapy, I think I'm able to recognise this thought as a distortion. Yes, I'm sure I am a little rusty. But it doesn't take much to remind myself that this stuff was always difficult. Indeed, if it wasn't difficult, it wouldn't be satisfying. And if wasn't difficult, everyone would be doing it.</p>
<p>I think probably what I miss is the sense of flow, born of hard-won familiarity. The reality of management is that there's just less time in the day for developing that muscle-memory readiness. </p>
<p>So, I persevere, inch by inch, and treasure the moments of flow when they come.</p>
<p><em>(first published on <a href="https://gregdetre.medium.com/getting-soft-or-just-soft-skills-fd73146fe477#.pbmzfexl9">Medium</a>)</em></p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/soft-and-hard-skills/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>John Cunningham Lily, RIP</title>
      <link>https://www.gregdetre.com/blog/john-cunningham-lily-rip/</link>
      <description>A tribute to consciousness researcher John C. Lilly inspires a humorous reflection on alternative paths to exploring the nature of human awareness.</description>
      <content:encoded><![CDATA[<p>According to the <a href="http://en.wikipedia.org/wiki/John_C._Lilly">wikipedia</a>:</p>
<blockquote>
<p>"[John Cunningham Lilly] is best remembered as a pioneer researcher into the nature of consciousness using as his principal tools the isolation tank, dolphin communication and psychedelic drugs, sometimes in combination."</p>
</blockquote>
<p>A noble epitaph. I hope to be remembered as "a pioneer researcher into the nature of consciousness using as my principal tools drugs, dirty dancing and pounding techno music, sometimes in combination".</p>
<p>[Thanks to Sara Szczepanski for pointing this out]</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/john-cunningham-lily-rip/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>Switching from Linux to Mac</title>
      <link>https://www.gregdetre.com/blog/switching-from-linux-to-mac/</link>
      <description>A longtime Linux user discovers the joys and frustrations of switching to macOS, finding **wonderful** hardware but some fundamentally **broken** behaviors that may never be fixed.</description>
      <content:encoded><![CDATA[<p>So, I caved. I crumbled like a biscuit in a blender. I am now the smug owner of a MacBook Air, and it really is wonderful. I went 10 days without rebooting before needing a firmware upgrade, and both sleep and wireless work flawlessly.  </p>
<p>Here are the bells and whistles that make my brain and fingers happy.  </p>
<p>Spaces (OS X Leopard's virtual desktops doodad) isn't as good as KDE's, but it turns out to be fine once you turn off the <a href="http://blogs.sun.com/bblfish/entry/why_apple_spaces_is_broken">auto-swoosh</a> by typing these two commands into a Terminal:  </p>
<blockquote>
<p>defaults write com.apple.dock workspaces-auto-swoosh -bool NO  </p>
<p>killall Dock  </p>
</blockquote>
<p>Before, switching to an application (such as Firefox) that didn't have a window open already on your current desktop would cause you to be whisked randomly to the first desktop which did have a Firefox/Terminal window open. Now, you can switch to Firefox, hit Cmd-N and a window pops up, ready to go with minimal context disruption.  </p>
<p><a href="http://www.atomicbird.com/mondomouse/">MondoMouse</a> makes it easy to move and resize windows. By default, moving a window involves awkward trips up to the title bar. The tiny resizing handle is even more awkward, and more awkwardly situated, and is often overpowered by an overeager Dock. Now, I just have to hold down Cmd and move the mouse to move the active window. Holding down Cmd and Alt resizes. Again, this doesn't work quite as nicely as KDE - for instance, the windows don't snap satisfyingly into place when they get close to other windows or the edge of the screen. MondoMouse is available for a trial period, after which it costs $15. </p>
<p>Various other niceties deserve a mention. I currently prefer the <a href="http://www.emacswiki.org/cgi-bin/wiki/CarbonEmacsPackage">Carbon Emacs</a> over <a href="http://aquamacs.org/">Aquamacs</a>, though there's not much to separate the two. <a href="http://docs.blacktree.com/quicksilver/quicksilver">Quicksilver</a>, Expose and hot corners are great. For Dashboard widgets, I'd recommend <a href="http://www.islayer.com/index.php?op=item&amp;id=7">iStat Pro</a>, <a href="http://www.apple.com/downloads/dashboard/reference/wordoftheday.html">Word of the Day</a> and maybe <a href="http://www.apple.com/downloads/dashboard/music/albumartwidget.html">Album Art</a>. Adding <a href="http://code.google.com/p/cdto/">cdto</a> to Finder is handy. <a href="http://skim-app.sourceforge.net/">Skim</a> is the way forward for reading PDFs in full screen. <a href="http://www.robbiehanson.com/alarmclock/index.html">Alarm Clock</a> for timers. <a href="http://cyberduck.ch/">Cyberduck</a> for accessing other computers over SSH (along with <a href="http://code.google.com/p/macfuse/">MacFuse and SSHFS</a>).  </p>
<p>Some things are just broken on the mac, and will probably never be fixed. Steve Yegge has described his <a href="http://steve-yegge.blogspot.com/2008/04/settling-osx-focus-follows-mouse-debate.html">heroic failure</a> to civilize the mac's shiny silvery savagery with proper focus-follows-mouse behavior. In general, OS X's switching behavior is wrong in a few respects. The application (rather than the window) is the wrong level at which to switch, and so Cmd-Tab is always jarringly confusing to use. There are still kinks with the way that windows activate and bring themselves to the fore, especially when multiple desktops are involved, and I seem to lose a modal dialog to this problem about once every few days. <a href="http://www.macports.org/">MacPorts</a> and <a href="http://www.finkproject.org/">Fink</a> are workable but still disappointing - having two package systems devalues both, downloading binaries is preferable to compiling every time, and the packages are generally patchier than Debian's <a href="http://www.debian.org/doc/manuals/apt-howto/">apt-get</a>. Finally, X11 is very clearly a second-class citizen, but works tolerably well. Oh, and unbelievably, the regular expressions engine in Python 2.5 appears to be <a href="http://bugs.python.org/issue1160">maddeningly sprained</a>!  </p>
<p>There were a few pleasant surprises. Two-finger two-dimensional scrolling is unutterably wonderful. The MacBook Air's colossal touchpad is very pleasant to use, even for nipple-lovers. OS X applications are pretty consistent about keyboard shortcuts. You can re-learn most muscle memories without too much effort so long as you don't have to switch back and forth between the old and the new too much in the first few weeks. Emacs is still Emacs.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/switching-from-linux-to-mac/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>Lawrence Lessig</title>
      <link>https://www.gregdetre.com/blog/lawrence-lessig/</link>
      <description><![CDATA[<p>on <a href="http://lessig.org/freeculture/free.html">free culture</a> (from <a href="http://presentationzen.blogs.com/presentationzen/2005/10/the_lessig_meth.html">PresentationZen</a>)</p>]]></description>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/lawrence-lessig/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>The real reason we can’t define 'Artificial General Intelligence'</title>
      <link>https://www.gregdetre.com/blog/the-real-reason-we-cant-define-artificial-general-intelligence/</link>
      <description>Could it be that **any clean, broad definition of general intelligence would exclude *us***? Our intelligence is only somewhat general, existing on a continuum rather than being binary.</description>
      <content:encoded><![CDATA[<p>Could it be that the real reason we struggle to define Artificial General Intelligence (AGI) is that <strong>any clean, broad definition of general intelligence would exclude <em>us</em></strong>?</p>
<p>Intelligence is hard to pin down, but I like these two related definitions:</p>
<ol>
<li>
<p>Flexible, goal-directed behaviour</p>
</li>
<li>
<p>Efficient skill acquisition (from Francois Chollet)</p>
</li>
</ol>
<p>I'm going to reformulate these as: <em>learning how to do something without much to go on, and then applying it in new ways</em>.</p>
<p>How do we measure up against this?</p>
<p>Yes, we can certainly acquire new skills that we didn't evolve for. We are comparatively flexible. The AIs that <em>are</em> super-human, like AlphaZero, tend to be pretty narrow in applicability.) And yes, our learning is comparatively sample-efficient. (For every book that a human child has read, GPT-4 has read 10,000.) We can occasionally generalise out of distribution in ingenious ways.</p>
<p>But. We struggle to transfer knowledge outside the learned context. We make mistakes even when we know better or in principle. We struggle to learn new concepts, languages, skills, habits - we struggle to change our ways, even when we want to. We behave sub-optimally, both wittingly and unwittingly, even when it's pointed out. In short, we struggle to learn how to do some things, and then we struggle to apply them.</p>
<p>So, our intelligence is more general than the best AIs in 2024, by a good distance. But 'general intelligence' is a continuum rather than binary, and our intelligence is only somewhat general.</p>
<p>In practice, the only clean definition in AI is for superintelligence - "better than the best of us at everything". And I think that's probably still a way off.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/the-real-reason-we-cant-define-artificial-general-intelligence/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>On patents as a business tool</title>
      <link>https://www.gregdetre.com/blog/on-patents-as-a-business-tool/</link>
      <description>Patents aren't just legal protection—they're **deterrents** that make large companies more likely to partner with or acquire you rather than compete directly.</description>
      <content:encoded><![CDATA[<p><a href="http://answers.onstartups.com/questions/155/how-do-you-protect-the-key-features-of-your-product-without-a-patent/851#851">This was posted as a comment here</a>:</p>
<p>After doing some early research into patents, I concluded that:</p>
<ul>
<li>
<p>they'd cost me $10k or so per patent</p>
</li>
<li>
<p>this would be a huge amount of time and effort for me</p>
</li>
<li>
<p>it would take at least a couple of years for the patents to be granted</p>
</li>
<li>
<p>I wouldn't have the money to enforce them</p>
</li>
<li>
<p>small companies would probably ignore them anyway</p>
</li>
</ul>
<p>So I was confident in asserting that patents were a waste of time (for my needs).</p>
<p>In the 6 months since, a few things have conspired to change my mind:</p>
<ul>
<li>
<p>we're close to raising investment, and investors care about patents because they provide evidence of value</p>
</li>
<li>
<p>likewise, they make you more acquirable</p>
</li>
<li>
<p>I've started to realize that patents' primary value is as a <em>deterrent</em> against large companies who might otherwise lumber into competition</p>
</li>
</ul>
<p>It's this final point - that patents provide a watered-down 'mutually assured destruction' kind of deterrent against incumbents and larger companies that seems most important now. If you can protect yourself with patents, you make it much more likely that larger companies will partner with, license from or acquire you, rather than compete with you.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/on-patents-as-a-business-tool/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>Reading: 1984 + 20 = 2004. coincidence?</title>
      <link>https://www.gregdetre.com/blog/reading-1984-20-2004-coincidence/</link>
      <description>A chilling examination of America's contingency plans for postponing elections, raising urgent questions about constitutional safeguards and the balance of power in crisis situations.</description>
      <content:encoded><![CDATA[<p>This is one of the most frightening articles I've ever read. I haven't got round to checking the facts, and presumably it only makes sense to have a contingency plan. But I'd want it to be a checked and balanced contingency plan, and since this is America, there must be something in the constitution that deals with this kind of situation and imposes a time constraint or requires input from the houses.</p>
<p><a href="http://www.kuro5hin.org/story/2004/7/11/221525/810">http://www.kuro5hin.org/story/2004/7/11/221525/810</a></p>
<p>"The election is a necessity. We cannot have a free government without elections; and if the rebellion could force us to forgo, or postpone, a national election, it might fairly claim to have already conquered us."-- Abraham Lincoln, 1864</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/reading-1984-20-2004-coincidence/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>The iPhone apps my cold, dead hands would cling most rigidly to</title>
      <link>https://www.gregdetre.com/blog/the-iphone-apps-my-cold-dead-hands-would-cling-most-rigidly-to/</link>
      <description>From **Instapaper** for reading queues to **Snaptell's** book-scanning wizardry, these essential iPhone apps prove indispensable for modern digital life.</description>
      <content:encoded><![CDATA[<ul>
<li>
<p><a href="http://www.instapaper.com/">Instapaper</a> - combine this with the <a href="https://chrome.google.com/extensions/detail/fldildgghjoohccppflaohodcnmlacpb">Instachrome</a> extension, and whenever I see a webpage I want to read later, it'll be waiting with me as I wait for a train</p>
</li>
<li>
<p>Light - it's bright! No more torches. If you lived in Hanborough, you'd need this too.</p>
</li>
<li>
<p>Trainline - faster than my laptop and/or a speeding bullet for checking train times in the UK</p>
</li>
<li>
<p>PlainText - write notes on your laptop, have them appear on your phone instantly through Dropbox and vice versa. Oh, and <a href="http://dropbox.com/">Dropbox</a> of course, too.</p>
</li>
<li>
<p>Dictionary.com - etymologies, pronunciations, the works.</p>
</li>
<li>
<p>Remote - control Keynote presentations from your phone.</p>
</li>
<li>
<p>Glympse - let other people know where you are.</p>
</li>
<li>
<p>Skype - I can call Mia for free while walking the streets</p>
</li>
<li>
<p>iTrans Tube and Tube Status for planning London Underground journeys</p>
</li>
<li>
<p>Snaptell - red laser black magic. Point at a book, and have elves whisper about it to you.</p>
</li>
<li>
<p>Angry Birds - the most popular mobile game of all time.</p>
</li>
<li>
<p>Spotify - all the music in the world on the go. Requires a Spotify subscription.</p>
</li>
<li>
<p>Shazam - tells you the name of songs that are currently playing.</p>
</li>
</ul>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/the-iphone-apps-my-cold-dead-hands-would-cling-most-rigidly-to/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>LTSF 2019</title>
      <link>https://www.gregdetre.com/blog/ltsf-2019/</link>
      <description><![CDATA[<p>Hi everyone. I’ll be at <a href="https://www.learningtechnologies.co.uk/learning-tech-summer-forum/ltsf-conference">LTSF 2019 (Learning Technologies Summer Forum)</a> in London on 11th July 2019.</p>
<p>If you’re interested in the slides, <a href="https://www.linkedin.com/in/gregdetre/">drop me a line on LinkedIn</a>.</p>]]></description>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/ltsf-2019/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>How to remember names at a party</title>
      <link>https://www.gregdetre.com/blog/how-memory-works/</link>
      <description>The biggest reason we forget names isn't poor memory—it's that we weren't paying attention in the first place while worrying about spinach in our teeth.</description>
      <content:encoded><![CDATA[<p>(First published at <a href="https://www.yourheights.com/blogs/health/how-memory-works">Heights</a>)</p>
<p>Can't remember the name of the person you just met at your party? We can help with that.</p>
<h2 id="why-do-we-forget-things">Why do we forget things?</h2>
<p>There are many reasons why we forget things. Let's consider a minor but excruciating example - even though you only met them ten minutes ago, you’ve forgotten the name of the person you’re talking to at a party and you know you’re going to need to introduce them.</p>
<p>Fortunately, there’s a good deal we can do about this if we choose. Perhaps the biggest reason we forgot their name was that we simply weren't paying enough attention in the first place.</p>
<p>While they were introducing themselves, we were focused on whether there’s spinach stuck in our teeth, the right stance to best show off our paunch, and what we’re going to say next. (Find out <a href="https://www.yourheights.com/blog/braincare-club-befriend-your-inner-critic-to-strengthen-your-confidence/">how to silence those critical thoughts</a> ).</p>
<p>In that flurry of self-conscious unawareness, our attention was elsewhere. Like a fragile snowflake turning immediately to slush on tarmac, the memory never got a chance to land, let alone settle.</p>
<h2 id="so-what-can-we-do">So what can we do?</h2>
<p>Well, we can follow Dale Carnegie’s immortal, self-serving advice to use their name in conversation as soon as possible… “So, Bill, how are things going at work, Bill?”. This forces us to listen. And more importantly, to use a name is to practice remembering it. This is known as <em>active recall</em>.</p>
<p>So, you’ve repeated their name to yourself soon after hearing it, even if only in your own head. That brings us to the second thing about memory. Memories need repetition in the same way that seeds need nourishment in order to grow.</p>
<p>At first, they benefit most from little reminders often. Once you’ve known something for a long time, those reminders can grow further apart. Tolkien said it best: <em>deep roots are not reached by the frost.</em></p>
<h2 id="how-to-boss-it">How to boss it</h2>
<p>So, if you really wanted to do a better job of remembering names, then you'd remind yourself almost instantly, and then right after the conversation ended, and then maybe on the way home, and then after a few days, and a few weeks... Schedule those intervals between reminders to grow exponentially for optimal recollection.</p>
<p>If we wanted to go further, we could bring out the big guns, the sort of mnemonic munitions that competitive memorisers rely on - <em>vivid mnemonics</em> .</p>
<h2 id="vivid-mnemonics">Vivid Mnemonics</h2>
<p>My name is Greg. Take a look at my face. It’s a punchable face, to be sure, but ignore that for now.</p>
<p>Try an auditory association - imagine me as Gr<em>eg</em> with a wooden p<em>eg</em> l<em>eg</em>. In other words, make a rich, vivid, visual association between the person’s face and their name. Caricature their face, distort their name, and build a bridge between them. The more obscene, funny, ridiculous or sexual you can make your image-bridge, the better.</p>
<p>And though it seems like quite a lot of work to go around imagining Jemima wearing ludicrous pyjamas, or Sam covered in sand, it will double the likelihood of remembering their name later.</p>
<h2 id="remember-what-you-just-read">Remember what you just read?</h2>
<p>So, listen in the first place, actively recall soon after, and remind oneself periodically. Beyond that, the trade-off between effort and benefit becomes less clear. Really effective mnemonic techniques involve a good deal of concentration and imagination.</p>
<p>You have to make a lot of effort if you want to be able to reliably bed down lots of names. You may be better off focusing your energies on being a good listener and interlocutor, or on being more accepting of your own faults, or on assembling a roster of urbane self-deprecating apologies to smooth over the occasional inevitable memory hiccup.</p>
<p>And if you've still not won over your new friend, <a href="https://www.heights.com/blogs/health/the-science-of-smiling?srsltid=AfmBOorGR_o4_ly6ChP-ZG1kFuNYjOM3nzJCNi0p4pb_R11NwUoMcYFb">just give them a smile</a> —at the very least you will feel great.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/how-memory-works/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>Google goes public</title>
      <link>https://www.gregdetre.com/blog/google-goes-public/</link>
      <description>Beyond reasoned analysis lies **faith in Google's potential**—a company whose motto includes *"you can make money without doing evil"* and *"great just isn't good enough."*</description>
      <content:encoded><![CDATA[<p>I didn't buy any <a href="http://slashdot.org/article.pl?sid=04/08/19/1216237&amp;tid=217&amp;tid=98&amp;tid=1">Google shares</a> because I don't have much money to risk, but I think I would have if I had. I have faith in Google, that goes beyond the reasoned.  </p>
<p>[  </p>
<p>](http://slashdot.org/article.pl?sid=04/08/19/1216237&amp;tid=217&amp;tid=98&amp;tid=1)  </p>
<p>Google motto  </p>
<p>- </p>
<p>Never settle for the best  </p>
<p>Focus on the user and all else will follow  </p>
<p>It's best to do one thing really, really well  </p>
<p>Fast is better than slow  </p>
<p>Democracy on the web works  </p>
<p>You don't need to be at your desk to need an answer  </p>
<p>You can make money without doing evil  </p>
<p>There's always more information out there  </p>
<p>The need for information crosses all borders  </p>
<p>You can be serious without a suit  </p>
<p>Great just isn't good enough</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/google-goes-public/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>Shameless bid to get a Gmail account</title>
      <link>https://www.gregdetre.com/blog/shameless-bid-to-get-a-gmail-account/</link>
      <description><![CDATA[<p>Apparently, active Blogger users get access to Gmail, so it's time to blog something.</p>]]></description>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/shameless-bid-to-get-a-gmail-account/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>Losing weight</title>
      <link>https://www.gregdetre.com/blog/losing-weight/</link>
      <description><![CDATA[<p>Whenever I want to lose weight, all I have to do is eschew solid food, shave my head and poop bulimically. I call it the neonate diet.</p>]]></description>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/losing-weight/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>Trophic levels, trade and the Terminator</title>
      <link>https://www.gregdetre.com/blog/trophic-levels-trade-and-the-terminator/</link>
      <description>Why would superintelligent machines want to dominate humans when they'll occupy completely different ecological niches? The key may lie in finding something uniquely human to trade.</description>
      <content:encoded><![CDATA[<p>As an AI aficionado, I've had my fair share of debates about the Terminator scenario. Perhaps blindly, I'm optimistic about the possibility of being enslaved or eradicated by robot overlords. Here's one possible response.  </p>
<p>Why would the machines be so obsessed with the idea of dominating or eradicating us? They'll occupy a completely different ecological niche. They'll almost certainly have entirely different energy/resource requirements, be free from human claustrophobia and may not even be embodied at all. They won't care if the earth runs out of resources because they'll just photosynthesise or transduce faeces. So, even if it turns out that aggression is a necessary component of intelligence, it just doesn't make sense that they'll want to wipe us off the face of the earth any more than we're determined to wipe bacteria off the face of the earth.  </p>
<p>Unfortunately though, that's a somewhat spurious parallel. We aren't in competition with bacteria - in fact, we depend on them. In contrast, there's less reason to suppose that intelligent machines will depend on us. In that case, the better parallel might be the relationship we have with chimpanzees. <em>That</em> would be more cause for concern.  </p>
<p>So the best case scenario would be mutual dependence and <a href="http://www.overcomingbias.com/2009/10/lets-not-kill-all-the-lawyers.html">integration</a> with the machines. It's no coincidence that trading partners rarely go to war with one another. However, we're going to have to find something we're better than machines at in order to have something to trade. Perhaps we could elevate captchas to a form of poetry?</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/trophic-levels-trade-and-the-terminator/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>What is wisdom?</title>
      <link>https://www.gregdetre.com/blog/what-is-wisdom/</link>
      <description>Wisdom is often about **truths whose opposite are truths** - balancing seemingly contradictory principles like self-love and self-honesty, accountability and forgiveness.</description>
      <content:encoded><![CDATA[<p>Wisdom is often about truths whose opposite are truths.</p>
<p>Wisdom is about learning from one's mistakes - without beating oneself up.</p>
<p>Wisdom is about loving yourself, forgiving yourself, accepting yourself - but being honest with yourself, listening to criticism, and looking to grow.</p>
<p>Wisdom is about expecting and holding other people accountable - but also making them feel safe and forgiving them and giving them a second chance.</p>
<p>Wisdom is about preserving our traditions and institutions - without being hide-bound and closed.</p>
<p>Wisdom is about taking advantage of technology - without letting it control you.</p>
<p>Wisdom is about appreciating what we have - without needing it to be happy. About <a href="https://www.linkedin.com/feed/update/urn:li:activity:7229592625215893504/">wanting the gold medal - without being defined by it</a>.</p>
<p>Wisdom is about taking care of those around you and yourself - but also being carefree.</p>
<p>Wisdom is about knowing and respecting one's own needs - but also the needs of others.</p>
<p>Wisdom is about being considerate of others' needs - without endlessly self-monitoring.</p>
<p>Wisdom is about being a good partner - without becoming codependent.</p>
<p>Wisdom is about knowing how to balance taking care of the person in front of you - with a million lives on the other side of the globe.</p>
<p>Wisdom is about loving yourself, and being enough - but also being dependable and loving and part of a community.</p>
<p>Wisdom is about excitement, receptiveness, and spontaneity - but also being dependable and thinking about consequences.</p>
<p>Wisdom is about zooming out, but still staying at the level of the human. Because what matters most in a human life is hard to see in atoms or galaxies.</p>
<p>Wisdom is about treating this life as precious - but also being ready to die without rancour at any moment.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/what-is-wisdom/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
    <item>
      <title>What new developments will we need for personalised, relevant one-to-one AI tutoring?</title>
      <link>https://www.gregdetre.com/blog/what-new-developments-will-we-need-for-personalised-relevant-one-to-one-ai-tutoring/</link>
      <description>Future AI tutoring may require more than prompt engineering—perhaps **custom teacher models** that learn alongside students, plus **explicit student models** to predict and optimize learning outcomes.</description>
      <content:encoded><![CDATA[<div class="toc">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#developing-a-teacher-model-thats-personalised-and-relevant">Developing a teacher model that's personalised and relevant</a></li>
<li><a href="#developing-an-explicit-model-of-the-student">Developing an explicit model of the student</a></li>
</ul>
</div>
<h2 id="introduction">Introduction</h2>
<p>In <a href="https://en.wikipedia.org/wiki/Bloom%27s_2_sigma_problem">"The 2 Sigma Problem"</a>, Bloom suggested that one-to-one tutoring could provide a phenomenal boost to students' learning. Maybe it's <a href="https://www.educationnext.org/two-sigma-tutoring-separating-science-fiction-from-science-fact/">not <em>quite</em> as big an effect as he suggested</a>, but I'm still willing to bet that the combination of personalisation, relevance, engagement, adaptive difficulty, and other benefits of one-to-one tutoring do make a huge difference, and at least some of them can be captured by generative AI. But even if we thought we did have a better approach, how could we be sure?</p>
<p>I'll try and tackle both sets of questions.</p>
<h2 id="developing-a-teacher-model-thats-personalised-and-relevant">Developing a teacher model that's personalised and relevant</h2>
<p>Let's focus on the question of how we might make a teacher model that provides very personalised, relevant tutoring to the student.</p>
<p>Relevance is rich and hierarchical: the environment, the task or context that the student is learning about right now, this particular course, their previous learning and progress, the history of interactions with the teacher, the language, the country they live in, recent news, macro changes, etc. All of these contextualise our learning.</p>
<p>This kind of rich, hierarchical, many-faceted personalisation presents challenges for a single gigantic, fixed model, such as an LLM.</p>
<p><em>Maybe</em> we can get away with just prompt engineering, feeding in an enormous dump of data about learner goals, preferences, and previous interactions, and rely on ever-larger context windows and more instructable models.</p>
<p>But I’m pessimistic about whether prompt engineering alone is enough for fine-grained, subtle, creative, optimal relevance and personalisation.</p>
<p>Firstly, there's the challenge of fitting all the necessary background into the model's context window <a href="https://www.dbreunig.com/2025/06/22/how-contexts-fail-and-how-to-fix-them.html">without overwhelming or confusing it</a>.</p>
<p>Secondly, LLMs struggle with prompts that tug against their pretraining. For example, at <a href="https://rehearsable.ai/">Rehearsable.ai</a>, it proved intractable to prompt-engineer GPT-4 to reliably and subtly follow a particular negotiation skills approach that flew in the face of the common advice found on the internet.</p>
<p>In the short-run, perhaps we can imagine hierarchies of LORA-style fine-tuning that can be layered on top of one another, for student, exam board, culture, etc. But my bet is that eventually the best AI teachers will learn too, alongside and about their students.</p>
<h2 id="developing-an-explicit-model-of-the-student">Developing an explicit model of the student</h2>
<p>Perhaps we also need to go beyond a single, main teacher model. Could it help to represent the learner with an explicit (separate?) model that assists the primary teacher model? To train such a model of the learner, we could train it to predict the learner’s behaviour, e.g. the answers they are giving, and the questions they are asking. We might then probe this model of the learner, to ask “How would the learner respond if we asked them this question?”, or to look at how it has changed over time to measure “Has the learner’s understanding of Topic X improved?”.</p>
<p>With such a model of the learner’s behaviour, we could run <a href="https://medium.com/@_michelangelo_/monte-carlo-tree-search-mcts-algorithm-for-dummies-74b2bae53bfa">Monte-Carlo Tree Search</a> or similar to simulate the effect of different teacher interventions, and pick the one that we believe will best help improve the learner’s eventual performance. In this way, we can consider relevance as exactly the content that <em>this</em> learner needs right now, in order to pass their particular exam, or indeed to unstick their current confusion. A rich model of the learner could help with choosing or generating particular problems that will help them see how to apply a new concept, develop the skill they’re missing, or correct a misconception. It could involve judicious examples, or analogies, or counter-examples.</p>
<p>So, future AI one-to-one tutoring might involve both custom teacher <em>and</em> custom student models.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/what-new-developments-will-we-need-for-personalised-relevant-one-to-one-ai-tutoring/</guid>
      <pubDate>Sun, 17 Aug 2025 16:10:37 +0300</pubDate>
    </item>
  </channel>
</rss>
