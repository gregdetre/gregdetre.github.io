<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Greg Detre</title>
    <link>https://www.gregdetre.com/blog/rss.xml</link>
    <description>Greg Detre's personal website</description>
    <atom:link href="https://www.gregdetre.com/blog/rss.xml" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>en</language>
    <lastBuildDate>Mon, 18 Aug 2025 11:13:19 +0300</lastBuildDate>
    <item>
      <title>What new developments will we need for personalised, relevant one-to-one AI tutoring?</title>
      <link>https://www.gregdetre.com/blog/what-new-developments-will-we-need-for-personalised-relevant-one-to-one-ai-tutoring/</link>
      <description>Future AI tutoring may require more than prompt engineering—perhaps **custom teacher models** that learn alongside students, plus **explicit student models** to predict and optimize learning outcomes.</description>
      <content:encoded><![CDATA[<div class="toc">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#developing-a-teacher-model-thats-personalised-and-relevant">Developing a teacher model that's personalised and relevant</a></li>
<li><a href="#developing-an-explicit-model-of-the-student">Developing an explicit model of the student</a></li>
</ul>
</div>
<h2 id="introduction">Introduction</h2>
<p>In <a href="https://en.wikipedia.org/wiki/Bloom%27s_2_sigma_problem">"The 2 Sigma Problem"</a>, Bloom suggested that one-to-one tutoring could provide a phenomenal boost to students' learning. Maybe it's <a href="https://www.educationnext.org/two-sigma-tutoring-separating-science-fiction-from-science-fact/">not <em>quite</em> as big an effect as he suggested</a>, but I'm still willing to bet that the combination of personalisation, relevance, engagement, adaptive difficulty, and other benefits of one-to-one tutoring do make a huge difference, and at least some of them can be captured by generative AI. But even if we thought we did have a better approach, how could we be sure?</p>
<p>I'll try and tackle both sets of questions.</p>
<h2 id="developing-a-teacher-model-thats-personalised-and-relevant">Developing a teacher model that's personalised and relevant</h2>
<p>Let's focus on the question of how we might make a teacher model that provides very personalised, relevant tutoring to the student.</p>
<p>Relevance is rich and hierarchical: the environment, the task or context that the student is learning about right now, this particular course, their previous learning and progress, the history of interactions with the teacher, the language, the country they live in, recent news, macro changes, etc. All of these contextualise our learning.</p>
<p>This kind of rich, hierarchical, many-faceted personalisation presents challenges for a single gigantic, fixed model, such as an LLM.</p>
<p><em>Maybe</em> we can get away with just prompt engineering, feeding in an enormous dump of data about learner goals, preferences, and previous interactions, and rely on ever-larger context windows and more instructable models.</p>
<p>But I’m pessimistic about whether prompt engineering alone is enough for fine-grained, subtle, creative, optimal relevance and personalisation.</p>
<p>Firstly, there's the challenge of fitting all the necessary background into the model's context window <a href="https://www.dbreunig.com/2025/06/22/how-contexts-fail-and-how-to-fix-them.html">without overwhelming or confusing it</a>.</p>
<p>Secondly, LLMs struggle with prompts that tug against their pretraining. For example, at <a href="https://rehearsable.ai/">Rehearsable.ai</a>, it proved intractable to prompt-engineer GPT-4 to reliably and subtly follow a particular negotiation skills approach that flew in the face of the common advice found on the internet.</p>
<p>In the short-run, perhaps we can imagine hierarchies of LORA-style fine-tuning that can be layered on top of one another, for student, exam board, culture, etc. But my bet is that eventually the best AI teachers will learn too, alongside and about their students.</p>
<h2 id="developing-an-explicit-model-of-the-student">Developing an explicit model of the student</h2>
<p>Perhaps we also need to go beyond a single, main teacher model. Could it help to represent the learner with an explicit (separate?) model that assists the primary teacher model? To train such a model of the learner, we could train it to predict the learner’s behaviour, e.g. the answers they are giving, and the questions they are asking. We might then probe this model of the learner, to ask “How would the learner respond if we asked them this question?”, or to look at how it has changed over time to measure “Has the learner’s understanding of Topic X improved?”.</p>
<p>With such a model of the learner’s behaviour, we could run <a href="https://medium.com/@_michelangelo_/monte-carlo-tree-search-mcts-algorithm-for-dummies-74b2bae53bfa">Monte-Carlo Tree Search</a> or similar to simulate the effect of different teacher interventions, and pick the one that we believe will best help improve the learner’s eventual performance. In this way, we can consider relevance as exactly the content that <em>this</em> learner needs right now, in order to pass their particular exam, or indeed to unstick their current confusion. A rich model of the learner could help with choosing or generating particular problems that will help them see how to apply a new concept, develop the skill they’re missing, or correct a misconception. It could involve judicious examples, or analogies, or counter-examples.</p>
<p>So, future AI one-to-one tutoring might involve both custom teacher <em>and</em> custom student models.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/what-new-developments-will-we-need-for-personalised-relevant-one-to-one-ai-tutoring/</guid>
      <pubDate>Sat, 09 Aug 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>How can we measure the effectiveness of an AI tutor?</title>
      <link>https://www.gregdetre.com/blog/how-can-we-measure-the-effectiveness-of-an-ai-tutor/</link>
      <description>Perhaps our future grades will depend not on *our* exam performance, but on how well our **digital avatars** handle a battery of simulated tests.</description>
      <content:encoded><![CDATA[<div class="toc">
<ul>
<li><a href="#measuring-human-learner-performance">Measuring human-learner performance</a></li>
<li><a href="#measuring-teacher-performance">Measuring teacher performance</a></li>
<li><a href="#ai-offers-new-more-accurate-more-humane-ways-to-evaluate">AI offers new, more accurate, more humane ways to evaluate</a></li>
</ul>
</div>
<h2 id="measuring-human-learner-performance">Measuring human-learner performance</h2>
<p>We can of course try to directly measure the learner’s performance, e.g. with explicit tests and exams - but these are effortful for the learner, inefficient, and don’t capture everything that matters.</p>
<p>We can measure the learner’s engagement and enjoyment, either explicitly (with surveys) or implicitly (by their continued interest). This makes some sense, in that a learner won't learn much if they give up. But on the flip side, it's possible to be engaged in something that isn't efficacious. So, engagement is <em>necessary</em> but not <em>sufficient</em> for learning.</p>
<p>We might even try and quantify aspects of the learner’s behaviour with automated LLM evals, e.g. whether the learner is exhibiting curiosity, or asking good questions.</p>
<p>Of course, even if we could measure all these dimensions at a given moment, it’s not easy to tell how much to attribute changes over time to the teacher. We have expensive gold-standard measurements (e.g. longitudinal between-subject AB tests/RCTs), but we can't run these very often. So in practice, we have to rely mostly on cheap &amp; immediate automated evaluations, after validating them occasionally against the gold standard - kinda like applying the <a href="https://en.wikipedia.org/wiki/Cosmic_distance_ladder">cosmic distance ladder</a> approach to product development, where we try and find a chain of proximal measures that ladder up to our expensive, gold-standard distal measure.</p>
<h2 id="measuring-teacher-performance">Measuring teacher performance</h2>
<p>We can also try and measure the teacher’s behaviour, as in the LearnLM paper, where the model is tuned towards important dimensions, like “be encouraging”, “don’t give away the answer prematurely”, and “keep the learner on track”.</p>
<p>Beyond these, there are many lower-level pedagogical best practices we might consider, e.g. encouraging an <a href="https://web.archive.org/web/20070317194011/http://www.stanfordalumni.org/news/magazine/2007/marapr/features/dweck.html">incremental mindset</a>, making the content <a href="https://www.memrise.com/blog/how-to-use-mnemonics-to-remember-new-vocabulary">memorable</a>, applying a <a href="https://www.notion.so/230422-Newspeak-House-Hackathon-with-Peppe-Cesar-Greg-2c73fa13af404c70b03263bf56124810?pvs=21">spiral curriculum</a>, use of analogies and examples, etc.</p>
<p>And of course there are various product metrics, e.g. latency, ease of use.</p>
<h2 id="ai-offers-new-more-accurate-more-humane-ways-to-evaluate">AI offers new, more accurate, more humane ways to evaluate</h2>
<p>We can also consider more speculative approaches that are too labour-intensive for human teachers:</p>
<ul>
<li>
<p>A deep dialogue with a teacher (almost like a low-stakes, ongoing PhD viva) provides a very rich measure of a learner’s handle on the material - the teacher can constantly probe, ask questions at the boundaries of the learner’s knowledge, and ask them to apply what they have learned in unexpected ways. Asking questions at the margins of the learner’s knowledge like this will maximise the informational payoff of each question to the teacher. It’s too expensive to have a human teacher discussing with the learner all the time, but this may be one of the ways that an AI teacher could develop a very rich sense of a learner’s ability over time.</p>
</li>
<li>
<p><em>"If you can't explain it simply, you don't understand it well enough”</em> (Feynman). We can discover a great deal about the learner’s understanding by asking them to teach someone else (either a human or AI peer), and noticing where in the material they succeed and where they struggle as a teacher. And as a nice side-effect, both these activities will also be very effective for helping the learner to learn.</p>
</li>
<li>
<p>If we have built a rich, explicit model of the learner, as above, then we could use the simulated learner’s performance as a proxy measure for the real learner’s ability - perhaps in the future, our grade will be based on how well our avatars do on a multi-day battery of simulated exams!</p>
</li>
</ul>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/how-can-we-measure-the-effectiveness-of-an-ai-tutor/</guid>
      <pubDate>Sun, 10 Aug 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Psychological effects and cognitive illusions in AI-assisted programming</title>
      <link>https://www.gregdetre.com/blog/psychological-illusions-in-ai-assisted-programming/</link>
      <description>We judge AI's coding mistakes as **permanent character flaws** whilst excusing our own as situational—and find a precarious flow. What can psychology experiments tell us about how it _feels_ to program with AI assistance?</description>
      <content:encoded><![CDATA[<p>There's something odd and new about the subjective experience of AI-assisted programming. It distorts our sense of time passing, difficulty, effectiveness, and joy.</p>
<p>Some of these are well-known psychological effects and illusions, and the psychological theories behind them can help us understand a bit better what's going on.</p>
<div class="toc">
<ul>
<li><a href="#its-hard-to-judge-how-much-work-the-ai-did-for-us">It's hard to judge how much work the AI did for us</a></li>
<li><a href="#context-change-and-feeling-of-duration">Context change, and feeling of duration</a></li>
<li><a href="#being-knocked-out-of-flow">Being knocked out of flow</a></li>
<li><a href="#weakened-reinforcement">Weakened reinforcement</a></li>
<li><a href="#its-a-different-kind-of-work">It's a different kind of work</a></li>
<li><a href="#a-new-kind-of-skill">A new kind of skill</a></li>
<li><a href="#where-does-this-leave-us">Where does this leave us?</a><ul>
<li><a href="#discussion-on-linkedin">Discussion on LinkedIn</a></li>
</ul>
</li>
</ul>
</div>
<h2 id="its-hard-to-judge-how-much-work-the-ai-did-for-us">It's hard to judge how much work the AI did for us</h2>
<p>When the AI takes an hour to do something, with all the attendant dead-ends and back-and-forth, it's hard to know how effective it has been.</p>
<p>In general, it's <a href="https://newsletter.pragmaticengineer.com/p/measuring-developer-productivity">very difficult to objectively measure programming productivity</a> , and <a href="https://www.makingdatamistakes.com/why-were-bad-at-estimating-and-what-to-do-about-it/">we are bad at estimating how long it would have taken us to do</a>.</p>
<p>When we watch the AI try and fail and iterate, we notice every failure and false start along the way. <em>"Gah, you've broken the unit tests again, you foolish robot!"</em> But when coding manually ourselves, we might make the same mistakes but experience them differently. Maybe this is a case of the <a href="https://en.wikipedia.org/wiki/Fundamental_attribution_error">fundamental attribution error</a>, that we see our own actions as contextualised by the situation, whereas we're more likely to attribute permanent 'dispositions' to others based on their actions. Or a little darker, that we use a <a href="https://pubmed.ncbi.nlm.nih.gov/2266485/">victim vs perpetrator narrative</a>: we use a 'perpetrator' narrative for our own mistakes (meaningful and comprehensible, with the incident as a closed, isolated event with no lasting implications) and a 'victim' narrative for the AI's mistakes (seeing the actions as arbitrary and incomprehensible, and portraying the incident in a long-term context with continuing harm and lasting grievances).</p>
<p>We <a href="https://claude.ai/chat/1fefa77f-b834-4f10-abad-0ed3e76c86c9">aren't always good judges of how hard things are, what works for us and what doesn't, or reliable in comparing ourselves to others</a>. (For example, there's a lovely <a href="https://www.pnas.org/doi/10.1073/pnas.1821936116">psychology experiment where students feel they've learned less from active recall than passive review, even though active recall is actually more effective</a>.) And things always <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC1743746/">seem simpler after we have understood them</a>, so once we see the AI's solution, it seems obvious.</p>
<p>So for all these reasons, expect to see a lot of people rationalising away even a 10x speedup (<a href="https://www.makingdatamistakes.com/ai-first-development/">1000+ lines of code in a day</a>, rather than <a href="https://www.perplexity.ai/search/how-many-lines-of-code-does-a-JStnps.URG.lO.1AClkOOw">&lt;100</a>). After all, <em>"<a href="https://www.goodreads.com/quotes/21810-it-is-difficult-to-get-a-man-to-understand-something">it is difficult to get a man to understand something, when his salary depends on his not understanding it</a>”</em> :~</p>
<h1 id="context-change-and-feeling-of-duration">Context change, and feeling of duration</h1>
<p>In an hour of AI-assisted programming, especially with multiple AI agents working in parallel, we might experience multiple distinct events, e.g. discussing ideas, exploring multiple approaches; hitting a dead end; reverting and try again; shipping a couple of small features. Each of these feels like a significant, distinct event.</p>
<p>Our <a href="https://claude.ai/chat/dafd3c21-06bd-4271-8f06-9c4e2efe64b7">sense of time is heavily influenced by the number of distinct events or changes we experience</a>. As a result, an hour spent like this will <em>feel</em> like it lasts longer than if we'd just been working on a single, undifferentiated problem. Even though you've accomplished more than you might have in a day of traditional coding.</p>
<h1 id="being-knocked-out-of-flow">Being knocked out of flow</h1>
<p>AI-assisted programming knocks us out of the <a href="https://en.wikipedia.org/wiki/Flow_(psychology)">"flow state"</a> - that magical zone of manageable-but-still-challenging where you lose track of time.</p>
<p>Classic programming provides an almost instant feedback loop: write code, compile, see results, tweak, repeat. Each micro-adjustment feeds into that cycle.</p>
<p>AI-assisted development breaks this pattern. Even a 10-second delay between your instruction and the AI's response can bump you out of flow. Watching the AI iterate feels like watching paint dry - even if it's actually drying in just a few moments.</p>
<p>Eventually, the models will just output so fast that this problem will go away. In the meantime, it helps to run multiple agents in parallel, and switch each time the AI delays (see "<a href="https://www.makingdatamistakes.com/ai-first-development/">Optimise for correctness"</a>). Of course, this creates switch costs that need to be managed...</p>
<h1 id="weakened-reinforcement">Weakened reinforcement</h1>
<p><a href="https://umbrex.com/resources/tools-for-thinking/what-is-variable-rewards/">'Variable reinforcement'</a> is when you get some kind of reward or positive result <em>some</em> of the time. Like slot machines... or video games, or capricious bosses/partners, or email. This unpredictable signal creates <a href="https://blog.nateliason.com/p/rats-levers-parks">very robust addictive behaviours</a>.</p>
<p>Programming has something of this character. Maybe <em>this time</em> when I hit 'compile', it'll work!</p>
<p>But it's <a href="https://claude.ai/chat/4e99dc52-82dd-4ab8-a611-3bcc2d3a386e">less reinforcing to watch someone someone else get the reward</a> - so when the AI's the one pulling the lever and sometimes getting the cocaine, we miss out on the rush.</p>
<h1 id="its-a-different-kind-of-work">It's a different kind of work</h1>
<p>While AI reduces the mechanical effort of coding, it increases the cognitive demand of communication. You're constantly making implicit knowledge explicit:</p>
<ul>
<li>
<p>Explaining background context</p>
</li>
<li>
<p>Defining success criteria</p>
</li>
<li>
<p>Articulating architectural principles</p>
</li>
<li>
<p>Catching unstated assumptions</p>
</li>
</ul>
<p>It's like the difference between driving a car (flow state, muscle memory) and teaching someone to drive (constant metacognition, explicit instruction).</p>
<p>When AI is writing most of the code, you spend most of your time reading what it wrote, often being spread across various parts of the codebase, and trying to figure out how a series of scattered edits fit into a larger context. And reading code is much more effortful than writing it. I'm not 100% sure why. But it partly explains why it's so much more fun to build from a greenfield than to modify an existing system, and contributes to the illusion that it would be better to start from scratch and <a href="https://www.joelonsoftware.com/2000/04/06/things-you-should-never-do-part-i/">rewrite the whole damn thing</a>!</p>
<p>Anybody know of any studies that would explain this?</p>
<h1 id="a-new-kind-of-skill">A new kind of skill</h1>
<p>AI-assisted programming is a skill. It might even be a meta-skill, a skill about skills, like teaching or learning to learn. It is multiplied by your own mastery at the task, and your own clarity of thinking, and your own metacognition.</p>
<p>We can expect various non-monotonicities, e.g.</p>
<ul>
<li><strong>Seniority</strong>: Junior programmers seem most positive about AI, because it levels the playing field for them. Senior programmers seem pretty positive, because their experience &amp; judgment enables them to make effective architecture decisions, without having to worry about semicolons), while mid-level programmers are most negative, because it threatens the craft and low-level skill on which their pride and economic value sit. see (<a href="https://www.wired.com/story/how-software-engineers-coders-actually-use-ai/">"Does experience affect attitude?"</a>)</li>
</ul>
<p><img alt="Does experience affect attitude" src="/blog/images/does_experience_affect_attitude.png"/></p>
<ul>
<li>
<p><strong>Expertise</strong>: This is part of what I'd guess explains the apparent <a href="https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/"><em>drop</em> in productivity when using AI tools in a recent (mid-2025) study</a>. Expert AI-assisted programming will involve <a href="https://www.makingdatamistakes.com/ai-first-development/">un-learning a lot of existing behaviours &amp; skills, and replacing them with new ones</a>.</p>
</li>
<li>
<p><strong>Verbal</strong>: We may find that the best AI-assisted programmers have different skillsets from the best human-only programmers. We may find that teachers and managers and other expert communicators are at a huge advantage.</p>
</li>
</ul>
<h1 id="where-does-this-leave-us">Where does this leave us?</h1>
<p>Even despite all this, I have started to experience a new kind of flow working with AI too. When the context is well-defined, and the guardrails are protective, then the speed at which dreams transform into working code can feel like magic. The impedance between imagination and implementation drops dramatically.</p>
<p>As AI models get faster and we develop better workflows, we might find a sweet spot that combines this new superpower with the satisfying flow of traditional programming. Until then, we're learning to appreciate a different kind of satisfaction: not the micro-dopamine hits of instant feedback, but the macro-achievements of watching our ideas materialise at unprecedented speed.</p>
<h2 id="discussion-on-linkedin">Discussion on LinkedIn</h2>
<p>see <a href="https://www.linkedin.com/posts/gregdetre_three-fascinating-psychological-illusions-activity-7358805359253225473-318L/?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAAF1wDIBUoRgjztIGLcmp5W_rk7loyfkMx0">LinkedIn discussion &amp; responses</a></p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/psychological-illusions-in-ai-assisted-programming/</guid>
      <pubDate>Mon, 11 Aug 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>I refuse to use this technology, even if it would make me money</title>
      <link>https://www.gregdetre.com/blog/i-refuse-to-use-this-technology-even-if-it-would-make-me-money/</link>
      <description>When AI systems produce hate speech, technologists dismiss it as mere 'bugs' - what would it actually take for us to refuse profitable technology on **moral grounds**?</description>
      <content:encoded><![CDATA[<p>There is a new version of Grok, the Elon Musk-owned xAI bot used on Twitter/X coming out later today.  </p>
<p>We were discussing it on a UK AI WhatsApp group that I'm part of, full of some great technologists.  </p>
<p>This AI is sympathetic to Hitler, and producing hate speech. Last time, when it started talking about white genocide, the company acknowledged that was in direct response to a change that someone (they won’t say who) had made to its system prompt.  </p>
<p>I asked "What would it take for each of us to take a stand, and say, "I refuse to use this technology (even if it would make me money)?” and included a link to a <a href="https://www.theguardian.com/technology/2025/jul/09/grok-ai-praised-hitler-antisemitism-x-ntwnfb">Guardian article</a>. Responses included:  </p>
<ul>
<li>
<p>"I just see it as a bug - we all get them. Theirs is just played out in public to a huge audience."  </p>
</li>
<li>
<p>"We are still at the stage of it being a “tool” and as such a tool has good and bad use, but also bugs"  </p>
</li>
</ul>
<p>I don't want to alienate those people because I feel positively about them as humans, but I want to stand up and say that I don't agree with that view.  </p>
<p>After all, even if we choose to give xAI the benefit of the doubt, and say it’s not deliberate...  </p>
<p>Wouldn’t that suggest then that it’s a sign of a systematically irresponsible, inadequate approach to safety and alignment?  </p>
<p>A genuine question to ask ourselves as technologists: is there anything an AI could say that would make you respond “Hmmm, I’m not willing to use this, even though it's economically valuable"?  </p>
<p>I don’t know whether it’s helpful to create division by calling people out. On balance I think it is more important to point out that we are blurring what I believe should be a clear line in the sand, and to stand publicly on one side of it.</p>
<p>see <a href="https://www.linkedin.com/posts/gregdetre_there-is-a-new-version-of-grok-the-elon-activity-7348708765765963777-_JOK/?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAAF1wDIBUoRgjztIGLcmp5W_rk7loyfkMx0">LinkedIn post &amp; responses</a></p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/i-refuse-to-use-this-technology-even-if-it-would-make-me-money/</guid>
      <pubDate>Thu, 14 Aug 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Product idea - an episode-focused podcast player</title>
      <link>https://www.gregdetre.com/blog/product-idea-an-episode-focused-podcast-player/</link>
      <description>Podcast apps still think in terms of *shows*, but what if they recommended the best *episodes* instead—using AI to parse transcripts and behavioural data to surface hidden gems across the entire medium?</description>
      <content:encoded><![CDATA[<ul>
<li>
<p>Podcast app user interfaces work at the level of a <em>show</em> - you subscribe to a <em>show</em>, it recommends <em>shows</em>.</p>
</li>
<li>
<p>But this is an old-world way of thinking.</p>
</li>
<li>
<p>Really, I want a podcast app that focuses on <em>episodes</em> - that recommends me the best episodes of a given show, and the episodes from other shows that I might like.</p>
</li>
<li>
<p>Recommendation engines usually rely on two main signals:</p>
</li>
<li>
<ol>
<li><strong>social</strong>, i.e. what we can learn from the behaviour of other users, e.g. <em>"other users like </em><em>you</em><em> liked </em><em>this</em><em>"</em></li>
</ol>
</li>
<li>
<ol start="2">
<li><strong>content</strong>, i.e. what we can learn from the substance of the content itself, e.g. "<em>this episode is about X and Y, and you listen to a lot of stuff about both X and Y</em>".</li>
</ol>
</li>
<li>
<p>Viewed through this lens, what data can tell us which are the best episodes of a given show?</p>
</li>
<li>
<ul>
<li><strong>Social</strong> - if you're a popular enough app, you can use behavioural analytics (e.g. the proportion of listeners that listen all the way to the end, or share the episode).</li>
</ul>
</li>
<li>
<ul>
<li><strong>Content</strong> - ask an LLM to read the transcripts, and ask it which episodes seem useful/interesting/funny, which involved famous guests, which involve trending topics, etc.</li>
</ul>
</li>
<li>
<p>What about the episodes from other shows that I might like?</p>
</li>
<li>
<ul>
<li><strong>Social</strong> - standard cross-user collaborative filtering, to identify episodes that other users like me liked. This might not work that well at an episode level, because so much of our listening behaviour currently is driven by show-level patterns.</li>
</ul>
</li>
<li>
<ul>
<li><strong>Content</strong> - build up a profile of the user's interests, and then look for episodes that seem relevant.</li>
</ul>
</li>
<li>
<p>The tricky part, as always with recommendation engines, is to <a href="https://www.makingdatamistakes.com/how-to-know-if-your-recommendations-algorithm-is-actually-doing-a-good-job/">balance relevance with serendipity</a>, e.g. just because I've listened to a couple of interviews with Demis Hassabis doesn't mean that I <em>only</em> want to listen to interviews with him...</p>
</li>
</ul>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/product-idea-an-episode-focused-podcast-player/</guid>
      <pubDate>Sun, 17 Aug 2025 00:00:00 +0000</pubDate>
    </item>
  </channel>
</rss>
