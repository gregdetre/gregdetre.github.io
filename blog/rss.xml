<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Greg Detre</title>
    <link>https://www.gregdetre.com/blog/rss.xml</link>
    <description>Greg Detre's personal website</description>
    <atom:link href="https://www.gregdetre.com/blog/rss.xml" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>en</language>
    <lastBuildDate>Mon, 25 Aug 2025 16:52:49 +0300</lastBuildDate>
    <item>
      <title>Master Turkers</title>
      <link>https://www.gregdetre.com/blog/master-turkers/</link>
      <description>What if Amazon's Mechanical Turk could identify **Master Turkers** capable of higher-skilled work like research, programming, or creative tasks—transforming micro-work into meaningful portfolio building?</description>
      <content:encoded><![CDATA[<p>Amazon's Mechanical Turk is an amazing service where one can create a simple task that can be micro-out-sourced to many people over the web, each of whom performs a small parcel of it. For instance, if you wanted 1000 people to highlight faces in photographs, think of synonyms for words, or provide from-the-hip feedback on your website, Mechanical Turk is ideal.  </p>
<p>For reasons that are unclear to me, people seem to be willing to work for far below a minimum wage performing pretty dull tasks. As an experimental psychologist, I'm torn between feelings of data lust at the number of participants I could thus thriftily recruit, and concern about the quality of their data. What kind of person is willing to engage in dull tasks that must feel meaningless from a worm's eye view? Where's the incentive to do a good job?  </p>
<p>It seems to me that there might be a market for tasks that require more effort, skill or thought, for which one would like to be able to cherry pick the participants. For this to work, you'd need a rich reputation scheme to Mechanical Turk, to pick out the Master Turks.  </p>
<p>I'm picturing myself in holidays as an undergraduate. If someone was willing to pay more £10/hour (roughly what I was earning as a medical secretary), I (or my more talented peers) would have happily:  </p>
<ul>
<li>
<p>researched historical facts for a novel</p>
</li>
<li>
<p>proofread a doctoral thesis</p>
</li>
<li>
<p>helped with market research for a business plan</p>
</li>
<li>
<p>written a catchy jingle</p>
</li>
<li>
<p>filmed a youtube video using your product</p>
</li>
<li>
<p>written a program to generate verbal reasoning or arithmetic questions for an exam</p>
</li>
<li>
<p>provided summaries of white papers</p>
</li>
</ul>
<p>You could imagine non-fixed-rate payment schemes, e.g.  </p>
<ul>
<li>
<p>a competition where the best submissions divide the spoils</p>
</li>
<li>
<p>an auction, so that more enjoyable tasks would be bid lower</p>
</li>
</ul>
<p>And, deliciously, you could create a meta peer-review system where other Master Turkers' task is to rate the submissions you've received.  </p>
<p>Stack Overflow is going to transform the programming job market by making answering people's questions satisfying, and then providing a metric of someone's expertise that will help them land a job.  </p>
<p>There have been many precedents of this kind of idea, but it seems strange that none of them have taken off. This feels like a way to demonstrate one's abilities on potentially interesting tasks that would provide a portfolio of work to supplement a job application.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/master-turkers/</guid>
      <pubDate>Sat, 30 Oct 2010 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>How can your iPhone make you even more entertaining and interesting than you already are?</title>
      <link>https://www.gregdetre.com/blog/how-can-your-iphone-make-you-even-more-entertaining-and-interesting-than-you-already-are/</link>
      <description>What if your iPhone could listen to conversations and automatically surface answers to disputed facts in real-time, keeping dialogue flowing instead of grinding to a halt?</description>
      <content:encoded><![CDATA[<p>Have you ever had a conversation with smart friends that got hung up on some disputed point of fact, or tip of the tongue memory failure? Don't you just wish someone would step in with the answer to unclog the free flow of ideas and happy banter? Disputes about facts and tip-of-the-tongue feelings *should* be a relic of the 20th century. So there are two things that are remarkable here:  </p>
<p>- Through smartphones and search engines, we can marshal thousands of machines to produce the answer in the blink of your mind's eye.  </p>
<p>- But we have to perform that instantaneous incantation with pudgy fingers and a 0.3G internet connection. I challenge anyone to find the name of an actor in under 2 minutes with an iPhone with crappy reception. While those 120 seconds creep past, you're coldly ignoring your friends, and the conversation is gasping on the table like a naked baby on a spacewalk.  </p>
<p>Here's one technological solution to this social problem:  </p>
<p>- At the beginning of the conversation, we all put our iPhones on the table, and fire up the Inforager app.  </p>
<p>- Inforager is listening to us, uploading the audio of our conversation to voice-recognizing clouds.  </p>
<p>- It runs dozens of google searches continually in the background, displaying result-snippet-bubbles that float past, driven by the whorls and eddies of our conversation. While we're talking about the beardy guy with the Greek name in The Hangover, a bubble for 'Zach Galifianakis - IMDb' looms large, only to be nudged offscreen as we move to debating whether the 'candied sunchokes' on the restaurant menu are likely to taste more like sunflowers or artichokes, while the other half of the table engages in a dialog on the nature of catnip.  </p>
<p>In other words, the answers to questions we have are being provided in real time in response to our conversation. <em>This frees us up to talk about what matters.</em> </p>
<p>---- </p>
<p>Technical notes:  </p>
<p>- If multiple people at the same table were calling Inforager, it would use the multiple sound sources to do a better job of distinguishing voices and improving audio quality.  </p>
<p>- Is it possible to use the phone (rather than the 3G connection) to upload the audio data? That would drain the battery much less.  </p>
<p>- I made up the name Inforager.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/how-can-your-iphone-make-you-even-more-entertaining-and-interesting-than-you-already-are/</guid>
      <pubDate>Sun, 31 Oct 2010 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>A wiki for spaces. A town anyone can edit. School architecture founded on mnemonic principles</title>
      <link>https://www.gregdetre.com/blog/a-wiki-for-spaces-a-town-anyone-can-edit-school-architecture-founded-on-mnemonic-principles/</link>
      <description>What if the collaborative editing power of Wikipedia could be applied to *spaces* instead of text—from virtual towns to schools designed as living timelines?</description>
      <content:encoded><![CDATA[<p>When we think of wikis, we think of text, like the Wikipedia. But this notion of content that anyone can view and anyone can edit has barely unfurled its wings. What if we were to apply it to space?  </p>
<p>For instance, imagine growing a World of Warcraft town as a community. Each person could design and improve upon the buildings, fill the walls with graffiti, neighborhoods would define themselves... the ease and pace of iteration might even generate new ideas about town planning.  </p>
<p>Alternatively, let's build on Ed Cooke's <a href="http://www.rememberremember.co.uk/?page_id=213">fantastic plan for school architecture in the future</a> [<a href="http://webcache.googleusercontent.com/search?q=cache:bYKUVPydjjYJ:www.rememberremember.co.uk/%3Fp%3D209+site:rememberremember.co.uk+school+OR+schools&amp;cd=2&amp;hl=en&amp;ct=clnk&amp;gl=uk">cached</a>]:  </p>
<blockquote>
<p><em>Children, well known to be compulsive absorbers of information, crucially learn what they are interested in. Like all animals, they are interested in spaces.</em></p>
<p><em>I’d like to see schools’ spatial layout reflect the history of Western culture, and thereby implicitly teach it. A snake-like line of school buildings could begin at one end in Ancient times and run on, in temporally organized fashion, up to the computer science blocks of the present day. Key themes and figures from each epoch could provide the names for classrooms, which could also reflect some of the architecture, customs and furniture of the day.</em></p>
<p><em>Because in five years of school, everyone learns every detail of the spatial organisation of the buildings, and because memories always attach to the spaces in which they were first formed, merely attending such a school would give one a wonderfully detailed sense of the history and structure of Western civilisation. And it wouldn’t need to be prescriptive, for one could take advantage of the second source of childrens’ interest - things they have a role in - to redouble the effect. Each year-group could, over the course of five years, reconsider, re-design and re-build one of the twelve epochs/buildings.</em></p>
</blockquote>
<p>Convincing someone to build a school organized on mnemonic principles is going to be tricky. But in the meantime, perhaps schools' online presence might take the form of a spatial wiki. Students could make changes ranging from decor to naming to overall organization, shaping their online school to their memories and vice versa. We love to deeply inhabit our environment by shaping it - what could be better than exercising our rich faculty for spatial navigation imaginatively?</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/a-wiki-for-spaces-a-town-anyone-can-edit-school-architecture-founded-on-mnemonic-principles/</guid>
      <pubDate>Sun, 31 Oct 2010 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>The muses are deaf, so speak up</title>
      <link>https://www.gregdetre.com/blog/the-muses-are-deaf-so-speak-up/</link>
      <description>Want to unlock better thinking while walking? **Talk out loud**—loudly, proudly, with gestures—because the muses are deaf and your inner monologue isn't entertaining enough.</description>
      <content:encoded><![CDATA[<p>Good thoughts tend to shy away from short walks with a destination. They're kept at bay by the neuroses and instant replays that circle endlessly like tethered carrion.</p>
<p>Do you want to know the only way I've found to think while walking? Talk out loud. Loudly proudly aloud. Feel free to gesticulate. Close your eyes if traffic conditions permit. Tell yourself a story. Don't use your normal voice.</p>
<p>Why would talking out loud make such a colossal difference? Perhaps because repetition feels explicitly boring out loud, so we avoid re-treading the same paths. Perhaps because full sentences flush and flesh out our half-thoughts? Perhaps because serializing our massively parallel murmur squeezes the thoughts out one at a time with greater velocity, like putting your thumb on a hose.</p>
<p>The effect is so striking that I've wondered about potential neuroscientific explanations. It could be that different neural pathways are being activated - perhaps it is only by vocalizing that we recruit speech production areas, or only by hearing our own voice we recruit speech comprehension areas. Or just that there's less neural juice sluicing down the byways of my mind during my inner monologue, and the extra oomph required to speak gives the thoughts extra vivacity.</p>
<p>The explanation I favour? If I'm going to have to listen to myself, I want to be entertained.</p>
<p>https://twitter.com/gregdetre/status/431452870961946624</p>
<p>P.S. For best results, wear a hat and <a href="http://www.youtube.com/watch?v=EOrG1r3S6ZA">learn to talk like Tom Waits</a>.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/the-muses-are-deaf-so-speak-up/</guid>
      <pubDate>Mon, 01 Nov 2010 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>But where does the wisdom come from?</title>
      <link>https://www.gregdetre.com/blog/but-where-does-the-wisdom-come-from/</link>
      <description>With courage and serenity within reach, the most elusive element of the Serenity Prayer remains: **where does the wisdom come from?**</description>
      <content:encoded><![CDATA[<p><em>God, grant me the serenity</em></p>
<p><em>To accept the things I cannot change;</em></p>
<p><em>Courage to change the things I can;</em></p>
<p><em>And wisdom to know the difference.</em> </p>
<p>With the support of those around me, I can struggle to be courageous.  </p>
<p>I believe I can summon forth the serenity.  </p>
<p>But where does the wisdom come from?</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/but-where-does-the-wisdom-come-from/</guid>
      <pubDate>Wed, 03 Nov 2010 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>When I am famous, I will decline interviews</title>
      <link>https://www.gregdetre.com/blog/when-i-am-famous-i-will-decline-interviews/</link>
      <description>Famous actors' glossy interviews fill the author with *existential dread*—conversations stripped of context, distilled by strangers into caricature.</description>
      <content:encoded><![CDATA[<p>Reading the 5-page staged and glossy magazine interview in a hotel room with a famous actor has always filled me with a peculiar kind of existential dread. There's something a little horrifying about an hour of conversation in cold type, bereft of the intonation, expression, context and rapport that make anything one says out loud bearable. And at the end of it all, to be distilled, distorted, interpreted and weighed by the pen of a stranger... Who could have the strength of character to read about but not become their own caricature?  </p>
<p>In contrast, the last page of the Sunday Times magazine features 'a life in the day of' a happy array of personalities and professions. I like the concreteness of a single day as a window into someone else's micro challenges and achievements. I realize that these days are probably fictionalized composites - but fiction makes for a sweet, concentrated and memorable pill. And at the end of it, there is no distillation, no weighing - just the reality of a daily rhythm.  </p>
<p>When I am famous, I will decline interviews.  </p>
<p>P.S. That said, I still remember being stopped in my tracks when a fashion photographer relative asked me sweetly 'what did you today?' in the midst of my PhD. My day had consisted of:  </p>
<ul>
<li>
<p>2 hours debugging a misplaced comma</p>
</li>
<li>
<p>so that I could finish the 3-day long project of rearchitecting my non-parametric statistics to work across-subjects</p>
</li>
<li>
<p>in order to get a better sense of whether results from the latest in a long line of experiments were actually better than chance</p>
</li>
<li>
<p>so that we could tell whether reminding people and distracting them at the same time was causing them to forget</p>
</li>
<li>
<p>to test our computational theory that half-remembering a memory actually weakens it</p>
</li>
<li>
<p>which would have deep implications for our understanding how the brain learns and self-organizes</p>
</li>
</ul>
<p>But really, I'd been comma-hunting, and it seemed hard to fit that into a the kind of response usually expected from 'what did you do today?'.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/when-i-am-famous-i-will-decline-interviews/</guid>
      <pubDate>Thu, 04 Nov 2010 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>A letter to a prospective grad student</title>
      <link>https://www.gregdetre.com/blog/a-letter-to-a-prospective-grad-student/</link>
      <description>A PhD can be wonderful when you're excited about it, but **don't expect the idiosyncrasies of my experience to apply closely to anyone else**.</description>
      <content:encoded><![CDATA[<p>Preface: I wrote this to a friend asking me for advice about whether to embark on a science PhD. At the time of writing I still had more than a year to go - so I could see the summit in the distance, but I was feeling grim about the steep icewall I had to climb to get there.</p>
<p>In retrospect, I think a lot about Jeff Bezos' advice: don't be proud of your talents - be proud of the things you really worked hard to achieve. For this reason, I'm more proud of (and glad about) my PhD than anything else I've yet done.</p>
<p>----</p>
<p>It's hard for me to summarize my thoughts on grad school, perhaps because it varied so much in so many ways. Grad school was wonderful when I was excited about it - for the first few years, there was literally nothing I wanted to do more than talk and think and write and program lab stuff. Every week was filled with new ideas, a sense of progress and discovery, and I bounded into the lab every morning.</p>
<p>I don't know what changed exactly, but at some point, I started to really lose enthusiasm. I'm perenially stymied by an inability to understand the source of my own motivations, and to make sense of my own emotions. So I don't really feel like I understand why the joy started to fade. Perhaps because I worked for years on ambitious experiments that didn't work out. Because I'd been in one place for years. Because I'm a little flighty. Because I thrive in a more competitive or fast-moving jobs. Because really I love AI and computers a little more than brains. Because I wanted to be my own boss. Because I lost confidence. Because I need to feel part of a team working towards a common goal. Because I needed more inter-personal contact with a range of different people. Because I'm not temperamentally suited to be a scientist. Because I need to be in a city. Because I felt obliged to finish it, after investing so much into it, long after I would have left a normal job. Because the specialization necessary can come to seem like a straitjacket. Because I got obsessed with new ideas. I don't know.</p>
<p>It seems to me that a PhD is the right move if one loves what one's doing, and one wants to be an academic. Of course, you can't know for sure in advance that both of those are true. But if you think they might be, then go for it! While I think we have some things in common, I don't expect the idiosyncracies of my experiences to apply closely to anyone else, so don't look too closely for parallels to yourself in my issues above.</p>
<p>Right now, starting a company feels like the job I've been looking for my whole life, but I wouldn't have the wherewithal to do it unless I'd been through the last few years.</p>
<p>I don't know where your path will lead. Like me, I think you get excited about a lot of things, and could happily set off in many different directions, including becoming a great and happy scientist.</p>
<p>This email doesn't really answer any of your questions. I'm sorry about that - I just don't want to give advice one way or the other, because I think you'll make the right choices without my advice, and because you'll make whatever choices you make into the right ones. You are a lucky guy, in this (technical) sense - http://gregdetre.blogspot.com/2009/10/i-dont-believe-in-luck.html</p>
<p>:)</p>
<p>Keep me posted.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/a-letter-to-a-prospective-grad-student/</guid>
      <pubDate>Thu, 04 Nov 2010 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Breaking the seal</title>
      <link>https://www.gregdetre.com/blog/breaking-the-seal/</link>
      <description>After years of procrastination, a simple context shift finally unlocked the floodgates: *"Writing's like peeing - once you break the seal, the words just spill forth all evening."*</description>
      <content:encoded><![CDATA[<p>I've wanted to try daily writing for an impossibly long time, but the first words didn't want to be dragged out.  </p>
<p>In my case, they were unstoppered by a day in London. I pinballed from train platforms to coffee shops, oblivious bustle all around me, far away from the furrowed-browed finger-pecking at Memrise HQ. That context-shift provided a firebreak from the quotidian, and I was finally in the mood to mentally roll up my sleeves and rub my hands together.  </p>
<p>Writing's like peeing - once you break the seal, the words just spill forth all evening.  </p>
<p>I was able to decant a dozen half-thoughts that I queued up like toy soldiers, to be birthed one by one over the following week. It's now rather fun to receive a blog post from my previous self every day.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/breaking-the-seal/</guid>
      <pubDate>Fri, 05 Nov 2010 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Setting up your own domain name and website</title>
      <link>https://www.gregdetre.com/blog/setting-up-your-own-domain-name-and-website/</link>
      <description>Setting up your own domain and website doesn't have to be complicated—there are three main approaches, from DIY hosting to **Google Apps** to all-in-one platforms like **Weebly**.</description>
      <content:encoded><![CDATA[<p>Someone asked me recently about getting their own domain name and setting up a website. I'm not very good at this stuff, but I have been through it once or twice, so I thought I'd at least offer this up in case it's useful.  </p>
<p>Let's say you want to buy <em>example.com</em>, and set it up as a series of static informational pages about <em>Example Business Inc</em>, along with <em>@example.com</em> email addresses. There are (at least) 3 ways you can go:  </p>
<p><strong>1) the standard method</strong> </p>
<p>- Grab the domain from <a href="http://www.godaddy.com/">GoDaddy</a> (or any other domain name registrar - they all do basically the same thing). It'll cost you $10-20 for a year or two  </p>
<p>- Then you need to find a place to host your site (e.g. <a href="http://www.rackspace.com/index.php">Rackspace</a>, <a href="http://www.dreamhost.com/">Dreamhost</a>). You'd spend c. $10/month to rent space on a server, point your new domain name to the server's IP address, write and upload some html and images, and away you go.  </p>
<p>- You then need to set up email addresses. If it's GoDaddy, I think you'll be able to set it up to forward your email to an existing account without too much trouble.  </p>
<p>- This is what i had to do with <a href="http://www.memrise.com/">Memrise</a> because I wanted control over everything. Honestly, it was much much more complicated than i had anticipated to figure it all out.  </p>
<p><strong>option 2) use Google</strong> </p>
<p>- Use Google to <a href="https://www.google.com/a/cpanel/domain/new">register your domain name</a> </p>
<p>- I think that'll automatically set you up with <a href="http://www.google.com/apps/">Google Apps</a> (custom Gmail, Calendar, Sites, Blogger, Docs etc.) for free.  </p>
<p>- Then you can set up the design and content of the pages of your website with <a href="https://sites.google.com/">Google Sites</a>.  </p>
<p>- So then you'd all use a custom Gmail interface to check your <em>@example.com</em> address, and have access to <em>blog.example.com</em>, <em>calendar.example.com</em> etc. I'm a big fan of Google Apps.  </p>
<p><strong>option 3) Weebly (or some equivalent competitor)</strong> </p>
<p>- Besides Google, there are a variety of companies that help build a site. I've heard good things about <a href="http://www.weebly.com/">Weebly</a>, but haven't closely investigated it for myself.  </p>
<p>- Much like Google Sites, it looks like they'll <a href="http://www.weebly.com/features.html">do most of what you'd wan</a>t: help with design templates, deal with the hosting, potentially help with domain names, and a bunch of other stuff. Nice!  </p>
<p><strong>Conclusions:</strong> </p>
<p>- As long as your needs are simple, I would consider the Google/Weebly approach, since I think it'll be the most straightforward.  </p>
<p>- Down the line, if you decide that you want to build something more complicated and interactive, you can always hire a programmer and switch from Google/Weebly to your own hosting set up.  </p>
<p>- If you have someone to help who enjoys techie stuff or has set up their own site before, then setting things up with GoDaddy and your own hosting will probably go smoothly. But otherwise, a company like Google or Weebly that'll do 90% of the work for you, so you can focus on building a great site :)</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/setting-up-your-own-domain-name-and-website/</guid>
      <pubDate>Sat, 18 Dec 2010 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Storyville and Jenga iPhone apps</title>
      <link>https://www.gregdetre.com/blog/storyville-and-jenga-iphone-apps/</link>
      <description>Two friends have released iPhone apps just in time for Christmas: a **fiendishly realistic** Jenga with beautiful physics, and Storyville delivering weekly stories to your pocket.</description>
      <content:encoded><![CDATA[<p>Hey everyone,</p>
<p>Two friends have just released iPhone apps in time for Christmas:</p>
<ul>
<li>
<p><a href="http://itunes.apple.com/us/app/jenga/id392915994?mt=8">Jenga</a>, by Natural Motion. This is the official Jenga iPhone app - there's a fiendishly realistic physics model of the world behind the scenes, and it's a thing of beauty.</p>
</li>
<li>
<p><a href="http://www.storyvilleapp.com/">Storyville</a> by Fatty Apps - I love the idea of receiving a short story each week, beamed straight to my outboard brain.</p>
</li>
</ul>
<p>Buy. Them.</p>
<p>P.S. I'd be happy to put you in touch with Torsten or Atul if you have any direct feedback you'd like to give them.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/storyville-and-jenga-iphone-apps/</guid>
      <pubDate>Mon, 20 Dec 2010 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Get your users to tell you what they really want</title>
      <link>https://www.gregdetre.com/blog/get-your-users-to-tell-you-what-they-really-want/</link>
      <description>A simple feedback box at the end of learning sessions generated **5 times more user suggestions** than a prominent red UserVoice button that users had learned to ignore.</description>
      <content:encoded><![CDATA[<p>We're building <a href="http://www.memrise.com/">a site that we want people to love using</a>, so we want as much feedback on it as we can get. From the get-go, we've had a big red <a href="http://www.uservoice.com/">UserVoice 'Feedback'</a> button on the lefthand site of every page. But only a tiny proportion of our users ever suggested, voted or commented on an idea.</p>
<p><img alt="" src="/img/blog/uservoice_box.png"/></p>
<ul>
<li>
<p>The red Feedback button was ubiquitous, but I bet that made it effectively invisible. Our feedback box is a straightforward text box that gets presented opportunely and prominently at the end of every learning session, at a moment when people will be most likely to have something they want to tell us.  </p>
</li>
<li>
<p>We get an email every time someone drops in a suggestion, with the user's email address, so we can respond quickly to them individually. We very much want to follow up with people that have made the effort to give us feedback.  </p>
</li>
</ul>
<p>I'd say we're getting at least 5 times as many suggestions as we were.  </p>
<p>On the downside, we don't have a nice communal forum any more that allows people to vote or comment on one another's ideas - we might do something about that in the future.</p>
<p>Here's what I wish UserVoice would do:  </p>
<ul>
<li>
<p>Make it much easier for people to suggest new ideas. Even if it were to bring down the overall quality, I think an increased volume of raw responses would be of greater value.  </p>
</li>
<li>
<p>Charge straightforwardly based on the number of new suggestions. This would set up the right incentives for UserVoice to make it really easy for users to make new suggestions. N.B. I don't have a problem with paying - I just don't want to be forced to start out with a $100/mo plan when we have a tiny userbase in order to get features that I consider essential to the user experience.</p>
</li>
<li>
<p>Corollary: Don't withhold important features like single sign-on and white labeling the design for the exorbitant options.  </p>
</li>
</ul>
<p>[1] We compared UserVoice and GetSatisfaction pretty closely, and they both hike the prices if you want to be able to transfer login status across. In fact, GetSatisfaction didn't (at least at the time) allow anonymous suggestions, which felt like a huge barrier to entry to new submissions - this was what convinced me to try UserVoice in the first place.</p>
<p>[2] Hmm - perhaps this <a href="http://dustincurtis.com/you_should_follow_me_on_twitter.htm">should be a command</a>, e.g. 'Tell us what we can do to improve things'?</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/get-your-users-to-tell-you-what-they-really-want/</guid>
      <pubDate>Tue, 21 Dec 2010 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>DaisyDisk - where has all my disk space gone? (Mac app)</title>
      <link>https://www.gregdetre.com/blog/where-has-all-my-disk-space-gone-mac/</link>
      <description>I have a theory that inability to manage one's finances and one's hard disk space have the same psychological factors at their root.</description>
      <content:encoded><![CDATA[<p><a href="http://www.daisydiskapp.com/">DaisyDisk</a> is the best app I've found so far for visualising where all my disk space has gone. It's free, but you can pay $20 to get it to stop nagging you.</p>
<p>P.S. I have a theory that inability to manage one's finances and one's hard disk space have the same psychological factors at their root. (<em>"Oh, I'll treat myself just this once..."</em>, <em>"It's only a small thing"</em>, etc)</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/where-has-all-my-disk-space-gone-mac/</guid>
      <pubDate>Thu, 30 Dec 2010 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Startup thinking - the people who have most influenced my thinking on startups.</title>
      <link>https://www.gregdetre.com/blog/startup-thinking-the-people-who-have-most-influenced-my-thinking-on-startups/</link>
      <description>From **Steve Blank's** lean methodology to **Paul Graham's** 'release early, iterate often' philosophy—exploring the foundational thinkers who shaped modern startup wisdom.</description>
      <content:encoded><![CDATA[<p><strong>Steve Blank and Eric Ries</strong> </p>
<p><a href="http://steveblank.com/">Steve Blank</a> teaches entrepreneurship at the Haas Business School in Berkeley, but has a pretty serious pedigree as a tech entrepreneur himself. I'm ashamed to admit that I still haven't read The 4 Steps to the Epiphany, but I've read most of what he's posted online.  </p>
<p><a href="http://www.startuplessonslearned.com/">Eric Ries</a> is a protege of Steve Blank's, applies and develops many of the same 'lean' ideas, and focuses specifically on web startups.  </p>
<p>See the links for both <a href="http://platformsandnetworks.blogspot.com/2011/01/launching-tech-ventures-part-iv.html">Steve Blank and Eric Ries here</a>.  </p>
<p>---- </p>
<p><strong>Paul Graham</strong> </p>
<p>He's a tour de force and a hero of mine. One of the early proponents of 'release early, iterate often'.  </p>
<p>http://www.paulgraham.com/start.html  </p>
<p>http://www.foxbusiness.com/search-results/m/25897600/funding-tech-start-ups.htm  </p>
<p>---- </p>
<p><strong>Joel Spolsky</strong> </p>
<p>These are my favorite as they relate to HR:  </p>
<p>http://www.joelonsoftware.com/articles/GuerrillaInterviewing3.html  </p>
<p>http://www.joelonsoftware.com/navLinks/fog0000000262.html</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/startup-thinking-the-people-who-have-most-influenced-my-thinking-on-startups/</guid>
      <pubDate>Sun, 09 Jan 2011 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>The iPhone apps my cold, dead hands would cling most rigidly to</title>
      <link>https://www.gregdetre.com/blog/the-iphone-apps-my-cold-dead-hands-would-cling-most-rigidly-to/</link>
      <description>From **Instapaper** for reading queues to **Snaptell's** book-scanning wizardry, these essential iPhone apps prove indispensable for modern digital life.</description>
      <content:encoded><![CDATA[<ul>
<li>
<p><a href="http://www.instapaper.com/">Instapaper</a> - combine this with the <a href="https://chrome.google.com/extensions/detail/fldildgghjoohccppflaohodcnmlacpb">Instachrome</a> extension, and whenever I see a webpage I want to read later, it'll be waiting with me as I wait for a train</p>
</li>
<li>
<p>Light - it's bright! No more torches. If you lived in Hanborough, you'd need this too.</p>
</li>
<li>
<p>Trainline - faster than my laptop and/or a speeding bullet for checking train times in the UK</p>
</li>
<li>
<p>PlainText - write notes on your laptop, have them appear on your phone instantly through Dropbox and vice versa. Oh, and <a href="http://dropbox.com/">Dropbox</a> of course, too.</p>
</li>
<li>
<p>Dictionary.com - etymologies, pronunciations, the works.</p>
</li>
<li>
<p>Remote - control Keynote presentations from your phone.</p>
</li>
<li>
<p>Glympse - let other people know where you are.</p>
</li>
<li>
<p>Skype - I can call Mia for free while walking the streets</p>
</li>
<li>
<p>iTrans Tube and Tube Status for planning London Underground journeys</p>
</li>
<li>
<p>Snaptell - red laser black magic. Point at a book, and have elves whisper about it to you.</p>
</li>
<li>
<p>Angry Birds - the most popular mobile game of all time.</p>
</li>
<li>
<p>Spotify - all the music in the world on the go. Requires a Spotify subscription.</p>
</li>
<li>
<p>Shazam - tells you the name of songs that are currently playing.</p>
</li>
</ul>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/the-iphone-apps-my-cold-dead-hands-would-cling-most-rigidly-to/</guid>
      <pubDate>Mon, 10 Jan 2011 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Pluviocabulary (rain words), Memrise-style</title>
      <link>https://www.gregdetre.com/blog/pluviocabulary-rain-words-memrise-style/</link>
      <description>From *petrichor* (the smell of first rain) to *tirl* (rain's sound on rooftops), exploring the rich vocabulary of precipitation reveals language as nuanced as weather itself.</description>
      <content:encoded><![CDATA[<p>I got distracted by the <a href="http://www.nytimes.com/interactive/2011/04/03/opinion/20110403_schott.html">NY Times' Pluviocabulary list</a>, and found myself <a href="http://www.memrise.com/cave/?iset=pluviocabulary-new-york-times-schott">playing with my new words</a> - just thought I'd share. I particularly enjoyed being reminded of <a href="http://www.memrise.com/item/40834/petrichor-the-smell-accompanying-the-first-rain-af/?set=pluviocabulary-new-york-times-schott">petrichor</a>, and learning <a href="http://www.memrise.com/mem/91054/oh-no-i-got-caught-in-a-flaught/">flaught</a> and <a href="http://www.memrise.com/item/40852/tirl-the-sound-of-rain-on-a-roof/?set=pluviocabulary-new-york-times-schott">tirl</a>.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/pluviocabulary-rain-words-memrise-style/</guid>
      <pubDate>Sun, 03 Apr 2011 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Final dissertation</title>
      <link>https://www.gregdetre.com/blog/final-dissertation/</link>
      <description><![CDATA[<p>I realized I never uploaded the final version of my dissertation, '<a href="/research/dissertation - after revisions - 100503.pdf">Weakening memories by half-remembering them</a>'.</p>]]></description>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/final-dissertation/</guid>
      <pubDate>Mon, 07 Jan 2013 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Blogging with Wordpress and Emacs</title>
      <link>https://www.gregdetre.com/blog/blogging-with-wordpress-and-emacs/</link>
      <description>A simple workflow combining Emacs, Python, and WordPress lets you write posts in your favorite editor and publish with a single command: `M-x wordpress-publish-this-file`.</description>
      <content:encoded><![CDATA[<p>When it comes to tools, I am a hedgehog rather than a fox. I like to have a small number of tools, and to know them well.</p>
<p>I recently resolved to start writing again. But I decided that I needed to sharpen my pencils first.</p>
<p>I have <a href="/2007/09/12/entangling-the-ground-and-cloud/">plans on how publishing and sharing should work</a>. Grand plans. Too grand, perhaps.</p>
<p>So for now, I wrote something simple for myself. Now I can type away, press buttons... publish.</p>
<p>If you like Emacs, Python and Wordpress, this might be interesting to you too. If not, it certainly won't be.</p>
<p><a href="https://github.com/gregdetre/wordpress-python-emacs">wordpress-python-emacs GitHub repository</a></p>
<p>Most of the work is being done by this great <a href="https://github.com/maxcutler/python-wordpress-xmlrpc">Python/Wordpress</a> library. Thank you.</p>
<p>I wrote some simple Python scripts. One grabs all my existing blog posts. One looks through their titles, and checks them against the filename to see if this is a new post.</p>
<p>And then there's a very simple Emacs function that calls them to save/publish the current text file.</p>
<p>I could add more things: deleting posts, or a proper workflow for moving from draft to published. Maybe later.</p>
<p>I wrote this post, then hit M-x wordpress-publish-this-file.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/blogging-with-wordpress-and-emacs/</guid>
      <pubDate>Fri, 27 Sep 2013 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Two-level tagging</title>
      <link>https://www.gregdetre.com/blog/two-level-tagging/</link>
      <description>What if you could combine the flexibility of tags with the organization of folders? **Two-level tagging** lets items belong to multiple categories *and* creates hierarchies between those categories.</description>
      <content:encoded><![CDATA[<p>Have you ever had trouble deciding where to store a file on your hard disk? Or worse, had trouble finding it later?</p>
<p>When you store a file on your hard disk, you have to decide which folder to put it in. That folder can in turn live inside other folders. This results in a hierarchy, known in computer science as a *tree*.</p>
<p>The main problem with trees is that sometimes you want things to live in multiple places.</p>
<p>Tagging provides an alternate system. Tags are a lot like folders, except that things can belong to multiple tags. However, but the tags can't themselves belong to anything. So you have just one level of organisation with no nesting.</p>
<p>The main problem with single-level tagging is that it's too simple. We want to be able to use fine-grained categories (e.g. 'lesser spotted greeb') that themselves belong to higher-level categories (e.g. 'greeb', or even 'bird' or 'animal'). But we said that tags can't themselves belong to tags.</p>
<p>Described like this, perhaps the solution will seem obvious to you too. We want things to belong to multiple tags, and for those tags to sometimes belong to other tags.</p>
<p>I built this into <a href="/software/freex/">Emacs Freex</a>, my note-taking system.</p>
<p>For instance, I have tagged this blog post with '<a href="/tag/tagging/">Tagging</a>' and '<a href="/software/freex/">Emacs Freex</a>'. In turn '<a href="/software/freex/">Emacs Freex</a>' is tagged with '<a href="/software/">Software</a>'. So I can find this blog post later in various ways, including by intersecting '<a href="/tag/tagging/">Tagging</a>' and '<a href="/software/">Software</a>'.  </p>
<p>This gives you the best of both worlds: things belong to multiple categories, along with a hierarchy of categories.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/two-level-tagging/</guid>
      <pubDate>Tue, 29 Oct 2013 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Sanity checks as data sidekicks</title>
      <link>https://www.gregdetre.com/blog/sanity-checks-as-data-sidekicks/</link>
      <description>When complex data analysis fails, test something **dead simple** first—like distinguishing a blank screen from visual stimuli in brain data.</description>
      <content:encoded><![CDATA[<p>Abe Gong asked for good examples of '<a href="https://web.archive.org/web/20160410165155/http://blog.abegong.com/2014/02/wanted-good-examples-of-data-sidekicks.html?utm_content=bufferdd80e&amp;utm_medium=social&amp;utm_source=twitter.com&amp;utm_campaign=buffer">data sidekicks</a>', i.e. <em>"i.e. small, curated data" that "can accelerate analysis, solve cold start problems, and simplify complicated data pipelines"</em>.</p>
<p>My response was <em><a href="https://twitter.com/gregdetre/status/431192954808451073">"brain-data sidekick: sanity-check you can classify blank screen vs stimuli"</a></em> - <em><a href="https://twitter.com/gregdetre/status/431199600033026048">e.g. when classifying mental states with fMRI, if you can't tell stimuli vs none, probably something awry upstream in your pipeline</a></em>.</p>
<p>I still haven't got the hang of distilling complex thoughts into 140 characters, and so I was worried my reply might have been compressed into cryptic nonsense. Here's what I was trying to say:</p>
<p>Let's say you're trying to do a difficult classification on a dataset that has had a lot of preprocessing/transformation, like fMRI brain data. There are a million reasons why things could be going wrong.</p>
<p>All successful analyses are alike, but every unsuccessful analysis is unsuccessful in its own way (sorry, <a href="http://en.wikipedia.org/wiki/Anna_Karenina_principle">Tolstoy</a>).</p>
<p>Things could be failing for meaningful reasons, e.g.:</p>
<ul>
<li>
<p>the brain doesn't work the way you think, so you're analysing the wrong brain regions or representing things in a different way</p>
</li>
<li>
<p>there's signal there but it's represented at a finer-grained resolution than you can measure.</p>
</li>
</ul>
<p>But the most likely explanation is that you screwed up your preprocessing (mis-imported the data, mis-aligned the labels, mixed up the X-Y-Z dimensions etc).</p>
<p>If you can't classify someone staring at a blank screen vs a screen with something on it, it's probably something like this, since visual input is pretty much the strongest and most wide-spread signal in the brain - your whole posterior cortex lights up in response to high-salience images (like faces and places).</p>
<p>In the time I spent writing this, <a href="https://x.com/AbeGong/status/431201791976288256">Abe had already figured out what I meant :)</a>.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/sanity-checks-as-data-sidekicks/</guid>
      <pubDate>Wed, 05 Feb 2014 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>"Oh, that should be easy - maybe a few minutes..."</title>
      <link>https://www.gregdetre.com/blog/oh-that-should-be-easy-maybe-a-few-minutes/</link>
      <description>We humans are walking sacks of blood, bile and bias, and estimating how long things will take brings out the worst in us.</description>
      <content:encoded><![CDATA[<p>Hearing those words makes me feel like I'm tied mutely to a railway track, unable to scream for help as a train thunders towards me. We humans are walking sacks of blood, bile and bias, and estimating how long things will take brings out the worst in us.</p>
<p>A product manager recently asked me if one can get better at knowing whether things are easy or hard, and how long they will take. The good news is that with practice, you can help people estimate much better with your help than they would on their own.</p>
<p><strong>Understand the problem you're trying to solve.</strong></p>
<p>If you don't understand the problem well enough, you're certainly blind to its potential complexities. Product managers are often in a *better* position than anyone else!</p>
<p><strong>Understand what's involved in the proposed solution(s).</strong></p>
<p>This can be the trickiest part for non-engineers, because the details of the solution may sometimes be pretty arcane. Here's what you can do:</p>
<ul>
<li>
<p>You can go a long way by asking good questions about how things work, and what's involved in the solution. Listen carefully to the answers. If they don't make sense, ask for a higher-level explanation, or from a different person. Explain it back - that will make sure you've got it right and help you internalise it. Take good notes. Over time, you'll start to see how the pieces interconnect, and what problems are similar to one another, and this will get easier and easier.</p>
</li>
<li>
<p>Don't ask for an estimate for the whole solution. Break the solution down into pieces, estimate the size of each piece, and add them back together. In my experience, <em>people can't reliably estimate how long things will take beyond a few hours - so if the estimates are much bigger than this, break the pieces down into smaller and smaller chunks</em>.</p>
</li>
<li>
<p>Be the <a href="https://en.wikipedia.org/wiki/Rubber_duck_debugging">rubber duck</a>!</p>
</li>
<li>
<p><em>Offer to pair-program with a developer during the unit testing</em>. You'll get a really deep understanding of how the system works, and where the difficulties lie. Better still, if you write your tests <a href="https://en.wikipedia.org/wiki/Test-driven_development">before writing your code</a>, your test suite provides a kind of score card for how close you are to a solution, and you'll reduce time spent in QA.</p>
</li>
</ul>
<p><strong>Be aware and on the alert for pitfalls and cognitive biases that lead to poor estimations</strong>.</p>
<p>Human beings tend to be lazy about thinking through all the pieces for a complete solution (just focusing on the major parts, or the interesting parts, and ignoring the detail or the final 20% to make things perfect that takes all the time). They also tend to focus on the best case (if everything goes right) and ignore all the things that might go wrong. You never know what will go wrong, but if you have a sense of some possible pitfalls, you can factor them into your estimate. Possible approaches:</p>
<ul>
<li>
<p>Start by asking out loud <em>'what are the hidden traps, complications, edge cases, difficulties or things that could go wrong. When we did similar things in the past, how long did it end up taking? Were there surprise pitfalls that made it harder than we anticipated?'</em> Or run a <a href="http://www.theguardian.com/lifeandstyle/2014/may/10/hindsight-in-advance-premortem-this-column-change-life">premortem</a>. You'll get much better estimates after this discussion.</p>
</li>
<li>
<p>Use <a href="https://en.wikipedia.org/wiki/Planning_poker">Planning Poker</a> as an estimation approach. Each person makes an estimate in isolation - this forces them to think things through, and avoids estimates being dominated by <a href="https://en.wikipedia.org/wiki/Anchoring">what was said first</a> or <a href="http://www.forbes.com/sites/derosetichy/2013/04/15/what-happens-when-a-hippo-runs-your-company/">most loudly</a>. The discussion afterwards creates an informed consensus view, and provides immediate feedback for people whose estimates are wildly off.</p>
</li>
<li>
<p>As a last resort: make an optimistic estimate and <a href="https://en.wikipedia.org/wiki/Hofstadter%27s_law">double it</a>.</p>
</li>
</ul>
<p><strong>Learn from feedback.</strong></p>
<ul>
<li>
<p>Force yourself (or the project team) to make an estimate in advance, then during the project retrospective, compare the actual time taken to the estimated time. That would be the best way for everyone to learn from feedback! <em>'We thought it was going to be X, but it turned out to be 2X'</em>.</p>
</li>
<li>
<p>If things take much longer than anticipated, ask how we could have predicted this in advance. That might help you avoid similar estimation mistakes in future.</p>
</li>
<li>
<p>Notice if certain kinds of tasks tend to take longer than anticipated.</p>
</li>
<li>
<p>Notice if certain people tend to be inaccurate, and give them feedback on this.</p>
</li>
</ul>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/oh-that-should-be-easy-maybe-a-few-minutes/</guid>
      <pubDate>Mon, 07 Sep 2015 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Listing things to look forward to</title>
      <link>https://www.gregdetre.com/blog/listing-things-to-look-forward-to/</link>
      <description>Making a list of things to anticipate felt like *mental physiotherapy* - unfurling a cramped limb that had been injured, atrophied but finally free.</description>
      <content:encoded><![CDATA[<p>I just made a list of things to look forward to. Like any deliberate deliberation, at first it took a slight push through the soft, invisible barrier of effort, but then things just started to flow.</p>
<p>I had worried I wouldn't be able to think of anything, that there'd be no colour even through a microscope.</p>
<p>It felt a little like mental physiotherapy - the unfurling of a cramped limb that had been injured. Atrophied still, but unpinioned, a growing sense of pins and needles, a shaky freedom... finally a stumble. But for a moment, just a moment, I felt free, like the sun had come out, like the wind was blowing through the trees.</p>
<p>But the more things I decided to think were worthy of being anticipated positively, the more other things seemed to be too. I could smell a faint tang of joy in the air and it will sustain me for a good while longer.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/listing-things-to-look-forward-to/</guid>
      <pubDate>Wed, 30 Sep 2015 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Mindfulness in scuba diving</title>
      <link>https://www.gregdetre.com/blog/mindfulness-in-scuba-diving/</link>
      <description>Beneath the surface, diving transforms into a meditation where **breath controls buoyancy** and the alien underwater world demands complete presence in the moment.</description>
      <content:encoded><![CDATA[<p>Diving is mindful.</p>
<p>Diving is about the breath. Breathe in, rise. Breathe out, sink. Breathe, stay.</p>
<p>The underwater world is captivating - colourful, beautiful, ever-changing. The underwater world is serene - slow, smooth, curved. The underwater world is indifferent - endless, alien, incomprehensible. The underwater world is meditative - quiet, isolated, but embodied.</p>
<p>Diving keeps you in the moment.</p>
<p>Diving encourages economy of movement - gentle, minimal, sinuous, billowing, winnowing, wafting, floating. The different physics encourages a separation from and deliberate consideration of the body and environment.</p>
<p>The moment of entry is exhilarating. The return to the sky and ground is welcome. The familiar has become surprising for a moment. The rest of the day is a little more reflective, a little more conscious.</p>
<p>Diving is mindful.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/mindfulness-in-scuba-diving/</guid>
      <pubDate>Thu, 01 Oct 2015 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Presentations at BarCamp Tampa 2015</title>
      <link>https://www.gregdetre.com/blog/barcamp-tampa-2015/</link>
      <description>Two presentations from BarCamp Tampa 2015: mastering Python unit testing for better code, and **killing your crusty old PHP system** through seamless replatforming.</description>
      <content:encoded><![CDATA[<p><a href="/wp-content/uploads/2015/10/presentation-barcamp-tampa-python-151017.pdf">Unit testing in Python  - better code faster</a>.</p>
<p><a href="/wp-content/uploads/2015/10/presentation-barcamp-tampa-replatforming-1510172.pdf">The best way to kill and bury your crusty old PHP system</a> - replatforming a legacy system without your users noticing.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/barcamp-tampa-2015/</guid>
      <pubDate>Sat, 17 Oct 2015 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Todo Zero</title>
      <link>https://www.gregdetre.com/blog/todo-zero/</link>
      <description>What if you finished each day with **nothing** left on your todo list? Like a juggler with six balls but only two in hand, focus on just one or two tasks at a time.</description>
      <content:encoded><![CDATA[<p>What if I suggested that you finish each day with nothing left on your todo list? This is the only rule of <em>Todo Zero.</em></p>
<p>You might find yourself biting back some choice words. This sounds like unhelpful advice from someone with a much simpler life than yours.</p>
<p>Not so fast. Picture a world-class juggler with half-a-dozen balls in motion. How many balls do they have in their <a href="http://juggler.com/numbers-juggling-more-than-3-balls">hands at once</a>? None, one, or two. Never more than two. The remainder are in the air.</p>
<p>By analogy, work on just one or two things at a time. The remainder can be scheduled for some time in the future. In this way, it's very possible to finish what's currently on your list.</p>
<p>Otherwise, all of the competing priorities of a long list clamour for your attention. They clutter one another, making it impossible to focus. When you're pulled in many directions, you'll end up immobilized and demotivated.</p>
<p>At least that's what has happened to me. My implicit solution was to procrastinate until panic seized me, and then enjoy its temporary clarity of focus.</p>
<p>So, here's a recipe for <em>Todo Zero</em> that will take an hour or two to start with:</p>
<ul>
<li>
<p>Go through your todo list and pull out anything that's going to take less than 10 minutes.</p>
</li>
<li>
<p>Pick out the one or two jobs that you really want to tackle - these should be the most important or urgent things on your list. Break them down into pieces that you could tackle today if you really put your mind to it, and note them down.</p>
</li>
<li>
<p>Schedule everything else as future events in your calendar (I usually just assign them to a date without a time). Give yourself enough room before the deadline to finish them without rushing. Don't be over-optimistic about how many or how quickly you can work through them.</p>
</li>
</ul>
<p>So, that leaves you with quick tasks that take less than 10 minutes, along with the one or two most urgent/important jobs for today.</p>
<p>Marvel at your wonderfully shortened todo list. Look away, take a deep breath. Do not look at your email. Make a coffee. Feel a little calmer than you did, and enjoy it.</p>
<p>Now, let's do the same for your email.</p>
<ul>
<li>
<p>Install the <a href="http://boomeranggmail.com/">Boomerang for Gmail plugin</a>, and pay the <a href="http://www.boomeranggmail.com/subscriptions.html">$5/month personal subscription</a> for it (read this if you have hardcore <a href="https://www.quora.com/How-secure-is-boomerang-for-gmail">information security</a> requirements).</p>
</li>
<li>
<p>Find any emails that are going to take less than 10 minutes to reply to, and boomerang them for 2 hours' time.</p>
</li>
<li>
<p>Pull out one or two emails that are urgent or important, and boomerang them for 1 hour's time.</p>
</li>
<li>
<p>If you have the energy, boomerang each of your remaining emails for future times individually (tomorrow, a week away or a month away, depending on urgency). If you don't have the energy, just boomerang them wholesale for tomorrow morning.</p>
</li>
</ul>
<p>Stand up, and take a deep breath. Walk around for a few minutes, and make a cup of coffee. This is going really well.</p>
<ul>
<li>
<p>By the time you get back, you should be staring at a short todo list and a pretty clear inbox. [If anything new has landed, or any have boomeranged back, send them away for an hour. We need a clear head]</p>
</li>
<li>
<p>Now, let's dispatch the less-than-ten-minute odds &amp; ends tasks. Do some of them, most of them, all of them, it doesn't matter. Just a few, to get back a sense of momentum.</p>
</li>
<li>
<p>Your most urgent emails have boomeranged back. Deal with them.</p>
</li>
</ul>
<p>Take a break.</p>
<p>At this point, you're close to the point where you have a clean slate, and just your important tasks. You probably have some meetings and stuff. Have lunch. Refresh.</p>
<ul>
<li>
<p>Now, it's time to tackle those one or two important high-priority tasks-for-today.</p>
</li>
<li>
<p>Picture yourself at the end of the day, leaning back in your chair with your hands knitted behind your head, smugly. For that to happen, double down on those one or two most important things, and the rest can wait. You will feel great.</p>
</li>
<li>
<p>Don't do anything else today. Don't check your email if you can avoid it. Your goal is to boomerang away (by email or calendar) anything but them.</p>
</li>
</ul>
<p>With any luck, you made progress on those one or two most important tasks.</p>
<p>Armed with this approach, you can triage your own life. You can choose to focus on the most urgent or important things first, and ignore the rest. They'll shamble back when their time has come, and then you can dispatch them in turn.</p>
<p>P.S. There are a few tools that will help:</p>
<ul>
<li>
<p><a href="https://calendar.google.com/">Google Calendar</a> - add a new 'Todo' calendar, whose notifications are set by default to email you at the time of the event.</p>
</li>
<li>
<p><a href="http://www.boomeranggmail.com/">Boomerang for Gmail plugin</a> - allows you to banish emails for as long as you choose.</p>
</li>
<li>
<p>Any simple todo list app or text editor of your choosing. It doesn't matter.</p>
</li>
</ul>
<p>P.P.S. One final note. I can't juggle two balls, let alone six. So take that into account, seasoned with a pinch of salt, in reading this.</p>
<p>P.P.P.S. Of course, there is nothing that's original here. It's a death-metal-mashup of <a href="http://www.43folders.com/2006/03/13/inbox-zero">Inbox Zero</a> and <a href="http://gettingthingsdone.com/">GTD</a>. It's not always feasible to work like this. If you don't procrastinate, you probably don't need it. Etc.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/todo-zero/</guid>
      <pubDate>Wed, 02 Dec 2015 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Entrepreneur vs Manager vs Technician</title>
      <link>https://www.gregdetre.com/blog/entrepreneur-vs-manager-vs-technician/</link>
      <description>Gerber identifies three business personalities: the **Entrepreneur** who dreams, the **Manager** who orders, and the **Technician** who crafts. Which one dominates your work life?</description>
      <content:encoded><![CDATA[<p>In <a href="https://www.goodreads.com/book/show/81948.The_E_myth_Revisited">The E-Myth Revisited</a>, Gerber talks about three selves:</p>
<ul>
<li>
<p>The Entrepreneur dreams of the future, imagining possibilities, and attempting to shape (exercise control over) the world according to that vision.</p>
</li>
<li>
<p>The Manager attempts to impose order. Perhaps also to understand?</p>
</li>
<li>
<p>The Technician lives for the work, the flow, the present, the building, the joy of craft.</p>
</li>
</ul>
<p>It's easy to see elements of all these in myself. At times in my life, I've emphasised one or other, depending on the world and the people around me.</p>
<p>I suppose I get the least personal satisfaction from The Manager. That self eventually emerges as a necessary response to a high level of chaos, or sometimes when trying to impose structure on a team. But I find it takes enormous work to be the Manager for myself.</p>
<p>I've learned to be a Technician, mostly with programming.  Indeed it's a tremendous source of satisfaction, and I feel an urge tugging me towards it at all times. I know I'm not the best Technician at anything, but I'm good enough.</p>
<p>The Entrepreneur comes and goes, and it's very forceful when present.</p>
<p>I'm only a few pages into the book. Perhaps no one likes being a Manager? Or perhaps some people do, and it's necessary in at least small doses?</p>
<p>I have a hunch that his book is aimed at Technicians, whose urge is to focus on that. I'd imagine his point will be that you need a little of all of them...? It's going to be hard. I wonder if I can outsource some of them, or carve out time for each to some degree. I wonder what ratios he will advocate.</p>
<p>UPDATE on 2024-Sept-28 - I first wrote this for myself many years ago. It still feels largely true for me - though I hadn't considered that there is enormous satisfaction to be found as a Manager in helping others to develop.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/entrepreneur-vs-manager-vs-technician/</guid>
      <pubDate>Tue, 29 Dec 2015 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Why has Google open sourced TensorFlow?</title>
      <link>https://www.gregdetre.com/blog/why-has-google-open-sourced-tensorflow/</link>
      <description>Google's open sourcing of TensorFlow isn't altruism—it's strategy. The real goal may be **"pay by the parameter"** cloud computing that disrupts AWS.</description>
      <content:encoded><![CDATA[<p>I was sitting in a sun-warmed pizza restaurant in London last week talking about deep learning libraries. Everyone had their favourites. I was betting on <a href="https://www.tensorflow.org/">TensorFlow</a>, the new kid in town released by Google in late 2015. In response, a Torch fan pointed out that Google may invest in building up TensorFlow internally, but there's no reason for them to invest in the shared, external version.</p>
<p>This got me thinking - why <em>has</em> Google open sourced TensorFlow?</p>
<p>Naively, I usually assume that companies keep their most crown jewels proprietary while open sourcing the periphery. In other words, keep your secret sauce close to your chest - but share the stuff that's more generic, since it builds brand and goodwill, others may contribute helpfully, and you're not straightforwardly giving a leg-up to your direct competitors.</p>
<p>Google's approach to open source has been a little more strategic than this. Look at a handful of their <a href="https://developers.google.com/open-source/projects">major open source projects</a> - Android, Chromium, Angular, Go, Dart, V8, Wave, WebM. The motivations behind them are various:</p>
<ul>
<li>
<p>Android, Angular, Chromium, V8, Wave, WebM - creating a new version of an existing technology (free, better engineered, or faster) to disrupt an incumbent, or increase usage and thus drive revenue for Google's core businesses.</p>
</li>
<li>
<p>Go, Dart and the <a href="https://github.com/google">long tail</a> of minor projects are peripheral to their goals and serve less direct strategic interest.</p>
</li>
</ul>
<p>For TensorFlow to make sense and be worthy of long-term support from Google, it needs to fall in the former category.</p>
<p>It is indeed a new version of an existing technology - it's free, it's better engineered, though not yet faster.</p>
<p>So, is it intended to either disrupt an incumbent, or to increase usage and thus drive revenue for core Google businesses? I can only think of two possibilities:</p>
<ol>
<li>
<p>TensorFlow is intended to be a major strategic benefit for Android. Machine learning is going to power a wave of new mobile applications, and many of them need to run locally rather than as a client-server app, whether for efficiency, responsiveness or bandwidth reasons. If TensorFlow makes it easier to develop cross-platform, efficient mobile machine learning solutions for Android but not for iOS, that could give the Android app market a major boost.</p>
</li>
<li>
<p>TensorFlow is intended to be a major strategic benefit for Google's platform/hosting, and to disrupt AWS. Right now, it's pretty difficult and expensive to set up a cloud GPU instance. TensorFlow opens up the possibility of a granularly-scalable approach to machine learning that allows us to finally ignore the nitty-gritty of CUDA installations, Python dependencies, and multiple GPUs. Just specify the size of network you want, and TensorFlow allocates and spreads it across hardware as needed. This is why TensorBoard was part of the original implementation, and why AWS support was an afterthought. <strong>"Pay by the parameter"</strong>. If I had to guess, I'd say this is the major reason for open sourcing TensorFlow.</p>
</li>
</ol>
<p>I want something like the above to be true, because I want there to be a strategic reason for Google to invest in TensorFlow, and I want it to get easier and easier to develop interesting and complex deep learning apps.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/why-has-google-open-sourced-tensorflow/</guid>
      <pubDate>Fri, 22 Jan 2016 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Chatbots - from Rogerian psychotherapists to cognitive behavioral therapists</title>
      <link>https://www.gregdetre.com/blog/hackathon-profbot-technical/</link>
      <description>Building a **stateful** Slack chatbot in two days taught us that representing conversational logic as *data rather than code* opens exciting possibilities—despite our demo day disaster.</description>
      <content:encoded><![CDATA[<p><em>[Reposted from https://medium.com/we-are-big-health/chatbots-from-rogerian-psychotherapists-to-cognitive-behavioral-therapists-237c2ee3cfb6]</em></p>
<p>I've been obsessed by chatbots ever since I first played around with an Eliza clone that shipped with my Soundblaster sound card at the age of 12. I taught myself to program because I so badly wanted to build a chatbot of my own. My first efforts were very simple - they matched keywords in the user's input, and picked at random from a bag of canned responses. It worked as well as you'd expect.</p>
<p>The more I scratched at this problem over the years, <a href="/2006/04/18/the-turing-tournament-a-proposal-for-a-reformulation-of-the-turing-test/">the deeper I realized it to be</a>. Even as we recognise the deficiencies of the Turing Test, we must also recognise its genius - it is a necessary but not sufficient test of intelligence. In other words, it's just too dang hard and too specific (see <a href="http://lead.u-bourgogne.fr/people/french/turing.pdf">Robert French's brilliant 'Subcognition and the Limits of the Turing Test'</a>).</p>
<p>So, with a familiar sense of futility and excitement, I joined forces with Helena, Jess and Brandon to build a Slack-chatbot version of Sleepio's The Prof in our first Hackathon. We had two days from start to finish.</p>
<p>Now, the real version of The Prof is our animated Scottish sleep expert, powered by complex algorithms that took years to build. He features in our full Sleepio product and scientific papers, personalizing his advice for each user, and has been tweaked, tested and polished to a high sheen.</p>
<p>We decided from the get-go that The Profbot, his Slack-chatbot little brother, was going to be a considerably scruffier character.</p>
<p>I knew from my teenage experiments that <em>stateless</em> chatbots are very unsatisfying. Their responses were based solely on the most recent input from the user. They had no memory of what had been said before. I wanted the Profbot to be stateful, i.e. for each response to progress from previous responses.</p>
<p>Brandon was knee-deep in Python Slack APIs, and had made a discovery. We initially thought we'd have to set up an internet-facing webserver that Slack would push to whenever someone posted. That would in turn require something like a database to store state across requests. He realized that we could just poll Slack regularly from a local Python process, allowing us to store state simply in Python variables. Much simpler, and fine for a Hackathon!</p>
<p>We intended for The Profbot to hang out in an open Slack channel where anyone in the company could chat with him. This interactivity would be risky, but would be much more exciting to demo than a passive screenshare.</p>
<p>For this, The Profbot would need a wide repertoire of conversational narratives. Helena &amp; Jess had already begun work on a massive flowchart showing how he could progress through different paths.</p>
<p>In our real production environment, the mills of Sleepio grind slow but exceedingly fine. Making changes to The real Prof's logic requires a GitHub commit from an engineer, unit tests, code review, a QA cycle, and deployment... </p>
<p>For a Hackathon, we needed content development to proceed much faster. We wanted to be able to easily modify The Profbot as we learned what kinds of conversation paths worked and what didn't. And we wanted to be able to do this without any input from an engineer, otherwise we'd be bottlenecked in how much content we could develop.</p>
<p>My dream would have been to train a recurrent deep learning network on various conversations so that it could learn to respond accordingly and robustly. In principle this would have made The Profbot able to generalize better to inputs he hadn't seen while still maintaining state. There are papers showing some generalization from trained sentences to grammatically different but semantically similar sentences, but I knew in my heart of hearts that this wasn't feasible in two days, so we abandoned the plan.</p>
<p>We also looked into 3rd party services that might provide this for us, but couldn't find anything plug-and-play in the time. Do shout out in the comments if you know of anything that might do the job!</p>
<p>So we took a gamble on the architecture, and decided to try to represent the entire conversational logic as a finite state machine. This way, the conversational paths could be treated as data, read in from an external file in a simple domain-specific language, rather than as code.</p>
<p>This had a few major benefits. A simple deterministic finite state machine can be neatly visualized as a flowchart, with a single defined starting point. With a little help from <a href="https://pypi.python.org/pypi/xlrd">Xlrd</a>, <a href="https://pypi.python.org/pypi/pydot">Pydot</a> and <a href="http://www.graphviz.org/">GraphViz</a>, we were able to define an entire conversational path as an Excel file, immediately visualize it as a flowchart, and run it.</p>
<p>In fact, we had a still bolder hope. We wanted to use crowd-sourcing as a crowd-pleaser - we moved the Profbot's Excel file into Dropbox, so that anyone in the company would be able to add to it during the demo and see the Profbot's responses change in real-time.</p>
<p>There are disadvantages to representing things as a deterministic finite state machine.</p>
<ul>
<li>
<p>The most obvious is right there in the name - though stateful, the Profbot would respond pretty <em>deterministically</em>, i.e. in exactly the same way every time to the same inputs. This repetitiveness is a sure sign of a bot!</p>
</li>
<li>
<p>It made it difficult to deal with global options. For instance, we might want the user to be able to type 'tell me a joke' or 'help me sleep' at any time, and have the Profbot immediately shift down that path. However, if he was already deep in a different mode, we would have to create edges from every node to the 'tell me a joke' node for it to be globally applicable. We didn't solve this in time.</p>
</li>
<li>
<p>Ultimately, the paths were still based on a simple disjunctive matching algorithm that looked for any of a variety of keyphrases (or a special '*' wildcard that matched against anything) to determine which state to move into next. This didn't provide much flexibility or robustness, but that was too hard a problem to address meaningfully. </p>
</li>
</ul>
<p>We'll discuss one subtle difficulty. We wanted The Profbot to pick randomly from a bag of responses when flummoxed, such as 'I cannae make head or tail of ye' and 'Ach, I'm afraid I didn't catch that', but remain in the same state he was in.</p>
<p>At this point, I made a bad architectural decision that created a great deal of complexity for little gain. I decided that The Profbot would need a stack. That way, when the user inputted something The Profbot didn't recognise, we could push the 'dunno' state to the top of his stack. He'd respond accordingly, pop off 'dunno', and his prior state would now be at the top, ready to respond to the next input. The intention was to allow him to be able to stash his current state and get into stateful digressions. We realized afterwards that we could have managed without this functionality, and we wasted precious hours getting it to work robustly.</p>
<p>With just a couple of hours to go, we had a working finite state machine in the terminal. Helena &amp; Jess had built a rich set of conversational paths in Excel - you could almost hear The Prof's soft, witty Scottish brogue as you read them. Brandon had built a beautiful wrapper to the Python Slack API. But we hadn't yet knitted it all together. We thought we had everything working with half an hour to go, and set about writing a script for the demo.</p>
<p>Here, our story ends in disaster. We had just enough time to QA the core functionality but there were a number of things we didn't have time to try (in part because of my bad decision about the dunno-stack). So when the stopwatch of our 5 minutes began, we invited the company to pour into The Profbot's home. Immediately, people started delightedly typing at him while others joined, and he crashed over and over.</p>
<p>By the time we had figured out the issue (something to do with people joining the room while he was running), we had wasted 3 of our precious 5 minutes. We rushed through the scripted part of our demo and explanations of what we had built, but we didn't have time to really explain how people could modify his logic, and all the obvious deficiencies of a simple keyword-matching strategy were very evident as people tried feeding him increasingly outlandish inputs.</p>
<p>So, no prize for us. Most of all, I'm excited about the possibilities of representing conversational logic as data rather than code. I don't think a finite state machine is rich enough to implement a lot of the logic in the real program, so I'm musing about more powerful formalisms. But we had an enormous amount of fun building The Profbot, and seeded new ideas for the future. I'm already looking forward to our next Hackathon!</p>
<h2 id="future-ideas">Future ideas</h2>
<p>Instead of an Excel file in Dropbox, we would have preferred to use a Google Sheet, but I already had lots of Xlrd code I could crib from to speed up the development process.</p>
<ul>
<li>
<p>We considered D3.js, but PyDot + GraphViz is very easy to work with, all Python, and I had code lying around that was easy to repurpose. Plus, I worried that D3's default force-directed graph would generate a different solution every time we made a change, whereas GraphViz's solutions tend to be pretty stable.</p>
</li>
<li>
<p>In the future, I'd love to try using TensorBoard. It's interactive, and deals really well with collapsing detail (so that you can see the high level but burrow down a path if you want to), but it would have been too much work to figure out how to decouple TensorBoard from TensorFlow objects for use with arbitrary inputs.</p>
</li>
</ul>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/hackathon-profbot-technical/</guid>
      <pubDate>Tue, 16 Feb 2016 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Soft and hard skills</title>
      <link>https://www.gregdetre.com/blog/soft-and-hard-skills/</link>
      <description>After years focused on management over coding, wrestling with TensorFlow raises a familiar question: **was this always this hard, or have I gone soft?**</description>
      <content:encoded><![CDATA[<p>PUBLISHED https://medium.com/@gregdetre/getting-soft-or-just-soft-skills-fd73146fe477#.pbmzfexl9</p>
<p>I'm sitting with a glass of red wine and a full-screen Jupyter Notebook window full of TensorFlow code open. Now, it may be the red wine, but it's just plain difficult to get things to work.</p>
<p>After more than a PhD's worth of wrestling with code and algorithms, I felt up to the challenge. But in between, there's been a few years where the emphasis has been on management rather than code.</p>
<p>So now, I find myself wondering whether it was always this hard, or whether my focus on soft rather than hard skills has left me, well, soft.</p>
<p>After a few months of cognitive behavioural therapy, I think I'm able to recognise this thought as a distortion. Yes, I'm sure I am a little rusty. But it doesn't take much to remind myself that this stuff was always difficult. Indeed, if it wasn't difficult, it wouldn't be satisfying. And if wasn't difficult, everyone would be doing it.</p>
<p>I think probably what I miss is the sense of flow, born of hard-won familiarity. The reality of management is that there's just less time in the day for developing that muscle-memory readiness. </p>
<p>So, I persevere, inch by inch, and treasure the moments of flow when they come.</p>
<p><em>(first published on <a href="https://gregdetre.medium.com/getting-soft-or-just-soft-skills-fd73146fe477#.pbmzfexl9">Medium</a>)</em></p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/soft-and-hard-skills/</guid>
      <pubDate>Sun, 13 Mar 2016 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Friendships and memory effects</title>
      <link>https://www.gregdetre.com/blog/friendships-and-memory-effects/</link>
      <description>The longer you've known something, the deeper its roots in your mind—it reassures me to think that friendships work the same way.</description>
      <content:encoded><![CDATA[<p>Within a few steps of getting off the plane at Newark on my way to Princeton, I could feel myself traveling back in time as well as in space.</p>
<p>I spent 7 years in Princeton as a research assistant and PhD student, thinking about human forgetting and new computational approaches to studying the brain.</p>
<p>I've been back once or twice in the 6 years since I graduated, but a lot has changed. For starters, I'm married now and accompanied by my wife. I manage the engineering team at Big Health, rather than working as a researcher in a lab. I spend most of my time in Florida and San Francisco, rather than Princeton and New York. And I only get to see the friends and colleagues from grad school when our paths cross sporadically and briefly.</p>
<p>So perhaps it's no surprise that I found myself thinking about memory. I was surrounded by old friends and old stomping grounds. And I spent years here thinking about how our recollections degrade, and how they are triggered.</p>
<p>Indeed, it's easier to remember things when you're in the same context that you learned them. This is why it can help to study in the exam room - by laying down your memories in the exam room, you'll find them easier to recall later on the test. Of course, there's a flip side to this - there may be a long-term benefit to laying down memories in a variety of contexts if you want them to be broadly accessible to you later. On the one hand, it's easiest to slip back into old habits with old friends from Princeton in Princeton - but on the other, seeing them in fresh contexts plants those new memories in a wider furrow.</p>
<p>The effects of context can also play tricks. Seeing people in places where you knew them long ago can make it easier to recall the distant memories from that place, than the recent memories laid down in other contexts. This compounds the likelihood of asking questions about things that have long since changed!</p>
<p>Perhaps most of all, I take the most solace in the spacing effect. In short, the spacing effect suggests that the longer you've known something, the deeper its roots in your mind. That is, the longer you've known it, the less often you have to remind yourself of it. It reassures me to think that friendships work the same way.</p>
<p><em>(first published on <a href="https://gregdetre.medium.com/friendships-and-memory-effects-e7491b2fb48">Medium</a>)</em></p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/friendships-and-memory-effects/</guid>
      <pubDate>Tue, 05 Apr 2016 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Small wins (list)</title>
      <link>https://www.gregdetre.com/blog/small-wins/</link>
      <description>Ten simple actions that can brighten your day, from learning song lyrics to waterproofing your favorite shoes. Small changes that create **disproportionate satisfaction** in daily life.</description>
      <content:encoded><![CDATA[<ol>
<li>
<p>Learn the lyrics to your favourite song, so you can sing along to it as you walk down the street.</p>
</li>
<li>
<p>Oil a door handle that's always been just a tiny bit awkward, with a can of WD-40 that has a long nozzle.</p>
</li>
<li>
<p>Go to a pottery exhibition and buy a mug that feels really good in your hands. Enjoy drinking out of it every day.</p>
</li>
<li>
<p>Pick one of your favourite old books and put it in the bathroom to dip in and out of when you feel like it.</p>
</li>
<li>
<p>Put just one picture up on the wall. If you don't have picture hooks, buy them now and then you'll be ready next time.</p>
</li>
<li>
<p>Notice another person in your organisation that you suspect wants to start a fire, and arrange to have lunch with them.</p>
</li>
<li>
<p>Spray your favourite leather shoes to make them waterproof, and then smile smugly and gratefully next time you accidentally step in a puddle with them on.</p>
</li>
<li>
<p>Create a playlist of songs that cheer you up, to have ready just in case.</p>
</li>
<li>
<p>Carry a small Ziploc bag with a couple of tablets of medicines you find useful (e.g. Alka Seltzer for excess, Tums for heartburn, Zirtek &amp; Beconase, for hayfever, Ibuprofen for headaches, Strepsils for sore throats). Replenish when you use one, and add to it when you wish you had something.</p>
</li>
<li>
<p>Start a list of 'Small wins' of your own, and add the first item to it.</p>
</li>
</ol>
<hr/>
<p>In <a href="https://blog.strategicedge.co.uk/2017/02/small-wins-23.html">grateful homage to Nicholas Bate</a>.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/small-wins/</guid>
      <pubDate>Sun, 18 Nov 2018 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Fun things to do for free in London (list)</title>
      <link>https://www.gregdetre.com/blog/fun-things-to-do-for-free-in-london/</link>
      <description>From roller skating through royal parks to staging impromptu street dramas, London offers countless free adventures for the creatively curious.</description>
      <content:encoded><![CDATA[<p>[together with <a href="http://carolinavasilikou.com">Carolina</a>]</p>
<ul>
<li>
<p>Roller skate from Regents Park to Hyde Park</p>
</li>
<li>
<p>Walk along the canal behind London Zoo and try and guess the animals from the backs of their cages</p>
</li>
<li>
<p>Peer into the houseboats along Little Venice and tell stories about the people inside</p>
</li>
<li>
<p>Sprint along the Millennium Bridge and feel it vibrate underneath you</p>
</li>
<li>
<p>Take a friend's kid to the Launchpad in the Science Museum</p>
</li>
<li>
<p>Stage an improvised drama on the street, walking</p>
</li>
<li>
<p>Invite your friends for a football game in the park</p>
</li>
<li>
<p>Have a campfire - shout out if you know where it's possible to set up a campfire legally in London</p>
</li>
</ul>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/fun-things-to-do-for-free-in-london/</guid>
      <pubDate>Sun, 02 Dec 2018 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>How to Cheat at Data Science (With Help from Centaurs and the Wizard of Oz) - presentation at Unbound 2019 (VIDEO)</title>
      <link>https://www.gregdetre.com/blog/unbound-2019/</link>
      <description><![CDATA[<p>I'll be at <a href="https://unbound.live/London/agenda#/">Unbound London</a> for a talk at 12.05 on Thu 18th July 2019 on <a href="https://www.youtube.com/watch?v=4kcjH4v6yrQ">How to Cheat at Data Science (With Help from Centaurs and the Wizard of Oz)</a> .</p>]]></description>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/unbound-2019/</guid>
      <pubDate>Thu, 18 Jul 2019 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>LTSF 2019</title>
      <link>https://www.gregdetre.com/blog/ltsf-2019/</link>
      <description><![CDATA[<p>Hi everyone. I’ll be at <a href="https://www.learningtechnologies.co.uk/learning-tech-summer-forum/ltsf-conference">LTSF 2019 (Learning Technologies Summer Forum)</a> in London on 11th July 2019.</p>
<p>If you’re interested in the slides, <a href="https://www.linkedin.com/in/gregdetre/">drop me a line on LinkedIn</a>.</p>]]></description>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/ltsf-2019/</guid>
      <pubDate>Fri, 19 Jul 2019 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>How to remember names at a party</title>
      <link>https://www.gregdetre.com/blog/how-memory-works/</link>
      <description>The biggest reason we forget names isn't poor memory—it's that we weren't paying attention in the first place while worrying about spinach in our teeth.</description>
      <content:encoded><![CDATA[<p>(First published at <a href="https://www.yourheights.com/blogs/health/how-memory-works">Heights</a>)</p>
<p>Can't remember the name of the person you just met at your party? We can help with that.</p>
<h2 id="why-do-we-forget-things">Why do we forget things?</h2>
<p>There are many reasons why we forget things. Let's consider a minor but excruciating example - even though you only met them ten minutes ago, you’ve forgotten the name of the person you’re talking to at a party and you know you’re going to need to introduce them.</p>
<p>Fortunately, there’s a good deal we can do about this if we choose. Perhaps the biggest reason we forgot their name was that we simply weren't paying enough attention in the first place.</p>
<p>While they were introducing themselves, we were focused on whether there’s spinach stuck in our teeth, the right stance to best show off our paunch, and what we’re going to say next. (Find out <a href="https://www.yourheights.com/blog/braincare-club-befriend-your-inner-critic-to-strengthen-your-confidence/">how to silence those critical thoughts</a> ).</p>
<p>In that flurry of self-conscious unawareness, our attention was elsewhere. Like a fragile snowflake turning immediately to slush on tarmac, the memory never got a chance to land, let alone settle.</p>
<h2 id="so-what-can-we-do">So what can we do?</h2>
<p>Well, we can follow Dale Carnegie’s immortal, self-serving advice to use their name in conversation as soon as possible… “So, Bill, how are things going at work, Bill?”. This forces us to listen. And more importantly, to use a name is to practice remembering it. This is known as <em>active recall</em>.</p>
<p>So, you’ve repeated their name to yourself soon after hearing it, even if only in your own head. That brings us to the second thing about memory. Memories need repetition in the same way that seeds need nourishment in order to grow.</p>
<p>At first, they benefit most from little reminders often. Once you’ve known something for a long time, those reminders can grow further apart. Tolkien said it best: <em>deep roots are not reached by the frost.</em></p>
<h2 id="how-to-boss-it">How to boss it</h2>
<p>So, if you really wanted to do a better job of remembering names, then you'd remind yourself almost instantly, and then right after the conversation ended, and then maybe on the way home, and then after a few days, and a few weeks... Schedule those intervals between reminders to grow exponentially for optimal recollection.</p>
<p>If we wanted to go further, we could bring out the big guns, the sort of mnemonic munitions that competitive memorisers rely on - <em>vivid mnemonics</em> .</p>
<h2 id="vivid-mnemonics">Vivid Mnemonics</h2>
<p>My name is Greg. Take a look at my face. It’s a punchable face, to be sure, but ignore that for now.</p>
<p>Try an auditory association - imagine me as Gr<em>eg</em> with a wooden p<em>eg</em> l<em>eg</em>. In other words, make a rich, vivid, visual association between the person’s face and their name. Caricature their face, distort their name, and build a bridge between them. The more obscene, funny, ridiculous or sexual you can make your image-bridge, the better.</p>
<p>And though it seems like quite a lot of work to go around imagining Jemima wearing ludicrous pyjamas, or Sam covered in sand, it will double the likelihood of remembering their name later.</p>
<h2 id="remember-what-you-just-read">Remember what you just read?</h2>
<p>So, listen in the first place, actively recall soon after, and remind oneself periodically. Beyond that, the trade-off between effort and benefit becomes less clear. Really effective mnemonic techniques involve a good deal of concentration and imagination.</p>
<p>You have to make a lot of effort if you want to be able to reliably bed down lots of names. You may be better off focusing your energies on being a good listener and interlocutor, or on being more accepting of your own faults, or on assembling a roster of urbane self-deprecating apologies to smooth over the occasional inevitable memory hiccup.</p>
<p>And if you've still not won over your new friend, <a href="https://www.heights.com/blogs/health/the-science-of-smiling?srsltid=AfmBOorGR_o4_ly6ChP-ZG1kFuNYjOM3nzJCNi0p4pb_R11NwUoMcYFb">just give them a smile</a> —at the very least you will feel great.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/how-memory-works/</guid>
      <pubDate>Wed, 23 Oct 2019 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Abstractions are distractions to an AI-assisted programmer</title>
      <link>https://www.gregdetre.com/blog/abstractions-are-distractions-to-an-ai-assisted-programmer/</link>
      <description>Might AI assistants, who can type faster and memorise vast codebases, actually perform better with *less* abstract programming languages and frameworks?</description>
      <content:encoded><![CDATA[<p>Hypothesis: <em>In a world of AI programming assistants, there will be less need for high-level software languages &amp; frameworks.</em></p>
<p>Right now, we human programmers like higher-level libraries because they save us keystrokes, and perhaps they’re <a href="https://paulgraham.com/avg.html">more powerful</a>. All other things being equal, fewer keystrokes means we can write code more quickly, with <a href="https://stackoverflow.com/questions/2898571/basis-for-claim-that-the-number-of-bugs-per-line-of-code-is-constant-regardless">fewer bugs</a>.</p>
<p>But all other things are not equal, because cognitive load matters a lot. For humans, there is a complicated relationship between cognitive load and abstraction. Too low-level is bad - it’s almost always harder to write software if we have to manage our own memory, implement all our own data structures from scratch, or indeed type out our code in binary. But at the same time there are costs to writing at a higher-level - at the very least, we have to worry about <a href="https://www.joelonsoftware.com/2002/11/11/the-law-of-leaky-abstractions/">leaky abstractions</a>, and the <a href="https://niltag.net/essays/racket_monad.html">effort to learn them</a>.</p>
<p>But my topic here is not the ideal level of abstraction for <em>human</em> programmers, but rather for AI programmers (being supervised by humans).</p>
<p>Imagine you have a friend who can type 100 times faster than you, and can recite most of the code from GitHub from memory. Your friend is really good at assembling well-understood building blocks, especially if there’s a commonly-used pattern.</p>
<p>Well in that case, your friend may be better off relying on less abstract, and less leaky libraries, even if it means writing lots of almost-copied-and-pasted boilerplate code. Especially if those libraries are very popular, well-documented, robust, slowly-changing, and there's lots of public code with common patterns to crib from.</p>
<p>But your friend’s code, written with less powerful abstractions, is harder to extend and modify, you say? Well, your friend types so fast that when the needs of the software change, they just throw a bunch of their defunct code out and rewrite it from scratch. (And they <a href="https://x.com/ylecun/status/1760293812486725696?lang=en-GB">might even benefit from typing more keystrokes</a>. This reminds me a little of how AI fighter pilots might be at an advantage by being <a href="https://www.newyorker.com/magazine/2022/01/24/the-rise-of-ai-fighter-pilots">more willing to play chicken than human pilots</a>.)</p>
<p>This may be the situation we find ourselves in with AI assistants.</p>
<p>This whole post grew out of a throwaway comment from <a href="https://corecursive.com/029-learn-to-think-andy-hunt/">Andy Hunt on the CoRecursive podcast</a>. He laughingly said that <em>this is the slowest the rate of change will ever be, with reference to new JavaScript frameworks</em>. While it is certainly true that this is the youngest you will ever be, maybe maybe in a world filled with AI assistants, there will be reduced pressure and incentive to seek new and better abstractions. So maybe today is, in fact, the <em>fastest</em> the rate of change will ever be for software frameworks.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/abstractions-are-distractions-to-an-ai-assisted-programmer/</guid>
      <pubDate>Sat, 10 Aug 2024 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Worst scientific experiment ever?</title>
      <link>https://www.gregdetre.com/blog/worst-experiment-ever/</link>
      <description>I changed almost every variable in human life simultaneously—diet, community, silence, meditation, environment—then felt frustrated as a scientist trying to identify the active ingredient.</description>
      <content:encoded><![CDATA[<p><em>Aim</em>: to understand what makes for a good life.</p>
<p><em>Methods</em>: The subject spent two weeks in <a href="https://plumvillage.org/">Plum Village</a>, a Zen Buddhist centre in southwest France. The weather was often sunny. Most of the day was spent outside, surrounded by forest, sometimes farming. The subject lived in a community of 250 people, include monastics, in communal housing. They were assigned to a small group that regularly sat in a circle together sharing from the heart, with occasional catchups in depth with an old friend. They meditated twice a day, walking everywhere at a snail's pace attempting to be mindful, while hearing a lot about bodhisattvas. They drank uncaffeinated tea, ate vegan meals three times a day, and mostly kept silent from late evening until after breakfast. Sources of entertainment included chatting in the tea house, journalling, reading (mostly books by Thich Nhat Hanh), going for a run, lying on the grass, with occasional naps, campfire singalong, and football game.</p>
<p>I’m not saying this is everyone’s cup of tea. And indeed, I often felt bored and restless.</p>
<p>Compare with the control condition: living with my partner in a two-person flat in London, working from home on a computer, and consuming a lot of meat, chocolate, coffee, Netflix, and podcasts.</p>
<p><em>Results</em>: I came away feeling so much happier and more human than when I'd arrived - I am not exaggerating when I say that I felt like a different person. My partner definitely noticed it. And some of the new habits and behaviours stuck, to varying degrees. So let’s consider that to be a pretty enormous effect.</p>
<p><em>Discussion</em>: But I couldn’t help but feel a little frustrated, as a scientist. I had changed almost every single variable you can think of in a human life. So how was I supposed to tell which was most causal, which was the active ingredient? We don’t even know whether we’re talking about one or two variables that matter a lot, or many small effects that add up, or even some kind of multiplicative interaction between them (e.g. perhaps something special happens when you combine meditation with veganism, and then bam!).</p>
<p>Worst scientific experiment ever?</p>
<p>Well, maybe, maybe not. I'll say three things about it.</p>
<ol>
<li>
<p>It replicated. I went back a year later and had a similar experience.</p>
</li>
<li>
<p>It was an enormous effect. It’s much easier to work backwards from a large uncontrolled effect to figure out which variables are driving it, than to be stuck tweaking lots of things carefully that don’t make any difference.</p>
</li>
<li>
<p>There is some information there. Above all, it was a very reassuring reminder that changing environmental factors <em>can</em> change my internal state, in under two weeks. And if I look back at other periods or experiences that have also felt really good, I can start to see which variables are common to them.</p>
</li>
</ol>
<p>When I went back to Plum Village the second time, I knew that the immediate effects would be temporary. The winds of morale there had gusted me a little way up, so I wanted to grab hold where I could, before rolling back down. The quiet voice inside had been urging changes for a long time, even if it was hard to hear or act on them. For a moment, jolted into a higher, more self-aware energy state, I had extra affordances available to me. I tried to plant seeds that would take root and grow in my life - applying for jobs, signing up for voluntary activities, seeking out communities… We’ll see if it works.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/worst-experiment-ever/</guid>
      <pubDate>Sun, 18 Aug 2024 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Buy Microsoft stock as they add AI to every one of their products</title>
      <link>https://www.gregdetre.com/blog/buy-microsoft-stock-as-they-add-ai-to-every-one-of-their-products/</link>
      <description>Microsoft makes AI integration *just about possible* across their product suite—frustratingly clunky today, but enterprise customers will choose familiar pain over third-party complexity.</description>
      <content:encoded><![CDATA[<p>I started consulting on AI innovation for an enterprise manufacturing company - a day later, I went out and bought a bunch of Microsoft stock.</p>
<p>Why? My client always looked for the Microsoft solution first. And I could see an endless march of Microsoft press releases, weaving GPT-4 into into their entire product portfolio. I'm betting they are representative of the way many large company IT departments think.</p>
<p>Secondly, Microsoft’s bet on OpenAI will continue to pay off - it’s easy to forget that OpenAI had an internal version of GPT-4 working <em>2 years</em> ago. So even if the pack has caught up with GPT-4-based models, that’ll change when the 5th shoe drops.</p>
<p>Transformers will change the face of knowledge work, and Microsoft can see it. They can see that we’ll readily use Copilot for Excel to analyse our data, Copilot for Dynamics to observe and improve customer interactions, Copilot for Power Automate to do our repetitive pointing and clicking, Copilot for Sharepoint to open up our institutional memory… And each customer company, team, and employee will build endless custom Copilots with a drag-and-drop low-code flowchart interface, thankfully assisted by Copilot for Copilot. It’s a grand vision, and I’m bullish about the value this will create in the long-term.</p>
<p>But in the here-and-now, I sat down to try and build stuff with Microsoft’s AI products. I started simply with an Excel file of HR survey data, nestled in OneDrive or SharePoint or some other double-barrelled Microsoft storage hell, and we set out to find sentiments and trends and clusters from some scattered, hasty, wordy, multilingual employee free-text responses. You know the drill.</p>
<p>But soon, I could feel bile building in my throat, bile with a flavour … that I can only describe as tasting like ‘Microsoft Windows’. I struggled to even attach my Excel file to the Bing For Work chatbot, struggled to make it read the Excel file format, struggled to point Copilot for Excel to the data right in front of it, to deal with empty cells, to generate and run code. Given that Excel is a flagship product, Copilot for Excel is really flimsy (as of summer 2024). More often than not, it would simply shrug and say “I can't help you with that”.</p>
<p>I had been spoilt by ChatGPT’s relentless improvements, which would have swallowed this problem whole in 30 seconds. But our information security team were (rightly IMHO) wary of handing business-sensitive data to OxymoronAI.</p>
<p>We also tried building a custom search. After a couple of person-weeks, with help from a couple of Azure consultants, we almost had a prototype working. But it was bewildering, it didn’t work that well, and we knew we still had to face the remaining 80% of the effort to productionise it. Azure’s power and complicatedness makes the easy things <em>just about possible</em> - while the hard things are also <em>just about possible</em>.</p>
<p>So what did I conclude from this? Microsoft's fighting on too many fronts. They're trying to integrate AI into their entire product portfolio at the same time, while building a new data centre every three days, and who knows what else behind the scenes. They’ve never had great taste for interfaces. Microsoft products often become a churning quicksand, accreting new features and occluding key functionality with each new version. The frenetic pace of AI progress has only exacerbated that.</p>
<p>And yet I have kept my newly-acquired Microsoft stock, because they will get it right <em>enough</em>. Companies will prefer to painstakingly build a Microsoft-based solution that no one can get fired for. The alternative would be to deal with the integration and information security hassles of working with third-party products. Microsoft makes $100b/year from Office and Azure, and AI services will double that for them in the next few years.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/buy-microsoft-stock-as-they-add-ai-to-every-one-of-their-products/</guid>
      <pubDate>Sun, 01 Sep 2024 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>The real reason we can’t define 'Artificial General Intelligence'</title>
      <link>https://www.gregdetre.com/blog/the-real-reason-we-cant-define-artificial-general-intelligence/</link>
      <description>Could it be that **any clean, broad definition of general intelligence would exclude *us***? Our intelligence is only somewhat general, existing on a continuum rather than being binary.</description>
      <content:encoded><![CDATA[<p>Could it be that the real reason we struggle to define Artificial General Intelligence (AGI) is that <strong>any clean, broad definition of general intelligence would exclude <em>us</em></strong>?</p>
<p>Intelligence is hard to pin down, but I like these two related definitions:</p>
<ol>
<li>
<p>Flexible, goal-directed behaviour</p>
</li>
<li>
<p>Efficient skill acquisition (from Francois Chollet)</p>
</li>
</ol>
<p>I'm going to reformulate these as: <em>learning how to do something without much to go on, and then applying it in new ways</em>.</p>
<p>How do we measure up against this?</p>
<p>Yes, we can certainly acquire new skills that we didn't evolve for. We are comparatively flexible. The AIs that <em>are</em> super-human, like AlphaZero, tend to be pretty narrow in applicability.) And yes, our learning is comparatively sample-efficient. (For every book that a human child has read, GPT-4 has read 10,000.) We can occasionally generalise out of distribution in ingenious ways.</p>
<p>But. We struggle to transfer knowledge outside the learned context. We make mistakes even when we know better or in principle. We struggle to learn new concepts, languages, skills, habits - we struggle to change our ways, even when we want to. We behave sub-optimally, both wittingly and unwittingly, even when it's pointed out. In short, we struggle to learn how to do some things, and then we struggle to apply them.</p>
<p>So, our intelligence is more general than the best AIs in 2024, by a good distance. But 'general intelligence' is a continuum rather than binary, and our intelligence is only somewhat general.</p>
<p>In practice, the only clean definition in AI is for superintelligence - "better than the best of us at everything". And I think that's probably still a way off.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/the-real-reason-we-cant-define-artificial-general-intelligence/</guid>
      <pubDate>Tue, 01 Oct 2024 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Advice to a younger self</title>
      <link>https://www.gregdetre.com/blog/advice-to-a-younger-self/</link>
      <description>**Honestly doing the exercise is far more interesting than reading other people's answers.** What would you tell your younger self about procrastination, balance, and learning?</description>
      <content:encoded><![CDATA[<h3 id="think-back-to-what-you-were-doing-5-10-15-years-ago-really-visualise-it-what-advice-would-you-give-your-younger-self-is-it-consistent-honestly-doing-the-exercise-is-far-more-interesting-than-reading-other-peoples-answers">"Think back to what you were doing 5, 10, 15+ years ago. Really visualise it. What advice would you give your younger self? Is it consistent? Honestly doing the exercise is far more interesting than reading other people’s answers."</h3>
<p><em>(<a href="https://twitter.com/naval/status/914189738453622784">from Naval Ravikant</a>)</em></p>
<ul>
<li>
<p>Drink more water when you go out boozing.</p>
</li>
<li>
<p>Procrastination is an unwillingness to sit with your own discomfort, and willpower is a mood/energy state.</p>
</li>
<li>
<p>Your environment and home situation both affect how well you recharge. Invest in them.</p>
</li>
<li>
<p>Improve your posture and tense your core to stay awake when sleepy in a lecture.</p>
</li>
<li>
<p>You feel way better and more energised when you exercise more and feel part of a community.</p>
</li>
<li>
<p>Learn as much maths and technical stuff as you can when you’re young. Don’t just read textbooks and audit the classes - you only learn if you do the exercises.</p>
</li>
<li>
<p>You are not invincible. You need balance in your life to be able to continue to work well and stay happy.</p>
</li>
<li>
<p>Switch the channel on your inner monologue when it’s not serving you.</p>
</li>
<li>
<p>Regular meditation gives you just a little more chance to notice your inner monologue, and to choose what you say and how you respond in the moment.</p>
</li>
<li>
<p>It’s better to be happy and effective than right. You'll never change someone's mind if they're feeling angry or threatened.</p>
</li>
<li>
<p>Practice saying the hard things earlier.</p>
</li>
</ul>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/advice-to-a-younger-self/</guid>
      <pubDate>Sun, 06 Oct 2024 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>What is wisdom?</title>
      <link>https://www.gregdetre.com/blog/what-is-wisdom/</link>
      <description>Wisdom is often about **truths whose opposite are truths** - balancing seemingly contradictory principles like self-love and self-honesty, accountability and forgiveness.</description>
      <content:encoded><![CDATA[<p>Wisdom is often about truths whose opposite are truths.</p>
<p>Wisdom is about learning from one's mistakes - without beating oneself up.</p>
<p>Wisdom is about loving yourself, forgiving yourself, accepting yourself - but being honest with yourself, listening to criticism, and looking to grow.</p>
<p>Wisdom is about expecting and holding other people accountable - but also making them feel safe and forgiving them and giving them a second chance.</p>
<p>Wisdom is about preserving our traditions and institutions - without being hide-bound and closed.</p>
<p>Wisdom is about taking advantage of technology - without letting it control you.</p>
<p>Wisdom is about appreciating what we have - without needing it to be happy. About <a href="https://www.linkedin.com/feed/update/urn:li:activity:7229592625215893504/">wanting the gold medal - without being defined by it</a>.</p>
<p>Wisdom is about taking care of those around you and yourself - but also being carefree.</p>
<p>Wisdom is about knowing and respecting one's own needs - but also the needs of others.</p>
<p>Wisdom is about being considerate of others' needs - without endlessly self-monitoring.</p>
<p>Wisdom is about being a good partner - without becoming codependent.</p>
<p>Wisdom is about knowing how to balance taking care of the person in front of you - with a million lives on the other side of the globe.</p>
<p>Wisdom is about loving yourself, and being enough - but also being dependable and loving and part of a community.</p>
<p>Wisdom is about excitement, receptiveness, and spontaneity - but also being dependable and thinking about consequences.</p>
<p>Wisdom is about zooming out, but still staying at the level of the human. Because what matters most in a human life is hard to see in atoms or galaxies.</p>
<p>Wisdom is about treating this life as precious - but also being ready to die without rancour at any moment.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/what-is-wisdom/</guid>
      <pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Pronouns &amp; plosive sounds - can you change society with a single word?</title>
      <link>https://www.gregdetre.com/blog/pronouns-plosive-sounds-can-you-change-society-with-a-single-word/</link>
      <description>Could adopting a single pronoun for everyone subtly reshape how we think about gender? Language creates perceptual boundaries in our minds—and changing those boundaries might change society.</description>
      <content:encoded><![CDATA[<h1 id="argument">Argument</h1>
<ul>
<li>
<p>Our brains can learn to divide a perceptual continuum at an arbitrary point, so that the two sides seem like distinct categories.</p>
</li>
<li>
<p>Our language's grammar rules have a subtle influence on the way we think.</p>
</li>
<li>
<p>Most languages distinguish between male and female in various ways (e.g. he/she, prince vs princess, blond vs blonde, and perhaps even the gender of nouns) - these grammatical distinctions will subtly reinforce gender distinctions in our minds.</p>
</li>
<li>
<p>Given this, we can make some predictions:</p>
<ul>
<li>
<ol>
<li>When a language emphasises the distinction between genders, we will see a larger gender gap in society.</li>
</ol>
</li>
<li>
<ol start="2">
<li>If we de-emphasised the distinction between male and female in our language (e.g. by adopting a single pronoun for everyone as the default), we would notice the distinction slightly less in general, and find it easier to think of gender as non-binary.</li>
</ol>
</li>
</ul>
</li>
</ul>
<h1 id="our-brains-can-learn-to-divide-a-perceptual-continuum-at-an-arbitrary-point-so-that-the-two-sides-seem-like-distinct-categories">Our brains can learn to divide a perceptual continuum at an arbitrary point, so that the two sides seem like distinct categories.</h1>
<p>What if I told you there's a relationship between backpacks and binary gender pronouns? But not in an obvious way.</p>
<p>The words 'back' and 'pack' sound pretty distinct to an English speaker. Yet those plosive 'b' and 'p' sounds are formed in a similar way - the main difference is the 'voice onset time', that is the interval between release of breath and vocal cord movement.</p>
<p>If the VOT is less than 30ms, you hear a 'b', and above that a 'p'. Our brains have placed a fence at an arbitrary point on that continuum, parting it like the Red Sea so that the two sides seem like separate categories. This helps us quickly distinguish English words.</p>
<p>But other languages don't always distinguish these sounds in the same way, so babies have to learn this distinction differently depending on where they grow up. And the downside of learning to amplify the difference <em>between</em> these VOT perceptual categories is that we lose the ability to hear small differences <em>within</em> a category - in other words, while our brains amplify the difference between 29 vs 31ms, we throw away information about 31 vs 33ms.</p>
<p>What does this have to do with gender?</p>
<h1 id="our-languages-grammar-rules-have-a-subtle-influence-on-the-way-we-think">Our language's grammar rules have a subtle influence on the way we think.</h1>
<p>Imagine you’re asked to describe a bridge. Is it slender and elegant, or sturdy and strong? Your answer might subtly depend on the language you speak.</p>
<p><a href="https://www.npr.org/sections/krulwich/2009/04/06/102518565/shakespeare-had-roses-all-wrong#:~:text=What%20explains%20the%20difference%3F,characteristics%20of%20their%20grammatical%20gender.">Lera Boroditsky's research showed that people describe objects differently based on the gender of the noun</a> - for example, the word for 'bridge' is feminine in German (<em>die Brücke</em>), but masculine in Spanish (<em>el puente</em>). German speakers are more likely to describe bridges as "beautiful" or "fragile," while Spanish speakers opt for descriptors like "strong" or "long." It's not that bridges inherently possess these traits, but rather that the gendered structure of the language shapes how people think about them.</p>
<p>Or consider the Guugu Yimithirr people in Australia - the <a href="https://www.perplexity.ai/search/tribe-that-uses-cardinal-direc-tGCilRCaRWWFtzfN.UeMvQ">cardinal directions (north, south, etc) are a ore part of their language</a>, and as a result speakers develop an "internal compass" from a very young age.</p>
<p>Language shapes thought in subtle ways.</p>
<p>Even in English, where nouns don't have genders, we still need to track the gender of every person in a conversation so that we can use the correct pronoun (<em>he</em> vs <em>she</em>). And indeed, we also distinguish between male and female in many nouns (prince vs princess), and even occasionally in adjectives (blond vs blonde).</p>
<p>In a sense, just like we draw perceptual categories for sound, our language draws boundaries between concepts like gender. And just as these distinctions help us navigate the world quickly, they also limit how fluidly we can perceive it.</p>
<h1 id="given-this-we-can-make-some-predictions">Given this, we can make some predictions:</h1>
<h2 id="1-when-a-language-emphasises-the-distinction-between-genders-we-will-see-a-larger-gender-gap-in-society">1. When a language emphasises the distinction between genders, we will see a larger gender gap in society.</h2>
<p>If your language commonly distinguishes between male and female, that will subtly reinforce gender distinctions in your mind.</p>
<p>We might predict that this will manifest at a larger scale in various ways. Concretely:</p>
<ul>
<li>
<p>We could quantify the centrality of gender distinctions in a language, e.g. separate pronouns, gendered nouns, gender morphology for nouns/adjectives/determinants, etc.</p>
</li>
<li>
<p>We could quantify the gender gap in society, e.g. gender pay gap, unconscious bias, gender balance in professions, maternity vs paternity leave policies, etc.</p>
</li>
<li>
<p>We predict a correlation between centrality of gender distinctions in a country's language and the breadth of gender gap in its society.</p>
</li>
</ul>
<p>There are apparently a few languages that don't have gendered pronouns (e.g. Finnish, Bengali, Persian), so there may be enough variability to test the hypothesis.</p>
<h2 id="2-if-we-de-emphasise-the-distinction-between-male-and-female-in-our-language-eg-by-adopting-a-single-pronoun-for-everyone-as-the-default-we-would-notice-the-distinction-slightly-less-in-general-and-find-it-easier-to-think-of-gender-as-non-binary">2. If we de-emphasise the distinction between male and female in our language (e.g. by adopting a single pronoun for <em>everyone</em> as the default), we would notice the distinction slightly less in general, and find it easier to think of gender as non-binary.</h2>
<p>To my knowledge, no one has attempted this experiment at a large scale. <a href="https://www.babbel.com/en/magazine/swedish-hen">Swedish has adopted <em>hen</em> as a gender-neutral pronoun for non-binary people</a>, but that's not the same thing as using the same pronoun for everyone.</p>
<p>Even if the hypothesis were true, we'd expect this to be a fairly small effect, and to take many years to be visible.</p>
<p>Won't this confuse everyone? Perhaps slightly - it might be slightly harder to tell whether a pronoun is referring to this person or that one. If we were to use <em>they</em> as a gender-neutral replacement for <em>he/she</em>, we would also lose information about singular vs plural. But in practice, we manage just fine with the gender- and plurality-neutral 2nd-person <em>you</em> in English. Context is usually enough.</p>
<h1 id="what-is-my-takeaway">What is my takeaway?</h1>
<p>The long arc of history and morality seem to lean towards self-determination, as long as others aren't harmed. After all, this is just a special case of the Golden Rule - <em>do as you would be done by</em>. The Golden Rule is rarely the final word, but it's often a good place to start. So as a policy, I try not to tell people what they should want or be or say.</p>
<p>I want to live in a more equitable society, that doesn't discriminate based on gender (for starters). There's evidence that arbitrary perceptual boundaries become more separated in our minds. And there's evidence that the rules of language shape our thought subtly. <strong>So perhaps we should change our language so that we don't have to practice noticing male vs female in every sentence. The smallest, most impactful step would be to adopt a single pronoun for everyone (e.g. <em>they</em>).</strong></p>
<p>It tickles me to think that such a small effect could have a noticeable macroscopic effect in time. This is a minor, seditious, evidence-based act of progressiveness that we can all take unilaterally, even if it feels a little awkward at times. So this is why I am attempting to use <em>they/their</em> in place of <em>he/she</em> and <em>his/her</em> wherever I can.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/pronouns-plosive-sounds-can-you-change-society-with-a-single-word/</guid>
      <pubDate>Sat, 19 Oct 2024 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>What will AI do to jobs?</title>
      <link>https://www.gregdetre.com/blog/what-will-ai-do-to-jobs/</link>
      <description>Even if AI-assisted programmers can do the work of ten, we'll build orders of magnitude more software—but technological unemployment will outpace this Jevons Paradox by the 2030s.</description>
      <content:encoded><![CDATA[<p>There is already a huge gap between what current models are capable of, and how they’re being currently used. Even if AI doesn't improve at all from the current state of the art: AI will automate lots of jobs away; AI will also increase productivity and create new jobs.</p>
<p><strong>The good news</strong>. Take programming, for example: if one AI-assisted programmer can do the work of 10 mere StackOverflow-assisted programmers, then we will build much more software. This is the <a href="https://www.greenchoices.org/news/blog-posts/the-jevons-paradox-when-efficiency-leads-to-increased-consumption">Jevons Paradox.</a> Programming teams may shrink, but more will proliferate. And we’ll also need more prompt engineers and product managers. So, perhaps fewer programmers per team, but more teams, and work created for other roles. We might imagine that the same will be true for many of the other fields where AI-assistance will turbocharge productivity. There will be orders of magnitude more writing, more medicine, more data insights, more architecture, more therapy, and indeed more pop songs, more personalised advertising messages, more lawsuits.</p>
<p><strong>The bad news</strong>. The wave of AI-automation is going to be larger and faster than any technological unemployment we’ve seen before. It won’t be obvious in the next year or two, but it will be stark by the 2030s. Even if new jobs are being created, there will be a lag, and the people who lost their jobs may not be qualified for the new jobs. This is terrifying. People will move towards jobs in lingering niches where AI can’t yet compete, perhaps involving physical interactions (like hairdressing and plumbing), and rich human interactions (like therapy and care work).</p>
<p>Will there be a limit to how far AI improves? In the long run, no. And even in the short run, AI will exceed many humans for many tasks, even if AI doesn't exceed all humans for all tasks.</p>
<p>So there will be more automation, but also more demand, and the equilibrium between the two will fluctuate. But the trend will be for technological unemployment to outpace Jevons Paradox.</p>
<p>What hope is left? The optimistic, post-scarcity vision is that we fund Universal Basic Income through taxation. But that feels like a global optimum, with many deep local pessimums in between - in other words, it's hard to see how we get there from here, without a few rapacious winners taking all.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/what-will-ai-do-to-jobs/</guid>
      <pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Metaphors for depression, clean language, and the DSM</title>
      <link>https://www.gregdetre.com/blog/metaphors-for-depression-clean-language-and-the-dsm/</link>
      <description>Could we diagnose depression by the metaphors people use to describe it? *Black veil*, *gravity*, *suffocation* - each might reveal different underlying patterns requiring different treatments.</description>
      <content:encoded><![CDATA[<div class="toc">
<ul>
<li><a href="#postscript-a-testable-hypothesis-treatment-idea">Postscript: a testable hypothesis &amp; treatment idea</a></li>
</ul>
</div>
<p>I've used a lot of metaphors for depression to myself, to help me understand what I'm feeling. And to remember later how it was - perhaps to exorcise it, to capture it, and to study it. And because I often want to write when I'm depressed, and the depression crowds out almost everything else.</p>
<p>It's a black veil, making it hard to take in a deep breath of clean air, masking all the colour, making it hard to even imagine colour.</p>
<p>It's a friction, a downwards. I feel like an ungainly bird trying to lift off, flailing and thrashing hard to heave off against the heaviness. I look at the others, aloft, buoyed by thermals, only needing a lazy flap here or there. They can't imagine how much more work it is to lift off than stay up.</p>
<p>When I'm still functioning in the face of the depression, I feel like I'm flying inches above the treetops. It takes only a slight drop in the wind and then I'm being snagged by a stray branch, stumbling and wheeling awkwardly, feeling the darkness of the forest yawning and gaping underneath me.</p>
<p>The physics of it makes me feel like a rocket yearning for escape velocity. On the ground the gravity is strongest, and from a standing start it's going to take a truly concerted burst of effort to make any headway - even a gigantic plume of flame barely seems enough to shrug off the inertia. But eventually, gradually, there is motion. But is it enough? Are you going to run out of go before you run out of slow? It feels like there's so so much more stop to start with. And every time you stall, defeat surrounds and clothes and clings to you, like raindrops in a drizzle.</p>
<p>But there are moments during the ascent that feel like suddenly bursting through the clouds, when thrust exceeds gravity and it feels like the acceleration could be endless. Smiles and good fortune are in arm's reach in every direction, like reassuring rounded pebbles on a British beach. Treasure and relive those moments, and they will help sustain you through the difficult times.</p>
<p>I suppose the message from the physics of a rocket is to keep on afterburnering upwards as fast as you can, that a small constant effort is never going to be enough to achieve escape velocity. This is why the NHS NICE guidelines suggest that you combine both intensive CBT <em>and</em> antidepressants for more serious or long-lasting depression. You need as much acceleration as soon as possible. And my hunch is that the different components have different timecourses, e.g. exercise gives you a quick boost, medication takes a few weeks to really kick in, and the cognitive interventions probably have the longest-term effects.</p>
<p>Suffocation and colour and friction and gravity. Intensity and activation and expansion.</p>
<h2 id="postscript-a-testable-hypothesis-treatment-idea">Postscript: a testable hypothesis &amp; treatment idea</h2>
<p>One further thought.</p>
<p>Of course, there are other metaphors that people have used. Could there be something to the different kinds of metaphors that people use?</p>
<p>After all, we try to characterise individual symptoms as precisely as we can, and to pay attention to how they cluster, as our means of individuating and categorising diseases. Could metaphor be a way to characterise the phenomenological symptoms, as a clue to what's underlying them?</p>
<p>There's evidence to suggest that depression isn't a unitary condition. This might explain why different people respond differently to medication.</p>
<p>Could we use something like <a href="https://en.wikipedia.org/wiki/Clean_Language">clean language</a> to gather people's metaphors for their depression, characterise the kinds of metaphors used, and then attempt a systematisation and diagnosis of depression by metaphor?</p>
<p>Or better still, try and map the system of metaphors to the patterns we see from our more traditional evidence-gathering, with brain imaging, response to medication, in symptoms, etc? <em>"Ah, heavy-veil, with family history, but no mania - try Sertraline until your sleep symptoms improve."</em> Oh, you're getting <em>"Black-dog with occasional mania and increased schmorbito-florbital activity - stop ruminating, and eat fewer hamburgers."</em>.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/metaphors-for-depression-clean-language-and-the-dsm/</guid>
      <pubDate>Thu, 07 Aug 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>We should de-stigmatise suicide, our innermost freedom and right</title>
      <link>https://www.gregdetre.com/blog/we-should-de-stigmatise-suicide-our-innermost-freedom-and-right/</link>
      <description>My greatest fear has never been dying—it's not being able to die if I want to.</description>
      <content:encoded><![CDATA[<p>My greatest fear has never been dying. It is and remains not being able to die if I want to.</p>
<p>I fear torture. I fear solitary confinement. I fear dementia. I fear the slow and painful breakdown of my body. I fear dependence and becoming a burden. I fear debilitating depression.</p>
<p>What is there to fear about death, nothingness itself? Maybe an end to joys, a sense of opportunity costs and failed potential, the pain our loved ones might feel at losing us... but these are the preoccupations of the living, not the non-existent.</p>
<p>As a society, we should support people's right to die at a time and in a manner of their choosing. We should try and help them make the decision - we should, of course, encourage them not to die, especially if there is hope for a good life, and provide assistance in building back up towards that. We should create checks and balances to avoid people being manipulated or coerced into choosing to die. We should provide support, both psychological and financial for those they leave behind. We should, in short, legalise suicide.</p>
<p>A leap further, we should <em>de-stigmatise</em> suicide. This is not to say that people should be told to die any more than people should be told to change their gender - only that they should be able to, be educated and supported in deciding to, and not stigmatised if they do.</p>
<p>The freedom to die is our innermost freedom.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/we-should-de-stigmatise-suicide-our-innermost-freedom-and-right/</guid>
      <pubDate>Fri, 08 Aug 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>What new developments will we need for personalised, relevant one-to-one AI tutoring?</title>
      <link>https://www.gregdetre.com/blog/what-new-developments-will-we-need-for-personalised-relevant-one-to-one-ai-tutoring/</link>
      <description>Future AI tutoring may require more than prompt engineering—perhaps **custom teacher models** that learn alongside students, plus **explicit student models** to predict and optimize learning outcomes.</description>
      <content:encoded><![CDATA[<div class="toc">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#developing-a-teacher-model-thats-personalised-and-relevant">Developing a teacher model that's personalised and relevant</a></li>
<li><a href="#developing-an-explicit-model-of-the-student">Developing an explicit model of the student</a></li>
</ul>
</div>
<h2 id="introduction">Introduction</h2>
<p>In <a href="https://en.wikipedia.org/wiki/Bloom%27s_2_sigma_problem">"The 2 Sigma Problem"</a>, Bloom suggested that one-to-one tutoring could provide a phenomenal boost to students' learning. Maybe it's <a href="https://www.educationnext.org/two-sigma-tutoring-separating-science-fiction-from-science-fact/">not <em>quite</em> as big an effect as he suggested</a>, but I'm still willing to bet that the combination of personalisation, relevance, engagement, adaptive difficulty, and other benefits of one-to-one tutoring do make a huge difference, and at least some of them can be captured by generative AI. But even if we thought we did have a better approach, how could we be sure?</p>
<p>I'll try and tackle both sets of questions.</p>
<h2 id="developing-a-teacher-model-thats-personalised-and-relevant">Developing a teacher model that's personalised and relevant</h2>
<p>Let's focus on the question of how we might make a teacher model that provides very personalised, relevant tutoring to the student.</p>
<p>Relevance is rich and hierarchical: the environment, the task or context that the student is learning about right now, this particular course, their previous learning and progress, the history of interactions with the teacher, the language, the country they live in, recent news, macro changes, etc. All of these contextualise our learning.</p>
<p>This kind of rich, hierarchical, many-faceted personalisation presents challenges for a single gigantic, fixed model, such as an LLM.</p>
<p><em>Maybe</em> we can get away with just prompt engineering, feeding in an enormous dump of data about learner goals, preferences, and previous interactions, and rely on ever-larger context windows and more instructable models.</p>
<p>But I’m pessimistic about whether prompt engineering alone is enough for fine-grained, subtle, creative, optimal relevance and personalisation.</p>
<p>Firstly, there's the challenge of fitting all the necessary background into the model's context window <a href="https://www.dbreunig.com/2025/06/22/how-contexts-fail-and-how-to-fix-them.html">without overwhelming or confusing it</a>.</p>
<p>Secondly, LLMs struggle with prompts that tug against their pretraining. For example, at <a href="https://rehearsable.ai/">Rehearsable.ai</a>, it proved intractable to prompt-engineer GPT-4 to reliably and subtly follow a particular negotiation skills approach that flew in the face of the common advice found on the internet.</p>
<p>In the short-run, perhaps we can imagine hierarchies of LORA-style fine-tuning that can be layered on top of one another, for student, exam board, culture, etc. But my bet is that eventually the best AI teachers will learn too, alongside and about their students.</p>
<h2 id="developing-an-explicit-model-of-the-student">Developing an explicit model of the student</h2>
<p>Perhaps we also need to go beyond a single, main teacher model. Could it help to represent the learner with an explicit (separate?) model that assists the primary teacher model? To train such a model of the learner, we could train it to predict the learner’s behaviour, e.g. the answers they are giving, and the questions they are asking. We might then probe this model of the learner, to ask “How would the learner respond if we asked them this question?”, or to look at how it has changed over time to measure “Has the learner’s understanding of Topic X improved?”.</p>
<p>With such a model of the learner’s behaviour, we could run <a href="https://medium.com/@_michelangelo_/monte-carlo-tree-search-mcts-algorithm-for-dummies-74b2bae53bfa">Monte-Carlo Tree Search</a> or similar to simulate the effect of different teacher interventions, and pick the one that we believe will best help improve the learner’s eventual performance. In this way, we can consider relevance as exactly the content that <em>this</em> learner needs right now, in order to pass their particular exam, or indeed to unstick their current confusion. A rich model of the learner could help with choosing or generating particular problems that will help them see how to apply a new concept, develop the skill they’re missing, or correct a misconception. It could involve judicious examples, or analogies, or counter-examples.</p>
<p>So, future AI one-to-one tutoring might involve both custom teacher <em>and</em> custom student models.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/what-new-developments-will-we-need-for-personalised-relevant-one-to-one-ai-tutoring/</guid>
      <pubDate>Sat, 09 Aug 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>How can we measure the effectiveness of an AI tutor?</title>
      <link>https://www.gregdetre.com/blog/how-can-we-measure-the-effectiveness-of-an-ai-tutor/</link>
      <description>Perhaps our future grades will depend not on *our* exam performance, but on how well our **digital avatars** handle a battery of simulated tests.</description>
      <content:encoded><![CDATA[<div class="toc">
<ul>
<li><a href="#measuring-human-learner-performance">Measuring human-learner performance</a></li>
<li><a href="#measuring-teacher-performance">Measuring teacher performance</a></li>
<li><a href="#ai-offers-new-more-accurate-more-humane-ways-to-evaluate">AI offers new, more accurate, more humane ways to evaluate</a></li>
</ul>
</div>
<h2 id="measuring-human-learner-performance">Measuring human-learner performance</h2>
<p>We can of course try to directly measure the learner’s performance, e.g. with explicit tests and exams - but these are effortful for the learner, inefficient, and don’t capture everything that matters.</p>
<p>We can measure the learner’s engagement and enjoyment, either explicitly (with surveys) or implicitly (by their continued interest). This makes some sense, in that a learner won't learn much if they give up. But on the flip side, it's possible to be engaged in something that isn't efficacious. So, engagement is <em>necessary</em> but not <em>sufficient</em> for learning.</p>
<p>We might even try and quantify aspects of the learner’s behaviour with automated LLM evals, e.g. whether the learner is exhibiting curiosity, or asking good questions.</p>
<p>Of course, even if we could measure all these dimensions at a given moment, it’s not easy to tell how much to attribute changes over time to the teacher. We have expensive gold-standard measurements (e.g. longitudinal between-subject AB tests/RCTs), but we can't run these very often. So in practice, we have to rely mostly on cheap &amp; immediate automated evaluations, after validating them occasionally against the gold standard - kinda like applying the <a href="https://en.wikipedia.org/wiki/Cosmic_distance_ladder">cosmic distance ladder</a> approach to product development, where we try and find a chain of proximal measures that ladder up to our expensive, gold-standard distal measure.</p>
<h2 id="measuring-teacher-performance">Measuring teacher performance</h2>
<p>We can also try and measure the teacher’s behaviour, as in the LearnLM paper, where the model is tuned towards important dimensions, like “be encouraging”, “don’t give away the answer prematurely”, and “keep the learner on track”.</p>
<p>Beyond these, there are many lower-level pedagogical best practices we might consider, e.g. encouraging an <a href="https://web.archive.org/web/20070317194011/http://www.stanfordalumni.org/news/magazine/2007/marapr/features/dweck.html">incremental mindset</a>, making the content <a href="https://www.memrise.com/blog/how-to-use-mnemonics-to-remember-new-vocabulary">memorable</a>, applying a <a href="https://www.notion.so/230422-Newspeak-House-Hackathon-with-Peppe-Cesar-Greg-2c73fa13af404c70b03263bf56124810?pvs=21">spiral curriculum</a>, use of analogies and examples, etc.</p>
<p>And of course there are various product metrics, e.g. latency, ease of use.</p>
<h2 id="ai-offers-new-more-accurate-more-humane-ways-to-evaluate">AI offers new, more accurate, more humane ways to evaluate</h2>
<p>We can also consider more speculative approaches that are too labour-intensive for human teachers:</p>
<ul>
<li>
<p>A deep dialogue with a teacher (almost like a low-stakes, ongoing PhD viva) provides a very rich measure of a learner’s handle on the material - the teacher can constantly probe, ask questions at the boundaries of the learner’s knowledge, and ask them to apply what they have learned in unexpected ways. Asking questions at the margins of the learner’s knowledge like this will maximise the informational payoff of each question to the teacher. It’s too expensive to have a human teacher discussing with the learner all the time, but this may be one of the ways that an AI teacher could develop a very rich sense of a learner’s ability over time.</p>
</li>
<li>
<p><em>"If you can't explain it simply, you don't understand it well enough”</em> (Feynman). We can discover a great deal about the learner’s understanding by asking them to teach someone else (either a human or AI peer), and noticing where in the material they succeed and where they struggle as a teacher. And as a nice side-effect, both these activities will also be very effective for helping the learner to learn.</p>
</li>
<li>
<p>If we have built a rich, explicit model of the learner, as above, then we could use the simulated learner’s performance as a proxy measure for the real learner’s ability - perhaps in the future, our grade will be based on how well our avatars do on a multi-day battery of simulated exams!</p>
</li>
</ul>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/how-can-we-measure-the-effectiveness-of-an-ai-tutor/</guid>
      <pubDate>Sun, 10 Aug 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Psychological effects and cognitive illusions in AI-assisted programming</title>
      <link>https://www.gregdetre.com/blog/psychological-illusions-in-ai-assisted-programming/</link>
      <description>We judge AI's coding mistakes as **permanent character flaws** whilst excusing our own as situational—and find a precarious flow. What can psychology experiments tell us about how it _feels_ to program with AI assistance?</description>
      <content:encoded><![CDATA[<p>There's something odd and new about the subjective experience of AI-assisted programming. It distorts our sense of time passing, difficulty, effectiveness, and joy.</p>
<p>Some of these are well-known psychological effects and illusions, and the psychological theories behind them can help us understand a bit better what's going on.</p>
<div class="toc">
<ul>
<li><a href="#its-hard-to-judge-how-much-work-the-ai-did-for-us">It's hard to judge how much work the AI did for us</a></li>
<li><a href="#context-change-and-feeling-of-duration">Context change, and feeling of duration</a></li>
<li><a href="#being-knocked-out-of-flow">Being knocked out of flow</a></li>
<li><a href="#weakened-reinforcement">Weakened reinforcement</a></li>
<li><a href="#its-a-different-kind-of-work">It's a different kind of work</a></li>
<li><a href="#a-new-kind-of-skill">A new kind of skill</a></li>
<li><a href="#where-does-this-leave-us">Where does this leave us?</a><ul>
<li><a href="#discussion-on-linkedin">Discussion on LinkedIn</a></li>
</ul>
</li>
</ul>
</div>
<h2 id="its-hard-to-judge-how-much-work-the-ai-did-for-us">It's hard to judge how much work the AI did for us</h2>
<p>When the AI takes an hour to do something, with all the attendant dead-ends and back-and-forth, it's hard to know how effective it has been.</p>
<p>In general, it's <a href="https://newsletter.pragmaticengineer.com/p/measuring-developer-productivity">very difficult to objectively measure programming productivity</a> , and <a href="https://www.makingdatamistakes.com/why-were-bad-at-estimating-and-what-to-do-about-it/">we are bad at estimating how long it would have taken us to do</a>.</p>
<p>When we watch the AI try and fail and iterate, we notice every failure and false start along the way. <em>"Gah, you've broken the unit tests again, you foolish robot!"</em> But when coding manually ourselves, we might make the same mistakes but experience them differently. Maybe this is a case of the <a href="https://en.wikipedia.org/wiki/Fundamental_attribution_error">fundamental attribution error</a>, that we see our own actions as contextualised by the situation, whereas we're more likely to attribute permanent 'dispositions' to others based on their actions. Or a little darker, that we use a <a href="https://pubmed.ncbi.nlm.nih.gov/2266485/">victim vs perpetrator narrative</a>: we use a 'perpetrator' narrative for our own mistakes (meaningful and comprehensible, with the incident as a closed, isolated event with no lasting implications) and a 'victim' narrative for the AI's mistakes (seeing the actions as arbitrary and incomprehensible, and portraying the incident in a long-term context with continuing harm and lasting grievances).</p>
<p>We <a href="https://claude.ai/chat/1fefa77f-b834-4f10-abad-0ed3e76c86c9">aren't always good judges of how hard things are, what works for us and what doesn't, or reliable in comparing ourselves to others</a>. (For example, there's a lovely <a href="https://www.pnas.org/doi/10.1073/pnas.1821936116">psychology experiment where students feel they've learned less from active recall than passive review, even though active recall is actually more effective</a>.) And things always <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC1743746/">seem simpler after we have understood them</a>, so once we see the AI's solution, it seems obvious.</p>
<p>So for all these reasons, expect to see a lot of people rationalising away even a 10x speedup (<a href="https://www.makingdatamistakes.com/ai-first-development/">1000+ lines of code in a day</a>, rather than <a href="https://www.perplexity.ai/search/how-many-lines-of-code-does-a-JStnps.URG.lO.1AClkOOw">&lt;100</a>). After all, <em>"<a href="https://www.goodreads.com/quotes/21810-it-is-difficult-to-get-a-man-to-understand-something">it is difficult to get a man to understand something, when his salary depends on his not understanding it</a>”</em> :~</p>
<h1 id="context-change-and-feeling-of-duration">Context change, and feeling of duration</h1>
<p>In an hour of AI-assisted programming, especially with multiple AI agents working in parallel, we might experience multiple distinct events, e.g. discussing ideas, exploring multiple approaches; hitting a dead end; reverting and try again; shipping a couple of small features. Each of these feels like a significant, distinct event.</p>
<p>Our <a href="https://claude.ai/chat/dafd3c21-06bd-4271-8f06-9c4e2efe64b7">sense of time is heavily influenced by the number of distinct events or changes we experience</a>. As a result, an hour spent like this will <em>feel</em> like it lasts longer than if we'd just been working on a single, undifferentiated problem. Even though you've accomplished more than you might have in a day of traditional coding.</p>
<h1 id="being-knocked-out-of-flow">Being knocked out of flow</h1>
<p>AI-assisted programming knocks us out of the <a href="https://en.wikipedia.org/wiki/Flow_(psychology)">"flow state"</a> - that magical zone of manageable-but-still-challenging where you lose track of time.</p>
<p>Classic programming provides an almost instant feedback loop: write code, compile, see results, tweak, repeat. Each micro-adjustment feeds into that cycle.</p>
<p>AI-assisted development breaks this pattern. Even a 10-second delay between your instruction and the AI's response can bump you out of flow. Watching the AI iterate feels like watching paint dry - even if it's actually drying in just a few moments.</p>
<p>Eventually, the models will just output so fast that this problem will go away. In the meantime, it helps to run multiple agents in parallel, and switch each time the AI delays (see "<a href="https://www.makingdatamistakes.com/ai-first-development/">Optimise for correctness"</a>). Of course, this creates switch costs that need to be managed...</p>
<h1 id="weakened-reinforcement">Weakened reinforcement</h1>
<p><a href="https://umbrex.com/resources/tools-for-thinking/what-is-variable-rewards/">'Variable reinforcement'</a> is when you get some kind of reward or positive result <em>some</em> of the time. Like slot machines... or video games, or capricious bosses/partners, or email. This unpredictable signal creates <a href="https://blog.nateliason.com/p/rats-levers-parks">very robust addictive behaviours</a>.</p>
<p>Programming has something of this character. Maybe <em>this time</em> when I hit 'compile', it'll work!</p>
<p>But it's <a href="https://claude.ai/chat/4e99dc52-82dd-4ab8-a611-3bcc2d3a386e">less reinforcing to watch someone someone else get the reward</a> - so when the AI's the one pulling the lever and sometimes getting the cocaine, we miss out on the rush.</p>
<h1 id="its-a-different-kind-of-work">It's a different kind of work</h1>
<p>While AI reduces the mechanical effort of coding, it increases the cognitive demand of communication. You're constantly making implicit knowledge explicit:</p>
<ul>
<li>
<p>Explaining background context</p>
</li>
<li>
<p>Defining success criteria</p>
</li>
<li>
<p>Articulating architectural principles</p>
</li>
<li>
<p>Catching unstated assumptions</p>
</li>
</ul>
<p>It's like the difference between driving a car (flow state, muscle memory) and teaching someone to drive (constant metacognition, explicit instruction).</p>
<p>When AI is writing most of the code, you spend most of your time reading what it wrote, often being spread across various parts of the codebase, and trying to figure out how a series of scattered edits fit into a larger context. And reading code is much more effortful than writing it. I'm not 100% sure why. But it partly explains why it's so much more fun to build from a greenfield than to modify an existing system, and contributes to the illusion that it would be better to start from scratch and <a href="https://www.joelonsoftware.com/2000/04/06/things-you-should-never-do-part-i/">rewrite the whole damn thing</a>!</p>
<p>Anybody know of any studies that would explain this?</p>
<h1 id="a-new-kind-of-skill">A new kind of skill</h1>
<p>AI-assisted programming is a skill. It might even be a meta-skill, a skill about skills, like teaching or learning to learn. It is multiplied by your own mastery at the task, and your own clarity of thinking, and your own metacognition.</p>
<p>We can expect various non-monotonicities, e.g.</p>
<ul>
<li><strong>Seniority</strong>: Junior programmers seem most positive about AI, because it levels the playing field for them. Senior programmers seem pretty positive, because their experience &amp; judgment enables them to make effective architecture decisions, without having to worry about semicolons), while mid-level programmers are most negative, because it threatens the craft and low-level skill on which their pride and economic value sit. see (<a href="https://www.wired.com/story/how-software-engineers-coders-actually-use-ai/">"Does experience affect attitude?"</a>)</li>
</ul>
<p><img alt="Does experience affect attitude" src="/blog/images/does_experience_affect_attitude.png"/></p>
<ul>
<li>
<p><strong>Expertise</strong>: This is part of what I'd guess explains the apparent <a href="https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/"><em>drop</em> in productivity when using AI tools in a recent (mid-2025) study</a>. Expert AI-assisted programming will involve <a href="https://www.makingdatamistakes.com/ai-first-development/">un-learning a lot of existing behaviours &amp; skills, and replacing them with new ones</a>.</p>
</li>
<li>
<p><strong>Verbal</strong>: We may find that the best AI-assisted programmers have different skillsets from the best human-only programmers. We may find that teachers and managers and other expert communicators are at a huge advantage.</p>
</li>
</ul>
<h1 id="where-does-this-leave-us">Where does this leave us?</h1>
<p>Even despite all this, I have started to experience a new kind of flow working with AI too. When the context is well-defined, and the guardrails are protective, then the speed at which dreams transform into working code can feel like magic. The impedance between imagination and implementation drops dramatically.</p>
<p>As AI models get faster and we develop better workflows, we might find a sweet spot that combines this new superpower with the satisfying flow of traditional programming. Until then, we're learning to appreciate a different kind of satisfaction: not the micro-dopamine hits of instant feedback, but the macro-achievements of watching our ideas materialise at unprecedented speed.</p>
<h2 id="discussion-on-linkedin">Discussion on LinkedIn</h2>
<p>see <a href="https://www.linkedin.com/posts/gregdetre_three-fascinating-psychological-illusions-activity-7358805359253225473-318L/?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAAF1wDIBUoRgjztIGLcmp5W_rk7loyfkMx0">LinkedIn discussion &amp; responses</a></p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/psychological-illusions-in-ai-assisted-programming/</guid>
      <pubDate>Mon, 11 Aug 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>I refuse to use this technology, even if it would make me money</title>
      <link>https://www.gregdetre.com/blog/i-refuse-to-use-this-technology-even-if-it-would-make-me-money/</link>
      <description>When AI systems produce hate speech, technologists dismiss it as mere 'bugs' - what would it actually take for us to refuse profitable technology on **moral grounds**?</description>
      <content:encoded><![CDATA[<p>There is a new version of Grok, the Elon Musk-owned xAI bot used on Twitter/X coming out later today.  </p>
<p>We were discussing it on a UK AI WhatsApp group that I'm part of, full of some great technologists.  </p>
<p>This AI is sympathetic to Hitler, and producing hate speech. Last time, when it started talking about white genocide, the company acknowledged that was in direct response to a change that someone (they won’t say who) had made to its system prompt.  </p>
<p>I asked "What would it take for each of us to take a stand, and say, "I refuse to use this technology (even if it would make me money)?” and included a link to a <a href="https://www.theguardian.com/technology/2025/jul/09/grok-ai-praised-hitler-antisemitism-x-ntwnfb">Guardian article</a>. Responses included:  </p>
<ul>
<li>
<p>"I just see it as a bug - we all get them. Theirs is just played out in public to a huge audience."  </p>
</li>
<li>
<p>"We are still at the stage of it being a “tool” and as such a tool has good and bad use, but also bugs"  </p>
</li>
</ul>
<p>I don't want to alienate those people because I feel positively about them as humans, but I want to stand up and say that I don't agree with that view.  </p>
<p>After all, even if we choose to give xAI the benefit of the doubt, and say it’s not deliberate...  </p>
<p>Wouldn’t that suggest then that it’s a sign of a systematically irresponsible, inadequate approach to safety and alignment?  </p>
<p>A genuine question to ask ourselves as technologists: is there anything an AI could say that would make you respond “Hmmm, I’m not willing to use this, even though it's economically valuable"?  </p>
<p>I don’t know whether it’s helpful to create division by calling people out. On balance I think it is more important to point out that we are blurring what I believe should be a clear line in the sand, and to stand publicly on one side of it.</p>
<p>see <a href="https://www.linkedin.com/posts/gregdetre_there-is-a-new-version-of-grok-the-elon-activity-7348708765765963777-_JOK/?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAAF1wDIBUoRgjztIGLcmp5W_rk7loyfkMx0">LinkedIn post &amp; responses</a></p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/i-refuse-to-use-this-technology-even-if-it-would-make-me-money/</guid>
      <pubDate>Thu, 14 Aug 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Product idea - an episode-focused podcast player</title>
      <link>https://www.gregdetre.com/blog/product-idea-an-episode-focused-podcast-player/</link>
      <description>Podcast apps still think in terms of *shows*, but what if they recommended the best *episodes* instead—using AI to parse transcripts and behavioural data to surface hidden gems across the entire medium?</description>
      <content:encoded><![CDATA[<ul>
<li>
<p>Podcast app user interfaces work at the level of a <em>show</em> - you subscribe to a <em>show</em>, it recommends <em>shows</em>.</p>
</li>
<li>
<p>But this is an old-world way of thinking.</p>
</li>
<li>
<p>Really, I want a podcast app that focuses on <em>episodes</em> - that recommends me the best episodes of a given show, and the episodes from other shows that I might like.</p>
</li>
<li>
<p>Recommendation engines usually rely on two main signals:</p>
</li>
<li>
<ol>
<li><strong>social</strong>, i.e. what we can learn from the behaviour of other users, e.g. <em>"other users like </em><em>you</em><em> liked </em><em>this</em><em>"</em></li>
</ol>
</li>
<li>
<ol start="2">
<li><strong>content</strong>, i.e. what we can learn from the substance of the content itself, e.g. "<em>this episode is about X and Y, and you listen to a lot of stuff about both X and Y</em>".</li>
</ol>
</li>
<li>
<p>Viewed through this lens, what data can tell us which are the best episodes of a given show?</p>
</li>
<li>
<ul>
<li><strong>Social</strong> - if you're a popular enough app, you can use behavioural analytics (e.g. the proportion of listeners that listen all the way to the end, or share the episode).</li>
</ul>
</li>
<li>
<ul>
<li><strong>Content</strong> - ask an LLM to read the transcripts, and ask it which episodes seem useful/interesting/funny, which involved famous guests, which involve trending topics, etc.</li>
</ul>
</li>
<li>
<p>What about the episodes from other shows that I might like?</p>
</li>
<li>
<ul>
<li><strong>Social</strong> - standard cross-user collaborative filtering, to identify episodes that other users like me liked. This might not work that well at an episode level, because so much of our listening behaviour currently is driven by show-level patterns.</li>
</ul>
</li>
<li>
<ul>
<li><strong>Content</strong> - build up a profile of the user's interests, and then look for episodes that seem relevant.</li>
</ul>
</li>
<li>
<p>The tricky part, as always with recommendation engines, is to <a href="https://www.makingdatamistakes.com/how-to-know-if-your-recommendations-algorithm-is-actually-doing-a-good-job/">balance relevance with serendipity</a>, e.g. just because I've listened to a couple of interviews with Demis Hassabis doesn't mean that I <em>only</em> want to listen to interviews with him...</p>
</li>
</ul>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/product-idea-an-episode-focused-podcast-player/</guid>
      <pubDate>Sun, 17 Aug 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Human readers deserve human writers: a policy on AI and human craft</title>
      <link>https://www.gregdetre.com/blog/human-readers-deserve-human-writers-a-policy-on-ai-and-human-craft/</link>
      <description>The magic hammer of AI makes everything look wonderfully nail-like, especially for the solo creative who finds in it both trampoline and anchor. But I've decided that human readers deserve the many tiny choices that make creative work personal, reserving my automation for the machines who won't notice the difference.</description>
      <content:encoded><![CDATA[<p>When you have a magic hammer, it's so very tempting to wallop everything you see with it. But should I use the magic AI hammer for my creative work (from writing to technology products)?</p>
<p>It certainly feels easy (usually a warning sign), and often helpful. As a solo creative, even an AI collaborator give me the sudden boost and release of a trampoline, along with a welcome, stabilising outside point of view.</p>
<p><a href="https://www.newyorker.com/culture/the-weekend-essay/why-ai-isnt-going-to-make-art">Ted Chiang gestures at a quantification of artistic expression</a>: <em>"... art is something that results from making a lot of choices. ... When you are writing fiction, you are—consciously or unconsciously—making a choice about almost every word you type; to oversimplify, we can imagine that a ten-thousand-word short story requires something on the order of ten thousand choices. When you give a generative-A.I. program a prompt, you are making very few choices; if you supply a hundred-word prompt, you have made on the order of a hundred choices."</em></p>
<p>So, for a piece of writing that I hope people will read, and perhaps be changed by, it should be primarily my ideas, with words I have chosen. I might ask AI to be my editor - give me a trenchant critique, or provide suggestions to unstick me, or for titles and excerpts. But I want to retain a full feeling of authorship. In other words, I make most of the choices.</p>
<p>If I'm writing primarily for a machine, then I'll use as much AI as I can. For example, it's fine for the AI to write the code, to worry about semicolons and syntax, because they will be consumed by a compiler. But the product vision, the user interface, even the software architecture, these are consumed by the human end-user, and so I want to make these choices.</p>
<p>Proposed policy: <em>I'll hand-craft the work myself according to how much it will be consumed by humans.</em> Corollary: I'll automate when it will be consumed by machines.</p>
<p>I sometimes feel that LinkedIn falls somewhere in the middle :~</p>
<p>(And if I break this policy, I'll be very transparent about it.)</p>
<p>Postscript:</p>
<ul>
<li>
<p>I wrote this post myself. I then asked Claude to critique it, it pointed out some weak bits and made suggestions, and I rewrote parts in response.</p>
</li>
<li>
<p>I wrote a title, and then asked it to generate a few potential better titles.</p>
</li>
<li>
<p>I picked a couple of ideas from those, and created my own title based on that. AI mostly created the excerpt, and completely assigned the tags. This is my standard practice.</p>
</li>
<li>
<p>For LinkedIn posts, it's more complicated. I fed the original post to a set of careful prompts that emphasise my original ideas/content, engaged in a lengthy back-and-forth about potential themes/arcs, picked from a few potential hooks and closes (often ignoring the generated ones and writing one myself), it generates a few candidates, and then I edit the final version. I am still iterating this - I'm not actually sure it's faster, but I think the result is sometimes much better.</p>
</li>
<li>
<p>For the image, I asked AI to generate some ideas based on the text, then picked my favourite few and tweaked them, applying my own visual style, trying out a couple of versions until I found one I liked.</p>
</li>
</ul>
<p>This dialogue shows what Centaur teams (human + AI) look like, with complementary strengths and roles. I think Chiang's framing about 'choice' (and <em>the number</em> of choices) helps characterise and quantify <em>the kind and level of human involvement</em>.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.gregdetre.com/blog/human-readers-deserve-human-writers-a-policy-on-ai-and-human-craft/</guid>
      <pubDate>Sun, 24 Aug 2025 00:00:00 +0000</pubDate>
    </item>
  </channel>
</rss>
