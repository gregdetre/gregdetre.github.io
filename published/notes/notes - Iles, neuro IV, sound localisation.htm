<html xmlns:o="urn:schemas-microsoft-com:office:office"
xmlns:w="urn:schemas-microsoft-com:office:word"
xmlns="http://www.w3.org/TR/REC-html40">

<head>
<meta http-equiv=Content-Type content="text/html; charset=windows-1252">
<meta name=ProgId content=Word.Document>
<meta name=Generator content="Microsoft Word 9">
<meta name=Originator content="Microsoft Word 9">
<link rel=File-List
href="notes%20-%20Iles,%20neuro%20IV,%20sound%20localisation_filelist.xml">
<title>Title</title>
<!--[if gte mso 9]><xml>
 <o:DocumentProperties>
  <o:Author>Greg Detre</o:Author>
  <o:LastAuthor>Greg Detre</o:LastAuthor>
  <o:Revision>2</o:Revision>
  <o:TotalTime>779</o:TotalTime>
  <o:Created>2003-07-02T01:10:00Z</o:Created>
  <o:LastSaved>2003-07-02T01:10:00Z</o:LastSaved>
  <o:Pages>1</o:Pages>
  <o:Words>5632</o:Words>
  <o:Characters>32104</o:Characters>
  <o:Company>-</o:Company>
  <o:Lines>267</o:Lines>
  <o:Paragraphs>64</o:Paragraphs>
  <o:CharactersWithSpaces>39425</o:CharactersWithSpaces>
  <o:Version>9.2720</o:Version>
 </o:DocumentProperties>
 <o:OfficeDocumentSettings>
  <o:DoNotOrganizeInFolder/>
 </o:OfficeDocumentSettings>
</xml><![endif]--><!--[if gte mso 9]><xml>
 <w:WordDocument>
  <w:DrawingGridHorizontalSpacing>4.5 pt</w:DrawingGridHorizontalSpacing>
  <w:DrawingGridVerticalSpacing>12.25 pt</w:DrawingGridVerticalSpacing>
  <w:DisplayHorizontalDrawingGridEvery>0</w:DisplayHorizontalDrawingGridEvery>
  <w:Compatibility>
   <w:WW6BorderRules/>
   <w:FootnoteLayoutLikeWW8/>
   <w:ShapeLayoutLikeWW8/>
   <w:AlignTablesRowByRow/>
   <w:ForgetLastTabAlignment/>
   <w:LayoutRawTableWidth/>
   <w:LayoutTableRowsApart/>
  </w:Compatibility>
 </w:WordDocument>
</xml><![endif]-->
<style>
<!--
 /* Font Definitions */
@font-face
	{font-family:Wingdings;
	panose-1:5 0 0 0 0 0 0 0 0 0;
	mso-font-charset:2;
	mso-generic-font-family:auto;
	mso-font-pitch:variable;
	mso-font-signature:0 268435456 0 0 -2147483648 0;}
@font-face
	{font-family:"Book Antiqua";
	panose-1:2 4 6 2 5 3 5 3 3 4;
	mso-font-charset:0;
	mso-generic-font-family:roman;
	mso-font-pitch:variable;
	mso-font-signature:647 0 0 0 159 0;}
@font-face
	{font-family:Tahoma;
	panose-1:2 11 6 4 3 5 4 4 2 4;
	mso-font-charset:0;
	mso-generic-font-family:swiss;
	mso-font-pitch:variable;
	mso-font-signature:553679495 -2147483648 8 0 66047 0;}
 /* Style Definitions */
p.MsoNormal, li.MsoNormal, div.MsoNormal
	{mso-style-parent:"";
	margin-top:8.0pt;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:0in;
	margin-bottom:.0001pt;
	text-align:justify;
	mso-pagination:widow-orphan;
	font-size:12.0pt;
	mso-bidi-font-size:10.0pt;
	font-family:"Times New Roman";
	mso-fareast-font-family:"Times New Roman";}
h1
	{mso-style-next:Normal;
	margin-top:0in;
	margin-right:0in;
	margin-bottom:6.0pt;
	margin-left:0in;
	text-align:center;
	mso-pagination:widow-orphan;
	page-break-after:avoid;
	mso-outline-level:1;
	font-size:12.0pt;
	mso-bidi-font-size:10.0pt;
	font-family:"Times New Roman";
	color:purple;
	mso-font-kerning:14.0pt;
	mso-bidi-font-weight:normal;
	font-style:italic;
	mso-bidi-font-style:normal;}
h2
	{mso-style-next:Normal;
	margin-top:16.0pt;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:0in;
	margin-bottom:.0001pt;
	mso-pagination:widow-orphan;
	page-break-after:avoid;
	mso-outline-level:2;
	font-size:12.0pt;
	mso-bidi-font-size:10.0pt;
	font-family:"Times New Roman";
	color:navy;
	mso-bidi-font-weight:normal;
	font-style:italic;
	mso-bidi-font-style:normal;}
h3
	{mso-style-next:Normal;
	margin-top:16.0pt;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:0in;
	margin-bottom:.0001pt;
	text-align:justify;
	mso-pagination:widow-orphan;
	page-break-after:avoid;
	mso-outline-level:3;
	tab-stops:right 451.45pt;
	font-size:12.0pt;
	mso-bidi-font-size:10.0pt;
	font-family:"Times New Roman";
	color:green;
	font-weight:normal;
	font-style:italic;
	mso-bidi-font-style:normal;}
h4
	{mso-style-next:Normal;
	margin-top:12.0pt;
	margin-right:0in;
	margin-bottom:3.0pt;
	margin-left:0in;
	text-align:justify;
	mso-pagination:widow-orphan;
	page-break-after:avoid;
	mso-outline-level:4;
	font-size:12.0pt;
	mso-bidi-font-size:10.0pt;
	font-family:Arial;
	mso-bidi-font-family:"Times New Roman";
	mso-bidi-font-weight:normal;}
h5
	{mso-style-next:Normal;
	margin-top:10.0pt;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:0in;
	margin-bottom:.0001pt;
	text-align:justify;
	mso-pagination:widow-orphan;
	mso-outline-level:5;
	font-size:12.0pt;
	mso-bidi-font-size:10.0pt;
	font-family:"Times New Roman";
	color:maroon;
	font-weight:normal;
	text-decoration:underline;
	text-underline:single;}
h6
	{mso-style-parent:"Heading 5";
	mso-style-next:Normal;
	margin-top:9.0pt;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:.5in;
	margin-bottom:.0001pt;
	text-align:justify;
	text-indent:-.25in;
	mso-pagination:widow-orphan;
	mso-outline-level:6;
	mso-list:l2 level1 lfo10;
	tab-stops:list .5in;
	font-size:12.0pt;
	mso-bidi-font-size:11.0pt;
	font-family:"Times New Roman";
	color:olive;
	font-weight:normal;
	mso-bidi-font-weight:bold;
	text-underline:#33CCCC;}
p.MsoToc1, li.MsoToc1, div.MsoToc1
	{mso-style-parent:"Heading 1";
	mso-style-next:Normal;
	margin-top:3.0pt;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:0in;
	margin-bottom:.0001pt;
	mso-pagination:widow-orphan;
	page-break-after:avoid;
	tab-stops:.75in right 451.45pt;
	font-size:11.0pt;
	mso-bidi-font-size:10.0pt;
	font-family:"Times New Roman";
	mso-fareast-font-family:"Times New Roman";
	font-variant:small-caps;
	color:purple;
	mso-font-kerning:14.0pt;
	font-weight:bold;
	mso-bidi-font-weight:normal;
	font-style:italic;
	mso-bidi-font-style:normal;}
p.MsoToc2, li.MsoToc2, div.MsoToc2
	{mso-style-parent:"Heading 2";
	mso-style-next:Normal;
	margin-top:3.0pt;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:10.1pt;
	margin-bottom:.0001pt;
	mso-pagination:widow-orphan;
	page-break-after:avoid;
	tab-stops:right 451.3pt;
	font-size:10.0pt;
	font-family:"Times New Roman";
	mso-fareast-font-family:"Times New Roman";
	color:navy;
	font-style:italic;
	mso-bidi-font-style:normal;}
p.MsoToc3, li.MsoToc3, div.MsoToc3
	{mso-style-parent:"TOC 2";
	mso-style-next:Normal;
	margin-top:3.0pt;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:20.0pt;
	margin-bottom:.0001pt;
	mso-pagination:widow-orphan;
	page-break-after:avoid;
	tab-stops:right 451.45pt;
	font-size:9.0pt;
	mso-bidi-font-size:10.0pt;
	font-family:"Times New Roman";
	mso-fareast-font-family:"Times New Roman";
	color:teal;
	font-style:italic;
	mso-bidi-font-style:normal;}
p.MsoToc4, li.MsoToc4, div.MsoToc4
	{mso-style-update:auto;
	mso-style-parent:"TOC 3";
	mso-style-next:Normal;
	margin-top:3.0pt;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:35.7pt;
	margin-bottom:.0001pt;
	mso-pagination:widow-orphan;
	page-break-after:avoid;
	tab-stops:right 451.45pt;
	font-size:7.5pt;
	mso-bidi-font-size:10.0pt;
	font-family:"Times New Roman";
	mso-fareast-font-family:"Times New Roman";
	color:maroon;
	mso-bidi-font-weight:bold;
	font-style:italic;
	mso-bidi-font-style:normal;}
p.MsoHeader, li.MsoHeader, div.MsoHeader
	{margin-top:8.0pt;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:0in;
	margin-bottom:.0001pt;
	text-align:justify;
	mso-pagination:widow-orphan;
	tab-stops:center 239.75pt right 6.65in;
	font-size:11.0pt;
	mso-bidi-font-size:10.0pt;
	font-family:"Times New Roman";
	mso-fareast-font-family:"Times New Roman";
	font-style:italic;
	mso-bidi-font-style:normal;}
p.MsoListNumber, li.MsoListNumber, div.MsoListNumber
	{margin-top:8.0pt;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:.25in;
	margin-bottom:.0001pt;
	text-align:justify;
	text-indent:-.25in;
	mso-pagination:widow-orphan;
	mso-list:l0 level1 lfo8;
	tab-stops:list .25in;
	font-size:12.0pt;
	mso-bidi-font-size:10.0pt;
	font-family:"Times New Roman";
	mso-fareast-font-family:"Times New Roman";}
p.MsoDate, li.MsoDate, div.MsoDate
	{margin-top:8.0pt;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:0in;
	margin-bottom:.0001pt;
	text-align:right;
	mso-pagination:widow-orphan;
	font-size:11.0pt;
	mso-bidi-font-size:10.0pt;
	font-family:"Times New Roman";
	mso-fareast-font-family:"Times New Roman";
	font-style:italic;
	mso-bidi-font-style:normal;}
p.MsoDocumentMap, li.MsoDocumentMap, div.MsoDocumentMap
	{margin-top:8.0pt;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:0in;
	margin-bottom:.0001pt;
	text-align:justify;
	mso-pagination:widow-orphan;
	background:navy;
	font-size:12.0pt;
	mso-bidi-font-size:10.0pt;
	font-family:Tahoma;
	mso-fareast-font-family:"Times New Roman";}
p.Section, li.Section, div.Section
	{mso-style-name:Section;
	margin-top:8.0pt;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:0in;
	margin-bottom:.0001pt;
	text-align:center;
	mso-pagination:widow-orphan;
	font-size:12.0pt;
	mso-bidi-font-size:10.0pt;
	font-family:"Times New Roman";
	mso-fareast-font-family:"Times New Roman";
	font-weight:bold;
	mso-bidi-font-weight:normal;}
p.Heading0, li.Heading0, div.Heading0
	{mso-style-name:"Heading 0";
	mso-style-parent:"Heading 1";
	margin-top:6.0pt;
	margin-right:0in;
	margin-bottom:6.0pt;
	margin-left:0in;
	text-align:center;
	mso-pagination:widow-orphan;
	page-break-after:avoid;
	mso-outline-level:1;
	border:none;
	mso-border-alt:solid windowtext .5pt;
	padding:0in;
	mso-padding-alt:1.0pt 4.0pt 1.0pt 4.0pt;
	font-size:16.0pt;
	mso-bidi-font-size:10.0pt;
	font-family:"Book Antiqua";
	mso-fareast-font-family:"Times New Roman";
	mso-bidi-font-family:"Times New Roman";
	font-variant:small-caps;
	color:purple;
	font-weight:bold;
	mso-bidi-font-weight:normal;}
@page Section1
	{size:595.3pt 841.9pt;
	margin:68.05pt 68.05pt 68.05pt 68.05pt;
	mso-header-margin:.5in;
	mso-footer-margin:.5in;
	mso-paper-source:0;}
div.Section1
	{page:Section1;}
 /* List Definitions */
@list l0
	{mso-list-id:-120;
	mso-list-type:simple;
	mso-list-template-ids:-1247397174;}
@list l0:level1
	{mso-level-style-link:"List Number";
	mso-level-tab-stop:.25in;
	mso-level-number-position:left;
	margin-left:.25in;
	text-indent:-.25in;}
@list l1
	{mso-list-id:1470240892;
	mso-list-type:hybrid;
	mso-list-template-ids:-318178384 1685484530 67698713 67698715 67698703 67698713 67698715 67698703 67698713 67698715;}
@list l1:level1
	{mso-level-tab-stop:.25in;
	mso-level-number-position:left;
	margin-left:17.85pt;
	text-indent:-17.85pt;}
@list l2
	{mso-list-id:1476406644;
	mso-list-type:hybrid;
	mso-list-template-ids:168605670 266271734 67698691 67698693 67698689 67698691 67698693 67698689 67698691 67698693;}
@list l2:level1
	{mso-level-number-format:bullet;
	mso-level-style-link:"Heading 6";
	mso-level-text:o;
	mso-level-tab-stop:.5in;
	mso-level-number-position:left;
	text-indent:-.25in;
	font-family:"Courier New";
	mso-bidi-font-family:"Times New Roman";}
@list l2:level2
	{mso-level-tab-stop:1.0in;
	mso-level-number-position:left;
	text-indent:-.25in;}
@list l3
	{mso-list-id:1746143482;
	mso-list-type:simple;
	mso-list-template-ids:-719658964;}
@list l3:level1
	{mso-level-tab-stop:.25in;
	mso-level-number-position:left;
	margin-left:.25in;
	text-indent:-.25in;}
ol
	{margin-bottom:0in;}
ul
	{margin-bottom:0in;}
-->
</style>
</head>

<body lang=EN-GB style='tab-interval:17.85pt'>

<div class=Section1>

<h1>Notes � Iles Neuro, sound localisation</h1>

<p class=MsoNormal align=right style='text-align:right'>Greg Detre</p>

<p class=MsoNormal align=right style='margin-top:2.0pt;text-align:right'>Sunday,
04 November, 2001</p>

<p class=MsoNormal align=right style='margin-top:2.0pt;text-align:right'>Dr
Iles, week 5</p>

<p class=MsoNormal><![if !supportEmptyParas]>&nbsp;<![endif]><o:p></o:p></p>

<h2>Essay title</h2>

<p class=MsoNormal><i style='mso-bidi-font-style:normal'><span
style='mso-fareast-language:EN-GB'>How does the auditory system perform sound
localisation? (made-up)<o:p></o:p></span></i></p>

<h3>Possible alternatives</h3>

<p class=MsoNormal><span style='mso-fareast-language:EN-GB'>Assess the relative
contributions of different parts of the auditory system in sound localisation.</span></p>

<p class=MsoNormal>How well can we understand the mechanisms of sound
localization by studying the physiology of neurones at specific locations in
the auditory pathways?</p>

<p class=MsoNormal>Develop a research strategy for investigating how human
observers might track a moving auditory target.</p>

<p class=MsoNormal>How relevant are studies of auditory localisation mechanisms
in birds to understanding auditory localisation in man?</p>

<p class=MsoNormal>With reference to experimental studies, explain how a person
who is deaf in one ear might localise sounds in space.</p>

<h2>Planned reading</h2>

<p class=MsoNormal>Konishi, M. (1993), 'Listening with Two Ears', Scientific
American, vol. 268, No. 4.</p>

<h2>Notes � Kandel and Schwarz (1991), ch 32, �Hearing�, pg 481</h2>

<p class=MsoNormal>Ohm proposed that the ear performs a type of Fourier
spectral analysis � complex waveforms are simplified into the sum of many
individual sine waves and cosine waves of appropriate frequencies, phases and
amplitudes</p>

<p class=MsoNormal>three parts to the ear � outer, middle and inner ear</p>

<p class=MsoNormal style='margin-left:17.85pt'><i style='mso-bidi-font-style:
normal'>cochlea</i> of the inner ear � a spiral bony canal filled with fluid
that contains the sensory transduction apparatus, the <i style='mso-bidi-font-style:
normal'>organ of Corti</i></p>

<p class=MsoNormal style='margin-left:17.85pt'>vestibular apparatus of the
inner ear � important for maintaining body posture and integrating head/eye
movements</p>

<p class=MsoNormal>sound is produced by vibrations � alternating
compressions/rarefaction (increased/decreased pressure) of surrounding air,
which radiates outward from the source as a pressure wave</p>

<p class=MsoNormal style='margin-left:17.85pt'>pitch = frequency (the number of
peaks that pass a given point per unit time)</p>

<p class=MsoNormal style='margin-left:17.85pt'><span style='font-size:9.0pt;
mso-bidi-font-size:10.0pt'>middle C = 261Hz, human sensitivity 20-20,000Hz<o:p></o:p></span></p>

<p class=MsoNormal style='margin-left:17.85pt'><span style='font-size:9.0pt;
mso-bidi-font-size:10.0pt'>amplitude = a maximum change in air pressure in
either direction = loudness<o:p></o:p></span></p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:35.7pt;margin-bottom:.0001pt'><span style='font-size:9.0pt;
mso-bidi-font-size:10.0pt'>sound pressure level (in decibels) = 20 log<sub>10</sub>
P<sub>t</sub>/P<sub>r</sub> (in newtons, N, per sq metre)<o:p></o:p></span></p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:53.55pt;margin-bottom:.0001pt'><span style='font-size:9.0pt;
mso-bidi-font-size:10.0pt'>where:<o:p></o:p></span></p>

<p class=MsoNormal style='margin-top:0in;margin-right:0in;margin-bottom:0in;
margin-left:71.4pt;margin-bottom:.0001pt'><span style='font-size:9.0pt;
mso-bidi-font-size:10.0pt'>P<sub>t</sub> = test pressure<o:p></o:p></span></p>

<p class=MsoNormal style='margin-top:0in;margin-right:0in;margin-bottom:0in;
margin-left:71.4pt;margin-bottom:.0001pt'><span style='font-size:9.0pt;
mso-bidi-font-size:10.0pt'>P<sub>r</sub> = reference presure (20 �N/m<sup>2</sup>)<o:p></o:p></span></p>

<p class=MsoNormal style='margin-left:53.55pt'><span style='font-size:9.0pt;
mso-bidi-font-size:10.0pt'>this scale was devised by Bell, who found that the
Weber-Fechner law applies to hearing<o:p></o:p></span></p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:71.4pt;margin-bottom:.0001pt'>incremental increases in subjective
loudness correspond to equal increments of sound pressure level (SPL)
regardless of the absolute value of the sound pressure level</p>

<p class=MsoNormal style='margin-left:71.4pt'>the ear has a range of about
120dB, which corresponds to about 6 orders of magnitude of pressure</p>

<p class=MsoNormal>�Sounds reaching the ear travel through the external ear
canal, or external auditory <i style='mso-bidi-font-style:normal'>meatus </i>(Latin,
opening) and reach the middle ear, causing the <i style='mso-bidi-font-style:
normal'>tympanic </i>(Latin, drum) <i style='mso-bidi-font-style:normal'>membrane
</i>to vibrate. This vibration is then conveyed through the middle ear by a
series of three small bones (ossicles), one of which the <i style='mso-bidi-font-style:
normal'>malleus</i> (Latin, hammer) is attached to the tympanic membrane. The
vibration of the malleus is transmitted to an opening in the cochlea, the <i
style='mso-bidi-font-style:normal'>oval window</i>, by the other two ossicles,
the <i style='mso-bidi-font-style:normal'>incus</i> (Latin, anvil) and the <i
style='mso-bidi-font-style:normal'>stapes</i> (Latin, stirrup). The major
components of the middle ear (tympanic membrane and ossicles) ensure that sounds
from the air in the outer area are transmitted efficiently to the fluid-filled
cochlea of the inner ear. Otherwise, sounds would reach the fluid at the oval
window directly, and most of the sound energy would be reflected because fluid
has a higher acoustic impedance than air, and so the sound pressure required
for hearing would be higher.� Because the area of the tympani membrane is
greater than the area of the oval window, the total pressure (force per unit
area) acting on the smaller oval window is increased.</p>

<p class=MsoNormal>�The cochlea spirals for two-and-a-half turns around a
central pillar called the <i style='mso-bidi-font-style:normal'>modiolus</i>
(Latin, pillar or hub). The cochlea has three fluid-filled compartments or <i
style='mso-bidi-font-style:normal'>scalae</i> (Italian, stairway): the <i
style='mso-bidi-font-style:normal'>scala tympani</i>, which follows the outer
contours of the cochlea; the <i style='mso-bidi-font-style:normal'>scala
vestibuli</i>, which follows the inner contours and is continuous with the
scale tympani at the <i style='mso-bidi-font-style:normal'>helicotrema</i> (Greek,
spiral hole); and between these two, the <i style='mso-bidi-font-style:normal'>scala
media</i> (or cochlear duct) which extends finger-like into the cochlear
channel and ends blindly near the apical end of the cochlear.</p>

<p class=MsoNormal>�Sound entering the ear causes the stapes to oscillate, and
these oscillations transmit energy to each of the three compartments. Because
fluid is not compressible, the pressure being put on the fluid in the scala
vestibuli causes an alternating outward and inward movement of the round window
membrane of the scala tympani, as well as oscillating movements of the scala
media and of the basilar membrane (the floor of the scala media). The organ of
Corti, the sensory transduction apparatus in the scala media rests on the
basilar membrane and is also stimulated by this movement. Thus, the cochlear
compartments convert the differential pressure between the scala vestibuli and
scala tympani into oscillating movements of the basilar membrane that excite
and inhibit the sensory transducing cells in the organ of Corti.�</p>

<p class=MsoNormal style='margin-left:17.85pt'><i style='mso-bidi-font-style:
normal'>bone conduction</i> = when sounds bypass the middle ear to reach the
cochlear directly by vibration of the entire temporal bone</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:35.7pt;margin-bottom:.0001pt'>but this is inefficient and important
only for audiological diagnosis <span style='font-size:9.0pt;mso-bidi-font-size:
10.0pt'>(Rinne compared hearing-impaired patients� ability to detect air- vs
bone-conducted sounds using a tuning fork in contact with the mastoid process
of the temporal bone behind the ear, highlighting disruption of the air
conductive apparatus)</span></p>

<p class=MsoNormal style='margin-left:17.85pt'>thus distinguish 2 broad classes
of deafness</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:35.7pt;margin-bottom:.0001pt'><i style='mso-bidi-font-style:normal'>conductive
deafness</i> � caused by damage to the inner ear (often surgically repairable)</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:35.7pt;margin-bottom:.0001pt'><i style='mso-bidi-font-style:normal'>sensorineural
deafness</i> � caused by damage to the cochlear, the eighth nerve or the
central auditory pathway</p>

<p class=MsoNormal>the organ of Corti contains the (inner and outer) <i
style='mso-bidi-font-style:normal'>hair cells</i>, the sensory receptor cells
of the inner ear, as well as a variety of supporting cells</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>particular sets of hair cells are moved
by motion initiated in a particular portion of the basilar membrane</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>there are three rows of outer hair
cells and one row of inner hair cells (inner/outer from the modiolous (central
pillar))</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>each hair cell has a bundle of
stereocilia on its apical surface � they are stiff because they are filled with
parallel arrays of cross-bridge actin filaments</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>these stereocilia project into (and
are embedded in) the overlying <i style='mso-bidi-font-style:normal'>tectorial
membrane</i> � if the tectorial membrane and the basilar membrane (in which the
bodies of the hair cells rest) move with respect to one another, the
stereocilia will be displaced/bent</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>this causes a change in ionic
conductance at the apical surface of the cell, a current flow and voltage
change - motion of the stereocilia hyper/depolarises the cell, by opening ion
channels that produce an inward current � thus the oscillatory movement of the
basilar membrane results in sinusoidal (depolarising-hyperpolarising) potential
changes at the frequency of the sound</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>the hair cell releases chemical
transmitter at the basal end, contacting the peripheral branches of axons of
bipolar neurons whose cell bodies lie in the spiral ganglion and whose central
axons constitute the auditory nerve</p>

<p class=MsoNormal>how are different frequencies of sound neurally encoded?
Helmholtz noticed two interesting features:</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>the basilar membrane has cross
striations (like the strings of a piano)</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>the basilar membrane varies in width
from the base to the apex of the cochlea � narrow and stiff near the oval
window and wide and flexible near the apex of the cochlea</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>Helmholtz <i style='mso-bidi-font-style:
normal'>resonance theory</i> � the cross striations resonate with different
frequencies of sounds (like piano strings which of different length/stiffness)</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>von Bekesy tested this and found
that each sound does not lead to the resonance of only one narrow segment of
the basilar membrane, but initates a <i style='mso-bidi-font-style:normal'>traveling
wave</i> along the length of the cochlea that starts at the oval window (like
snapping a rope tied at one end to a post)</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:35.7pt;margin-bottom:.0001pt'>different frequencies of sound
produce different travelling waves with <i style='mso-bidi-font-style:normal'>peak</i>
amplitudes at different points along the basilar membrane</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:53.55pt;margin-bottom:.0001pt'>at low frequences, the peak
amplitude of the motion is near the apex of the cochlea (in the region of
helicotrema)</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:53.55pt;margin-bottom:.0001pt'>as the frequency of the stimulus
increase, the peak amplitude of motion occurs closer to the base of the cochlea</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:53.55pt;margin-bottom:.0001pt'>higher amplitudes cause the peak
vibration to increase in displacement and a broader region of the membrae to
vibrate</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:53.55pt;margin-bottom:.0001pt'>the representation of frequencies
along the basilar membrane is logarithmic</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:53.55pt;margin-bottom:.0001pt'>the hair cells situated at the site
where the oscillation is maximal are the most excited (occuring at the points
predicted by Helmholtz�s resonance theory)</p>

<p class=MsoNormal>the hair cells within the organ of Corti differ from one
another in their electromechanical properties � this may be very important in
determining frequency selectivity</p>

<p class=MsoNormal style='margin-left:17.85pt'><i style='mso-bidi-font-style:
normal'>mechanical resonance</i> � the hair cells themselves are differently
tuned</p>

<p class=MsoNormal style='margin-top:2.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>at the base of the cochlea (narrow +
stiff basilar membrane) � the outer hair cells and their stereocilia are short
+ stiff</p>

<p class=MsoNormal style='margin-top:2.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>at the apex (more flexible basilar
membrane) � hair cells + stereocilia are much longer and more flexible</p>

<p class=MsoNormal style='margin-left:17.85pt'><i style='mso-bidi-font-style:
normal'>electrical resonance</i> � the hair cell membrane shows spontaneous voltage
fluctuations/oscillations in membrane potential around the resting potential</p>

<p class=MsoNormal style='margin-top:2.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>the characteristic frequency of the
spontaneous electrical oscilation in each cell matches the frequency at which
the cell is most responsive to mechanical stimuli</p>

<p class=MsoNormal style='margin-left:17.85pt'>the interaction of the
mechanical (the physical properties of the hair cell and its stereocilia) and
electrical (the electrical membrane characteristics of the cell) resonances of
the cell tune the hair cell to a particular frequency (like a tuned amplifier)</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>also, the outer hair cells can
increase or decrease the length of their cell bodies in response to
transcellular alternating current stimulation, when maintained in culture
(Brownell)</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>in reverse, the ear itself can
produce sounds, termed <i style='mso-bidi-font-style:normal'>otoacoustic
emissions</i> (Kemp)</p>

<p class=MsoNormal><![if !supportEmptyParas]>&nbsp;<![endif]><o:p></o:p></p>

<p class=MsoNormal>the hair cells are innervated by bipolar neurons of the
spiral ganglion in the modiolus of the cochlea</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>there are about 33,000 spiral
ganglion cells in the human cochlea</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>approximately 90% of the fibres
innervate the inner hair cells</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>each of the 3000 inner hair cells
receives contacts from about 10 fibres, and each fibre contacts one inner hair
cells</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>the remaining 10% innervate many
outer hair cells</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>there are also some efferent fibres
synapsing on outer hair cells (which may be related to changing their length
and tuning the organ of Corti and possibly the entire cochlear to sounds of
particular interest) and on the afferent axons innervating inner hair cells</p>

<p class=MsoNormal>individual auditory nerve fibres respond to a particular <i
style='mso-bidi-font-style:normal'>characteristic frequency</i> of sound, according
to their <i style='mso-bidi-font-style:normal'>tuning curve<o:p></o:p></i></p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>given a maximum firing rate of 500Hz
for a neuron, afferent fibres can�t use a one-to-one code</p>

<p class=MsoNormal style='margin-top:2.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>the firing pattern response of a
brief tone is not instantaneous, but builds up</p>

<p class=MsoNormal style='margin-top:2.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>most sounds that are biologically
significant to humans contain both amplitude-modulate and frequency-modulated
components, so there must be a mechanism to de-modulate these components to
receive the input signal � this requires information about the average
characteristics of the response and its time structure</p>

<p class=MsoNormal style='margin-top:2.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>the <i style='mso-bidi-font-style:
normal'>volley principle</i> (Weaver) � several fibres <i style='mso-bidi-font-style:
normal'>phase-locking</i> (preferential firing at a particular point of the
sound wave) at different cycles of a high-frequency stimulus might work in
concert to signal high frequencies</p>

<p class=MsoNormal style='margin-top:2.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>the <i style='mso-bidi-font-style:
normal'>place principle</i> � emphasises the importance of orderly connections
between the auditory nerve and the brain as the basis of our ability to detect
a broad range of sound frequencies, with each nerve fibre identified by the
site (i.e. frequency) it innervates in the cochlea</p>

<p class=MsoNormal>detecting speech sounds is difficult because the vibrations
of the mouth and tongue are much slower (c. 10Hz) than the vocal cords, and
below the frequency sensitivity of the ear</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>like a frequency analyser, the
auditory system looks for speech <i style='mso-bidi-font-style:normal'>formants</i>,
spectral peaks at particular frequencies that characterise different vowel
sounds � these are represented in specific temporal and spatial patters of
nerve fibre discharges</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>an individual fibre may have
spectral compoentns in its firing pattern at both the frequency of vocal cord
vibration and at the lower modluating frequency imposed on the speech sounds by
the resonances of mouth and tone, acting like a demodulator in a radio to
extract significant low-frequency information from a high frequency carrier
wave</p>

<p class=MsoNormal><![if !supportEmptyParas]>&nbsp;<![endif]><o:p></o:p></p>

<p class=MsoNormal>auditory nerves in the eighth nerve terminate in the
cochlear nucleus (on the external aspect of the inferior cerebellar peduncle),
divided into dorsal and ventral (anteroventral and posteroventral)</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'><i style='mso-bidi-font-style:normal'>tonotopic
organisation</i> � the topography of the cochlear nucleus</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>2 main cells of the ventral cochlear
nucleus (Oertel). when depolarised by steady current pulse:</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:35.7pt;margin-bottom:.0001pt'>stellate cell � generates a spaced
series of action potentials at regular intervals, called a <i style='mso-bidi-font-style:
normal'>chopper resopnse</i></p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:35.7pt;margin-bottom:.0001pt'>bushy cell � generates only one or
two spikes at the beginning of the pulse, signaling the onset and timing
(important for localisation)</p>

<h5>Bilateral auditory pathways provide cues to localise sound</h5>

<p class=MsoNormal>the localisation of sounds in space is achieved in the brain
by comparison of differences in the intensity and timing of sounds received in
each ear</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>the duration of the delay between the
sound being heard by both ears depends on the distance between the two ears,
the speed of sound and the location of the sound source</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>if the sound is along the midline,
front or back, the delay would be zero</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>at 90<span style='font-size:10.0pt;
font-family:Symbol;mso-ascii-font-family:"Times New Roman";mso-hansi-font-family:
"Times New Roman";mso-char-type:symbol;mso-symbol-font-family:Symbol'><span
style='mso-char-type:symbol;mso-symbol-font-family:Symbol'>�</span></span> to
the right or left, the inter-aural delay would be up to 50�s � between tese
extremes, there is a spectrum of <i style='mso-bidi-font-style:normal'>interaural
time difference</i></p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>for low frequency sounds
(&lt;1400Hz), a continuous tone can be localised by phase difference</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>at higher frequencies, where the
wavelength of the sound is less than the distance between the two ears, the
phase or time difference of a continuous tone becomes ambiguous, because the
phases could align along multiple cycles</p>

<p class=MsoNormal style='margin-top:2.0pt;margin-right:0in;margin-bottom:0in;
margin-left:35.7pt;margin-bottom:.0001pt'>at these frequencies, the head acts
as a sound shield, reflecting and absorbing the shorter wavelengths of sound to
produce an <i style='mso-bidi-font-style:normal'>interaural intensity
difference</i></p>

<p class=MsoNormal>Konishi used the owl�s brain as a model system for analysing
sound localisation. �Neurons concerned with the detection of interaural timing
differences are found in the laminar nucleus, a part of the central auditory
pathway in the medulla of the bird�s brain. This bilateral nucleus receives
fibres from the cochlear nuclei on either side and is organised tonotopically
into a number of isofrequency zones, where the neurons and fibres share the
same characteristic frequency. When a recording electrode is advanced through
each isofrequency zone to record the response of successive fibres to a sound
stimulus, orderly shifts in the time of arrival of spikes phase-locked to the
sound stimulus are observed. Neurons in the isofrequency zones act as coincidence
detectors by integrating inputs from fibres at the same frequency but at
different interaural delays. Therefore frequency and time are mapped along
orthogonal axes in the <i style='mso-bidi-font-style:normal'>timing pathway</i>
of the brain.</p>

<p class=MsoNormal>�Interaural differences in sound intensity are analysed
using excitatory and inhibitory interactions between inputs from the two ears
in the <i style='mso-bidi-font-style:normal'>intensity pathway</i> of the brain.
Neurons in the principal relay nucleus of this pathway are ecited by
contralateral stimulation and inhibited by ipsilateral stimulation of the ear.
When both ears are stimulated equally, excitation and inhibition balance, and
there is little response from the post-synaptic cell. When sound amplitude is
decreased in one ear, neurons in the intensity pathway on the same side
respond, since ipsilateral inhibition has been decreased. Neurons are arranged
according to the specific interaural intensity difference to which they are
most responsive. At higher levels, the timing and intensity pathways converge,
where there are neurons broadly tuned in frequency but specific in spatial
localisation of sound.</p>

<p class=MsoNormal>�In the mammalian brain, axons from the cochlear nuclei project
to several brain stem auditory nuclei, so there are many possibilities for
interconnections among the relay nuclei. From the cochlear nucleus, axons
project along three pathways:</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>the <i style='mso-bidi-font-style:
normal'>dorsal acoustic stria<o:p></o:p></i></p>

<p class=MsoNormal style='margin-top:2.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>the <i style='mso-bidi-font-style:
normal'>intermediate acoustic stria</i></p>

<p class=MsoNormal style='margin-top:2.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>the <i style='mso-bidi-font-style:
normal'>trapezoid body<o:p></o:p></i></p>

<p class=MsoNormal style='margin-top:2.0pt;margin-right:0in;margin-bottom:0in;
margin-left:35.7pt;margin-bottom:.0001pt'>this contains fibres that go to <i
style='mso-bidi-font-style:normal'>superior olivary nuclei</i> on both sides of
the brain stem</p>

<p class=MsoNormal style='margin-top:2.0pt;margin-right:0in;margin-bottom:0in;
margin-left:35.7pt;margin-bottom:.0001pt'>the <i style='mso-bidi-font-style:
normal'>medial superior olive</i> is concerned with sound localisation on the
basis of interaural time differences � this nucleus is composed of
spindle-shaped neurons with one medial and one lateral dendrite, which receive
input from the contralateral and ipsilateral cochlear nuclei, respectively. The
binaural cells in the medial superior olive are very sensitive to phase differences
between continuous tones presented to the two ears</p>

<p class=MsoNormal style='margin-top:2.0pt;margin-right:0in;margin-bottom:0in;
margin-left:35.7pt;margin-bottom:.0001pt'>the <i style='mso-bidi-font-style:
normal'>lateral superior olive</i> is concerned with interaural differences in
sound intensity</p>

<p class=MsoNormal>�Axons arising from the superior olivary nuclei join the
crossed and uncrossed axons from the cochlear nucleus to form the <i
style='mso-bidi-font-style:normal'>lateral lemniscus</i>. Thus there is extensive
bilateral auditory input in the CNS from the outset, so that lesions of the
central auditory pathwya do not cause monaural disability. The lateral
lemniscus courses through <i style='mso-bidi-font-style:normal'>nuclei of the
lateral lemniscus</i>, where some fibres synapse. Here again there is extensive
crossing between the two sides through <i style='mso-bidi-font-style:normal'>Probst�s
commissure</i>. All fibres in the lateral lemniscus eventually synapse in the <i
style='mso-bidi-font-style:normal'>inferior colliculus</i>. The cells of the
inferior colliculus receive binaural input and are arranged tonotopically. Most
of the cells in the inferior colliculus send their axons to the <i
style='mso-bidi-font-style:normal'>medial geniculate body</i> of the thalamus
on the same side of the brain. The cells in the medial geniculate body send
their axons to the ipsilateral <i style='mso-bidi-font-style:normal'>primary
auditory cortex</i> in the superior temporal gyrus (Brodmann�s areas 41 and
42).</p>

<p class=MsoNormal><![if !supportEmptyParas]>&nbsp;<![endif]><o:p></o:p></p>

<p class=MsoNormal>�The primary auditory cortex contains several distinct
tonotopic maps of the frequency spectrum, analogous to the multiple
representations of the periphery in the somatic sensory and visual cortices,
e.g. layer IV is the input layer, layer V projects back to the medial
geniculate nucleus, and layer VI projects back to the inferior colliculus.</p>

<p class=MsoNormal>organisation of the primate auditory cortex (Brugge):</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>functionally organised into columns
� binaural cells are found clustered into two alternating columnar groups, <i
style='mso-bidi-font-style:normal'>summation columns</i> and <i
style='mso-bidi-font-style:normal'>suppression columns</i>, running from the
pial surface to the underlying white matter</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:35.7pt;margin-bottom:.0001pt'>in summation columns, the response of
a cell to binaural input is greater than to monaural input</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:35.7pt;margin-bottom:.0001pt'>in suppression columns, input from
one ear is dominant; the response of a clell to input from the dominant ear is
greater than to binaural input � columns of this kind may be related to spatial
maps of sound localisation in the cortex</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>important callosal connections �
zones that do and don�t receive them are interspersed</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>extensive inputs from each ear in
both hemispheres, so unilateral lesions of the auditory cortex do not
dramatically disrupt the perception of sound frequency (though they do affect
the ability to localise sounds slightly). Each hemisphere is concerned mainly
with localising sounds on the contralateral side.</p>

<p class=MsoNormal style='margin-top:4.0pt'>Broca�s area and Wernicke�s area are
related to the perception of speech sounds.</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>Oddly enough, the neural machinery
in echolocating bats uses many of the same sound cues known to be important for
speech (Saga). The sounds emitted by echolocating bats have two principal
components:</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:35.7pt;margin-bottom:.0001pt'>a <i style='mso-bidi-font-style:normal'>constant
frequency component</i> similar to the formants in vowel sounds</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:35.7pt;margin-bottom:.0001pt'>a <i style='mso-bidi-font-style:normal'>frequency
modulated component</i> similar to the changing frequencies in consonants</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>between the emitted sound and the
echo, bats must distinguish 16 components to gauge the velocity and distance of
their prey � this is qualitatively similar to the task of perceiving the
subtleties in speech sounds</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>there is strong evidence that the
bat�s cerebral cortex includes areas where harmonic combinations of frequences
are represented � although no such cells have yet been found in the human
brain, they would be ideal detectors of the multiple frequencies in the speech
sounds produced by the human voice</p>

<p class=MsoNormal>�In addition to parallel pathways, the auditory system has an
extensive set of <i style='mso-bidi-font-style:normal'>feedback connections</i>,
e.g. some cells in the auditory cortex send their axons back to the medial
geniculate nucleus and some back to the inferior colliculus. The inferior
colliculus sends recurrent fibres to the cochlear nucleus. A cluster of cells
near the superior olivary complex gives rise to the efferent olivochlear
bundle, which terminates either on the hair cells of the cochlea directly or on
the afferent fibres innervating them. These connections may be important for
regulating attention to particular sounds by modulating the transduction
mechanism in the organ of Corti.</p>

<h2>Notes - Kaas &amp; Hackett, 1999, �What� and �where� processing in auditory
cortex</h2>

<p class=MsoNormal>they consider the idea that perception (in all three major
sensory systems) divides neatly into what and where processing pathways</p>

<p class=MsoNormal>support comes from:</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>lesions studies in monkeys, in which
impairment of spatial abilities or object identification can be separated</p>

<p class=MsoNormal style='margin-top:2.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>anatomical studies, showing that
connections in the visual pathways are fairly strongly segregated</p>

<p class=MsoNormal>the auditory system too must analyse both identity and
location of stimuli</p>

<p class=MsoNormal>the initial signals from the two cochleas are conveyed to a
complex network of pathways and nuclei in the brainstem and thalamus, where
spectral and temporal information is extracted to determine the identity and
location (requiring binaural comparison in the brainstem) of the sound source</p>

<p class=MsoNormal>Romanski et al examined the connectivity of higher auditory
cortical areas in macaque monkeys, using a combination of anatomical tracer
dyes with electrophysiological recordings. their results support the
ventral/dorsal temporal/parietal what/where processing dichotomy, contributing
to functionally distinct regions of the frontal lobe</p>

<p class=MsoNormal>primate auditory cortex has three similar primary or
primary-like areas, each tonotopically organised and receiving activating
inputs directly from the auditory thalamus</p>

<p class=MsoNormal>these project to 7 or 8 proposed fields - 3 are easy to get
at with an electrode, and these seem to provide anatomical support for the
beginnings of ventral and dorsal cortical processing streams - they project to
largely different portions of the frontal lobe</p>

<p class=MsoNormal>however, the middle belt area makes ocnnections to teh
frontal lobe that overlap those of the two putative streams, indicating
possible intermediate or additional auditory streams (also analogous to the
additional functional streams or 'streams within streams' found in the visual
system)</p>

<h2>Notes - Moore &amp; King, 1999, Auditory perception: the near and far of
sound localisation</h2>

<p class=MsoNormal>Most experiments on auditory localisation have been
concerned with horizontal and vertical positions of sound sources, ignoring the
third dimension - distance. There has been little work on this since von
Bekesy, until Bronkhorst and Houtgast's demonstration using virtual sound
technology that �the perception of sound distance in an enclosed room by human
listeners can be quite simply modelled by fitting a temporal window around the
ratio of direct-to-reverberant sound energy; and Graziano et al have shown that
neurons in the frontal cortex of monkeys respond preferentially to sounds
presented at particular near distances, within a hand grasp of the monkey's
head.�</p>

<h3>Distance cues in an enclosed space</h3>

<p class=MsoNormal>�In addition to the classic cues of interaural time and
level differences, sound localisation in the horizontal and vertical planes -
'direction perception' - is known to depend on spectral cues provided by the
directional filtering of higher frequency sounds by the body, head and
particularly, outer ear. Sound direction perception usually works best in the
'free field' (or anechoic rooms). However, distance perception of an unfamiliar
sound is not particularly good in the free field - at distances greater than
the sound's longest wavelength � because it is largely determined by, and
therefore confounded with, the level of the sound.�</p>

<p class=MsoNormal>virtual space sounds - the cues provided by the head, the
ears and the room are measured, digitally synthesised and mixed with the
acoustic characteristics of the presenting headphones - indistinguishable from
real sounds presented within rooms by distant loudspeakers</p>

<p class=MsoNormal>this has enabled the independent manipulation of various
cues</p>

<h3>Neural encoding of sound source distance</h3>

<p class=MsoNormal>&quot;Neurophysiological studies with a range of spcies have
shown that neurons throughout the central auditory pathway tend to be tuned, to
a greater or lesser extent, to the direction of a sound source, e.g.
echolocation in bats.</p>

<p class=MsoNormal>The activity of the multi-sensory mammalian superior
colliculus can be enhanced or suppressed when stimuli of different sensory
modalities are presented in combination, especially depending on the relative
timing of the signals. For visual-auditory neurons, the largest response
enhancements are often observed when the auditory stimulus is delayed with
respect to the visual stimulus, a consequence of the difference in the time
course of the transduction mechanisms for these two sensory systems.</p>

<p class=MsoNormal>There is less evidence for distance tuning in the auditory
system of non-echolocating animals. Graziano recorded from the monkey's ventral
premotor cortex which, like the superior colliculus, is a multisensory area
involved in the sensory guidance of movement. They have shown that the auditory
receptive fields of ventral premotor cortex neurons, like their visual
counterparts, rarely extend beyond 30cm from the head (and are therefore
restricted to a region of space within the monkey's reach).</p>

<p class=MsoNormal>About 60% of these neurons responded more strongly to
broadband sounds positioned 10cm from the head compared to those at distances
of 30cm or 50cm.</p>

<p class=MsoNormal>These ventral premotor neurons had overlapping visual,
auditory and tactile receptive fields - this seems to be for control of head
and arm movements toward or away from stimuli in the vicinity of the animal's
body</p>

<h3>Basis for auditory distance representation in the cortex</h3>

<p class=MsoNormal>&quot;There is, however, a fundamental difference in the way
that auditory distance was examined in the two studies. The virtual space stimuli
used by Bronkhurst and Houtgast simulated source distances of a metre or more -
the far field, and refers to the region of space within which both monaural and
binaural cue values are essentially independent of distance. In contrast, the
distances in the Graziano et al. study were within the near field, a more
complex region within which energy circulates without propagating. Monaural
spectral cues and interaural level differences associated with near-field sound
sources therefore vary with distance, providing a possible basis for distance
discrimination by both individual neurons and human listeners. This is
obviously useful for localising nearby sounds, but doesn't establish whether
auditory neurons in non-echolocating mammals are sensitive to the other cues
available for more distant sound sources.&quot;</p>

<h2>Notes � web, �Locating a mouse by its sound�</h2>

<p class=MsoNormal>Payne had shown that the owls make use of their superb
scotopic vision to pounce on mice in very dim light. However, they are still
able to catch the mouse even if the lights are turned off as the owl is leaving
its perch, as long as the mouse makes some sort of noise. This ability is
impaired if the owl's ears are plugged.</p>

<p class=MsoNormal>&quot;Eric Knudsen and Mark Konishi took this behavior into
the lab and developed a technique to assess the animal's ability to localize
sound. They trained an owl to sit on a perch in a dark, sound proof, anechoic
room and attend to a sound produced by a speaker that could be positioned
anywhere about the owl. When an owl localizes a sound, it turns its head,
toward the direction of the sound. The owl's localization of sound was
monitored by recording its head position with a special device -- a simple
detector mounted on the head recorded induced electric currents produced by a
pair of electric coils mounted around the animal's head. The speaker was
mounted on a track and its position was controlled by a computer.&quot;</p>

<p class=MsoNormal>Owls' ears are not symmetrical - the left one points down
and the right one points up. &quot;A partial block of the left ear causes the
animal to miss the speaker location by orienting the head just a little to the
right and up from the actual location&quot;, and vice versa. These errors were
greater in elevation than in azimuth. Based on this result, and the anatomy of
the two ears, it seems that the owl uses IID.</p>

<p class=MsoNormal>By feeding different signals into the two ears, Moiseff and
Konishi showed that the owls were calculating the OTD for a particular location
in the acoustic world since the animal looked in the expected direction.</p>

<p class=MsoNormal>&quot;Intensity differences (elevation) via IID in Nucleus
Angularis (to the ventral nucleus of the lateral lemniscus, VLVp) and time
differences (Azimuth) via ITD in Nucleus Magnocellularis (to the Nucleus
Laminaris&quot;</p>

<p class=MsoNormal>&quot;Takahashi, Moiseff and Konishi [in 1984] used a local
anesthetic to show that binaural intensity differences giving rise to
space-specific cells in the MLD require the normal activity of the nucleus
Magnocellularis. The processing of OTD requires the nucleus Angularis. Manley,
Koppl, Carr and Konishi [in 1988]) showed that IID from L/R Nucleus Angularis
is processed in the VLVp region of the Nucleus of the Lateralis Lemniscus while
OTD from L/R Nucleus Magnocellularis is processed in the Nucleus
Laminaris&quot;</p>

<h2>Notes � web, �A brain map in auditory space�</h2>

<p class=MsoNormal>Konishi &amp; Knudsen�s (1977) study with owls� sound
localisation abilities was an attempt to answer the question, �Why do we have
two ears?�, given that one is more or less sufficient for identifying sounds.
Most hearing creatures can identify where sounds are coming from using auditory
cues alone, but owls (especially barn owls) so excel at the task that they can
hunt in the dark. </p>

<p class=MsoNormal>They used microelectrode recording while a remote-controlled
speaker was moved around the owl's head, imitating sounds the owl would hear in
the wild. They identified an area in the midbrain, where ten thousand
'space-specific' neurons would fire only when sounds were presented in
particular location. These cells are topographically arrayed, such that
aggregates of space-specific neurons pinpointed precise vertical and horizontal
coordinates of the speaker, firing only when a tone was played at that
location.</p>

<p class=MsoNormal>&quot;Since sound is a mechanical pressure disturbance in
the medium, it is inherently non-directional -- that is, PRESSURE is a Scalar
(magnitude) quantity rather than a VECTOR quantity. So, a simple pressure
detector (such as the moth ear) is unable to determine the direction of the
sound source. &quot;Clever design&quot; however, can result in directional
sensitivity. One solution is to use two ears and monitor interaural differences
in the arriving sound at the two detectors (we do this).</p>

<p class=MsoNormal>For example, it is possible to determine a sound's source
direction by comparing Interaural Intensity Differences (IID) if the structures
separating the two ears shadow the sound sufficiently (or if the distance is
large enough for there to be differential attenuation). This effect is
wavelength dependent (high frequency sound with shorter wavelengths are
attenuated more and are shadowed more easily by the head.</p>

<p class=MsoNormal>In addition, if the ears are far enough apart, then the time
of arrival of the sound at the two ears will differ and Interaural Time
Differences (ITD) can be used (either Ongoing Time Disparities, OTD, or sound
Onset Disparities, OD). The owl uses both IID and ITD (so do we).&quot;</p>

<h2>Notes � web, �Biology 480: notes on Sci Am article�</h2>

<p class=MsoNormal>&quot;A sound originating from directly in front of your
head has an equal intensity in each ear and arrives at the same time at each
ear.<span style="mso-spacerun: yes">� </span>However if the sound is displace
to the left or right, both the intensity and the time of arrival will be
different at each ear. Consider the following experiment: a subject is asked to
point to the approximate location in space of a sound delivered through
earphones.<span style="mso-spacerun: yes">� </span>When the sound is louder in
the left earphone and arrives earlier than the sound in the right earphone the
subject perceives that the sound is to the left of the midline</p>

<p class=MsoNormal>the location of the owl's head was measured using the search
coil technique - when the head is moved, it causes a variation of the flow of
current proportional to both the horizontal and vertical movement of the head.</p>

<p class=MsoNormal>They found that altering the timing between two sounds
caused the owl to move his head in the horizontal plane, and that altering the
intensity between sounds in the two ears caused the owl to move it's head in
the vertical plane</p>

<p class=MsoNormal>The owl's midbrain contains a map of sound location. neurons
located near the midline of the nucleus sensitive to sounds the left of the
animal's head and neurons located near the lateral edge of the nucleus
sensitive to sounds in front of the animal's head. The neurons' RFs overlap,
providing a continual representation.</p>

<p class=MsoNormal>Cells in the auditory nerve that project to the cochlear
nuclei are sensitive to both the frequency and intensity of sound.<span
style="mso-spacerun: yes">� </span>Each cell responds best to a limited
frequency range and as an ensemble, cells are sensitive to a broad range of
sound frequencies. </p>

<p class=MsoNormal>Each cell in the auditory nerve also exhibits &quot;phase
locking&quot; to a certain part of the sound wave.<span style="mso-spacerun:
yes">� </span>A particular cell will fire an action potential at almost exactly
the same point in the sound wave for each successive cycle of the sound
wave.<span style="mso-spacerun: yes">� </span>This response pattern helps to
mark the time of arrival of the sound at each ear</p>

<p class=MsoNormal>Each cell in the auditory nerve projects to two distinct brain
nuclei: the nucleus magnocellularis and the nucleus angularis.<span
style="mso-spacerun: yes">� </span>Here the information about timing and
intensity splits into two pathways: the timing pathway includes the nucleus
magnocellularis and n. laminaris, and the intensity pathway includes the n.angularis
and lateral lemniscal nuclei</p>

<p class=MsoNormal>Cells in the n. magnocellularis also exhibit phase locking
to stimuli, however they are insensitive to the intensity of the stimulus.<span
style="mso-spacerun: yes">� </span>In contrast, cells in the n. angularis do
not phase lock, but change their firing frequency according to the intensity of
the stimulus</p>

<p class=MsoNormal>Cells from the n. magnocellularis project to the n.
laminaris.<span style="mso-spacerun: yes">� </span>It is here that the
computation of time differences takes place.<span style="mso-spacerun: yes">�
</span>The n. laminaris is a bilateral structure, and each nucleus receives input
from each ear.<span style="mso-spacerun: yes">� </span>The n. laminaris is the
first place where inputs from the two ears are combined</p>

<p class=MsoNormal>In 1948, Jeffress proposed a model by which timing
differences could be computed by the auditory system.<span style="mso-spacerun:
yes">� </span>He suggested that signals from each ear would vary in how rapidly
they traveled to the same location in the brain.<span style="mso-spacerun:
yes">� </span>He proposed that a specific class of neurons (called coincidence
detectors) would only fire when inputs from the two ears arrived simultaneously</p>

<p class=MsoNormal>For example: imagine a sound source located close to the
left ear.<span style="mso-spacerun: yes">� </span>The sound signals would reach
the left ear sooner than the right.<span style="mso-spacerun: yes">�
</span>Thus a coincidence detector would fire only when the two signals arrive
simultaneously.<span style="mso-spacerun: yes">� </span>Thus the signals coming
from the left ear need to be delayed in time to allow the signals from the
right ear to catch up.<span style="mso-spacerun: yes">� </span>Catherine Carr
did an anatomical analysis of this circuit and demonstrated the anatomical
basis for the delay of signals within the auditory circuit. </p>

<p class=MsoNormal>Each coincidence detector is sensitive to a specific time difference.<span
style="mso-spacerun: yes">� </span>Some will be stimulated when the sound is
directly in front of the animal, whereas another coincidence detector will be
stimulated when the sound is closer to the right ear</p>

<h2>Notes � web, �Birder�s world, Those amazing birds: seeing with ears�</h2>

<p class=MsoNormal>�The facial disk, characteristic of all owls, is best
developed in the Barn Owl. It consists of several layers of densely packed,
stiff feathers located in circular rings. Each feather has a wide shaft and
short barbs, and bends forward near the tip. In addition, two prominent grooves
in the facial disk run from either side of the mouth up past the ear openings.
These grooves are not unlike the grooves in our outer ear, which help funnel
sounds into the ear. An essential feature of the facial- disk grooves is that
the feathers lining them are designed to reflect high frequency sound waves
into the ear. Consequently, the grooves in the facial disk selectively
concentrate and direct to the ears such sounds as the squeaks of mice and
rustle of leaves.�</p>

<h2>Questions</h2>

<p class=MsoNormal>how can we tell elevation???</p>

<p class=MsoNormal>how does having the two ears pointing up and down help the
owl???</p>

<p class=MsoNormal>how can we tell front and back???</p>

<p class=MsoNormal>are the spatial maps in the higher levels of the auditory
cortex head-centred???</p>

<h3>Kandel and Schwarz</h3>

<p class=MsoNormal>what�s the pinna � the outside bit???</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>�the largely cartilaginous
projecting portion of the external ear�</p>

<p class=MsoNormal>�the scala vestibuli and scala tympani communicate with each
other at the helicotrema where the scala media ends, so that perilymph is
continuous�???</p>

<p class=MsoNormal>endolymph/perilymph</p>

<p class=MsoNormal>�Thus, the cochlear compartments convert the differential
pressure between the scala vestibuli and scala tympani into oscillating
movements of the basilar membrane that excite and inhibit the sensory
transducing cells in the organ of Corti�???</p>

<p class=MsoNormal>apical � related to or situated at an apex</p>

<p class=MsoNormal>�parallel arrays of cross-bridge actin filaments�???</p>

<p class=MsoNormal>how quickly can the outer hair cells change their own
length???</p>

<p class=MsoNormal>what does �monaural disability� mean???</p>

<p class=MsoNormal>would it be fair to say that the auditory system does a lot
more pre-cortical processing???</p>

<p class=MsoNormal>�pial surface�???</p>

<p class=MsoNormal>binaural interaction columns</p>

<h3>Kaas &amp; Hackett</h3>

<p class=MsoNormal>is there evidence for a what/where division in the
somatosensory system???</p>

<p class=MsoNormal>does it make sense to talk of faster processing by having 3
parallel primary areas, when simply lumping them together would not slow things
down in a connectionist system, would it???</p>

<h3>Moore &amp; King</h3>

<p class=MsoNormal>just how accurate is human sound localisation??? what�s its
adaptive value??? how far does it extend � reaching distance only???</p>

<p class=MsoNormal>with the 10, 30 and 50cm distance trials, did they
increase/decrease the stimulus level so that the only variable in the cue was
distance???</p>

<p class=MsoNormal style='margin-top:4.0pt;margin-right:0in;margin-bottom:0in;
margin-left:17.85pt;margin-bottom:.0001pt'>it sounds as though they didn�t, but
they had a control of increasing level but keeping the same distance, I think</p>

<p class=MsoNormal>where did they measure the centre of the azimuth from??? did
they find complementary neurons that preferred distant stimuli??? </p>

<p class=MsoNormal>ratio of direct-to-reverberant sound energy???</p>

<p class=MsoNormal>temporal integration window??? duration 6ms that agrees
closely with other estimates of auditory temporal processing???</p>

<p class=MsoNormal>the 'acoustically-responsive neurons showed some tuning for
sound azimuth'???</p>

<h3>web, �Locating a mouse by its sound�</h3>

<p class=MsoNormal>ah, so unlike in the PPC, there are actually �space� cells
with different cells representing different head-centred coordinates??? is that
any different to a head-centred frame???</p>

<p class=MsoNormal>MLD???</p>

<p class=MsoNormal>why should timing indicate azimuth alone, and intensity
elevation (in the owl)???</p>

<h3>web, �A brain map of auditory space�</h3>

<h3>web, �Biology 480: notes on Sci Am article�</h3>

<h3>web, �Birder�s world, Those amazing birds: seeing with ears�</h3>

<p class=MsoNormal><![if !supportEmptyParas]>&nbsp;<![endif]><o:p></o:p></p>

<p class=MsoNormal><![if !supportEmptyParas]>&nbsp;<![endif]><o:p></o:p></p>

<p class=MsoNormal><![if !supportEmptyParas]>&nbsp;<![endif]><o:p></o:p></p>

</div>

</body>

</html>
