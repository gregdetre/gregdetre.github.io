<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<html>
  <head>
    <title>posterior parietal cortex</title>
    <meta name="generator" content="muse.el">
    <meta http-equiv="Content-Type"
          content="text/html; charset=iso-8859-1">
    
    
<link rel="stylesheet" type="text/css" charset="utf-8" media="all" href="common.css" />
<link rel="stylesheet" type="text/css" charset="utf-8" media="screen" href="screen.css" />
<link rel="stylesheet" type="text/css" charset="utf-8" media="print" href="~print.css" />

  </head>
  <body>

    <!-- Page published by Emacs Muse begins here -->
<h2>Posterior <a href="parietal%20cortex.html">parietal cortex</a></h2>

<h3>Part of</h3>

<p><a href="parietal%20cortex.html">parietal cortex</a></p>

<p>end of the <a href="dorsal%20pathway.html">dorsal pathway</a></p>



<h3>Sends to</h3>

<p><a href="lower%20superior%20colliculus.html">lower superior colliculus</a></p>



<h3>Function</h3>

<p class="first">representation of space</p>

<p>movement planning</p>



<h3>Clinical</h3>

<p><a href="simultanagnosia.html">simultanagnosia</a> - which one???</p>

<p><a href="optic%20ataxia.html">optic ataxia</a></p>

<blockquote>
<p class="quoted">can't use visuospatial information to guide arm movements</p>
</blockquote>

<p><a href="hemispatial%20neglect.html">hemispatial neglect</a></p>

<p><a href="akinetopsia.html">akinetopsia</a></p>

<p><a href="apraxia.html">apraxia</a></p>



<h3>Reference frames</h3>

<p class="first">head-centerd - combines information about eye position and
the location of a visual stimulus on the <a href="retina.html">retina</a></p>

<p>body-centerd - combines information about head, eye and
retinal position</p>

<p>world-centerd - combines vestibular signals with eye
poistion and retinal position</p>



<h3>Stein (1992)</h3>

<p class="first">Stein (1992) claimed that these are two characteristics of
all posterior parietal neurons:</p>

<p>1.  combinations of sensory, motivational and motor
information are received</p>

<p>2.  their response is greatest when the animal attends to,
or moves towards, a target</p>

<p>All of them respond to movements of the eyes and to the
position of the eye in its socket (some are most responsive
to behaviourally relevant stimulus, e.g. a <a href="reward.html">reward</a>). Some are
barely activated by stationary visual stimuli but respond
strongly when <a href="attention.html">attention</a> is directed or eye/arm movement made
towards stimulus. Some respond to manipulation of the
object, or its structural features.</p>



<h3>Notes - Andersen (1997), Multimodal representation of space in the <a href="posterior%20parietal%20cortex.html">posterior parietal cortex</a> and its use in planning movements</h3>

<p class="first">According to Andersen, the parietal lobe contains an
abstract multi-modal distributed representation of space,
combining <a href="vision.html">vision</a>, somatosensation, audition, and vestibular
sensation, which 'can then be used to construct multiple
frames of reference to be used by motor structures to code
appropriate movements', as well as selecting stimuli and
helping to plan movements. Efference copies of motor
commands, probably generated in the frontal lobes, also
converge on the <a href="posterior%20parietal%20cortex.html">posterior parietal cortex</a> and provide
information about body movements (all coded in different
coordinate frames).</p>

<h4>Posterior parietal</h4>

<p class="first">Andersen singles out various areas within the posterior
<a href="parietal%20cortex.html">parietal cortex</a> as being extensively investigated and of
particular interest: area 7a, the <a href="lateral%20intraparietal.html">lateral intraparietal</a> area
(LIP), the medial superior temporal area (<a href="MST.html">MST</a>), area 7b, and
the <a href="ventral%20intraparietal.html">ventral intraparietal</a> area (VIP).</p>

<p>Andersen et al (1992) speculate that LIP is the 'parietal
eye field', 'specialised for visual-motor transformation
functions related to <a href="saccades.html">saccades</a>', on the basis of the strong
direct projections from extrastriate visual areas and
projections to various cortical and subcortical areas
concerned with saccadic <a href="eye%20movements.html">eye movements</a>, and results from
electrical stimulation.</p>

<p><a href="MST.html">MST</a> (subdivided into MSTd (dorsal) and MSTl (lateral)) seems
highly involved in <a href="motion.html">motion</a> processing. MSTd may be involved
in navigation using motor cues, since many of the patterns
of <a href="motion.html">motion</a> MSTd neurons are selective for occur with
self-<a href="motion.html">motion</a> (e.g. expansion, contraction, rotation and
spiraling), as well as having large <a href="receptive%20fields.html">receptive fields</a> and
receiving signals related to <a href="smooth%20pursuit.html">smooth pursuit</a> <a href="eye%20movements.html">eye movements</a>
and vestibularly derived head pursuit signals.</p>

<p>Area 7a has large bilateral fields, with strong cortical
connections to other visual areas, as well as 'areas of the
cortex associated with the highest cognitive functions,
including the parahippocampal gyrus and <a href="cingulate%20cortex.html">cingulate cortex</a>'.</p>

<p>Area 7b and VIP are closely tied in with the <a href="somatosensory.html">somatosensory</a>
system, and to a lesser extent, <a href="vision.html">vision</a>.</p>

<p>All of the areas are strongly interconnected via
corticortical projections. Thus even seemingly unimodal
visual areas like LIP and <a href="MST.html">MST</a> 'can reveal their multimodal
nature when probed with the right set of tasks'.</p>


<h4>Multimodal representation of space</h4>

<h5>Eye position and visual signals</h5>

<p>Andersen claims that areas 7a and LIP use their eye position
and retinal input signals to represent the location of a
visual target with respect to the head, a 'head-centerd
reference frame'. He concedes that 'intuitively one would
imagine that an area representing space in a head-centerd
reference frame would have <a href="receptive%20fields.html">receptive fields</a> that are
anchored in space with respect to the head', but proposes
instead that instead a highly distributed pattern is used to
uniquely specify each head-centerd location in the activity
across a population of cells with different eye position and
retinal position sensitivities. Indeed, he argues that 'when
neural networks are trained to transform retinal signals
into head-centerd coordinates by using eye position signals,
the middle-layer units that make the transformation gain
fields similar to the cells in the <a href="parietal%20cortex.html">parietal cortex</a> (Zipser &amp;
Andersen, 1988)'.</p>


<h5>Head position</h5>

<p>A body-centerd reference frame combines information about
where the retinal position and head orientation. About half
of all 7a and LIP cells that have eye gain fields also have
head gain fields. He argues that these cells were
generalising for a body-centric gaze direction(???).</p>

<p>There are at least three sources for information about head
position: the motor signals to move the head (an 'efference
copy'), a vestibular signal and neck proprioceptive signals.</p>

<p>Snyder et al (1993) showed that supplying solely vestibular
signals (in the dark to isolate from visual input), or
solely proprioceptive cues (rotating the trunk while keeping
the head fixed), both elicit responses from these cells in
7a and LIP, indicating that both of these sources are used
in constructing the body-centerd frame. Furthermore, input
from the vestibular and visual systems (e.g. landmarks and
<a href="optic%20flow.html">optic flow</a>) can contribute to a world-centerd
representation.</p>


<h5>Auditory signals</h5>

<p>Binding visual and auditory signals is difficult, yet seems
to come so naturally. It requires a representation abstract
enough to combine 2-D retinal inputs (eye-centerd), with
computations of intra-aural time, intra-aural intensity and
spectral cues from both ears (head-centerd).</p>

<p>This problem is related to the ease with which we are able
to saccade to auditory and tactile stimuli and verbal
commands, as well as visual stimuli. It requires there to be
somewhere where these modalities can be integrated and
processed before the oculomotor area can send signals to the
<a href="extraocular%20muscles.html">extraocular muscles</a> around the eyes.</p>

<blockquote>
<p class="quoted">&quot;Mazzoni et al (1996) recently demonstrated that when a
monkey is required to memorize the location of an auditory
target in the dark and then to make a saccade to it after
a delay, there is activity in LIP during the presentation
of the auditory target and during the delay period. This
auditory response generally had the same directional
preference as the visual response, suggesting that the
auditory and visual <a href="receptive%20fields.html">receptive fields</a> and <a href="memory.html">memory</a> fields may
overlap one another.&quot;</p>
</blockquote>

<blockquote>
<p class="quoted">&quot;The above experiments were done when the animal was
fixating straight ahead, with its head also oriented in
the same direction. Under these conditions, the eye and
head coordinate frames overlap. However, if the animal
changes the orbital position of its eyes, then the two
coordinate frames move apart. Do the auditory and visual
<a href="receptive%20fields.html">receptive fields</a> in LIP move apart when the eyes move, or
do they share a common spatial coordinate frame?&quot;</p>
</blockquote>

<p>Stricanne et al (1996) showed that almost half of the
auditory-responding cells in LIP coded the auditory location
in eye-centerd coordinates, like in the <a href="superior%20colliculus.html">superior colliculus</a>,
where auditory fields are also in eye-centerd coordinates
(Jay &amp; Sparks, 1984).</p>

<p>A third were coded in head-centerd coordinates, and a
quarter were intermediate between the two coordinate
frames. Cells of all three types also had gain fields for
the eye. He argues that 'at least this subpopulation shares
a common, distributed representation with the visual signals
in LIP'(???).</p>


<h5>Visual <a href="motion.html">motion</a> and pursuit</h5>

<p><a href="MST.html">MST</a> may be solving an important spatial problem, that of
'computing the direction of self-<a href="motion.html">motion</a> in the world based
on the changing retinal image'.</p>

<p>Visual navigation = finding one's heading based on visual
information, e.g. using the center of expanding visual
<a href="motion.html">motion</a> generated by self-<a href="motion.html">motion</a> as the direction of heading
(Gibson, 1950). Interestingly, we can recover the direction
of heading even when we are fixating/tracking an object that
is not directly ahead of us. The resulting <a href="optic%20flow.html">optic flow</a> field
needs to be decomposed into a) the movement of the observer
(expanding field) and b) eye rotation (linearly moving
field). It seems (Royden et al, 1992) that an efference copy
of the pursuit command may be very helpful in recovering the
direction of heading.</p>

<p>Handily, MSTd contains cells selective for one or more of
the following: expansion-contraction, rotation and linear
<a href="motion.html">motion</a> (Saito, 1986). However, it appears that MSTd is not
decomposing the <a href="optic%20flow.html">optic flow</a> into channels of expansion,
rotation and linear <a href="motion.html">motion</a> - Andersen produced a spiral
space with expansion on one axis and rotation on another,
and found that disappointingly few of the MSTd neurons had
tuning curves aligned directly along these
axes. Interestingly though, the MSTd neurons displayed a
high degree of position and size invariance, as well as
form/cue invariance. The MSTd cells seems to convey 'the
abstract quality of a pattern of <a href="motion.html">motion</a>, e.g. rotation',
which may be important in analysing <a href="optic%20flow.html">optic flow</a> by gathering
information from any part of the visual field. <a href="MST.html">MST</a> may use
cells sensitive to <a href="motion.html">motion</a> pattern in combination with the
pursuit eye movement signal it receives to code direction of
heading. They found that many MSTd neurons shift their
<a href="receptive%20fields.html">receptive fields</a> during eye pursuit movements to more
faithfully code the direction of heading than the focus of
expansion on the <a href="retina.html">retina</a>. For instance, when viewing an
expanding pattern while making a pursuit movement towards,
say, the left, the retinal position of the focus shifts
left, which many expansion-selective MSTd neurons compensate
for by shifting their <a href="receptive%20fields.html">receptive fields</a> to the left (and
often vice versa for rightward movements).</p>

<blockquote>
<p class="quoted">&quot;When the eyes move, the focus tuning curve of these cells
shifts in order to compensate for the retinal focus shift
due to the eye movement. In this way MSTd could map out
the relationship between the expansion focus and heading
with relatively few neurons, each adjusting its focus
preference according to the velocity of the eye.&quot;</p>
</blockquote>

<p>Perrone &amp; Stone's (1994) and Warren's (1995) similar models
require more neurons for separate heading maps for different
combinations of eye direction and speed (rather than just
eye movement)(???).</p>

<p>This pursuit compensation is achieved by a non-uniform gain
and distortion applied to different locations in the
receptive field. He details 2 methods by which this might be
accomplished.</p>

<p>Experiments have yet to determine in which coordinate frame
the direction of heading is coded. If it's eye-centerd, then
eye and head gain fields could map this direction of heading
signal to other coordinate frames for appropriate motor
behaviours such as walking or driving.</p>

<p>MSTd may also be more generally used in providing perceptual
stability during tracking movements. Rotation cells' focus
of rotation is also displaced during pursuit <a href="eye%20movements.html">eye movements</a>,
only orthogonal to eye movement direction (rather than in
the same direction, like expansion) - and indeed the focus
tuning (of the <a href="receptive%20fields.html">receptive fields</a>???) of rotation cells in
MSTd shifted orthogonal to the direction of pursuit.</p>

<p>MSTd may compensate spatially for the consequences of eye
movements for all patterns of <a href="motion.html">motion</a>.</p>



<h4>Models of coordinate transformation</h4>

<p class="first">Neural network models can illustrate methods employing gain
fields to transform between coordinate frames.</p>

<h5>Transformations between coordinate frames</h5>

<p>Mentions Zipser &amp; Andersen (1988) again, which showed that
when 'retinal position signals are converted to a map of the
visual field in head-centerd coordinates, the hidden units
that perform this transformation develop gain fields very
similar to those demonstrated in the posterior parietal
cortex', and that the activities found for posterior
parietal neurons could be the basis of a distributed
representation of head-centerd space.</p>


<h5>Converting auditory and visual signals to oculomotor coordinates</h5>

<p>In Xing et al's (1995) model, which takes in head-centerd
auditory signals and eye position and retinal position
signals as input, and whose output codes the metrics of a
planned movement in motor coordinates, the middle layers
develop overlapping <a href="receptive%20fields.html">receptive fields</a> for auditory and visual
stimuli and eye position gain fields. It is interesting that
the visual signals also develop gain fields, since both the
retinally based stimuli and the motor error signals are
always aligned when training the network and, in principle,
do not need to use eye position information. However, the
auditory and visual signals share the same circuitry and
distributed representation, which results in gain fields for
the visual signals.</p>


<h5>Multiple coordinate frames in <a href="parietal%20cortex.html">parietal cortex</a></h5>

<p>By using the gain field mechanism, a variety of modalities
in different coordinate frames can be integrated into a
distributed representation of space.</p>

<p>In this way, information is not collapsed and lost - for
instance, if the gain field mechanism is used to produce a
head-centerd frame from retinal position and eye position,
the eye-centerd coordinates could be read out by another
structured - the two components have not been converged -
it's almost like shifting all the information in a
spreadsheet one column along.</p>

<p>Lesions to the <a href="posterior%20parietal%20cortex.html">posterior parietal cortex</a> give rise to
spatial deficits in multiple coordinate frames. This could
be because many coordinate frames might conceivably
representable in the same population of neurons. Or it could
simply be that the different coordinate frames exist in
close proximity to one another and so would all be affected
at the same time.</p>


<h5>Converting retinotopic signals to oculocentric coordinates</h5>

<p>No coordinate transformation is necessary for a simple
visual saccade. However, there are occasionally times when
the oculomotor ocordinates are in a different frame from
sensory-retinal coordinates (e.g. displacement of the eye
from electrical stimulation or an intervening saccade), yet
the cells in the PPC, <a href="frontal%20eye%20fields.html">frontal eye fields</a> and superior
colliculus are still able to code the impending movement
vector, even though no visual stimulus has appeared in their
<a href="receptive%20fields.html">receptive fields</a>. Krommenhoek et al's (1993) and Xing et
al's (1995) networks were able to replicate this result,
both developing eye gain fields in the hidden layer. The
Xing et al neural network was trained on a double-saccade
task; it inputted two retinal locations and then outputted
the motor vectors of two <a href="eye%20movements.html">eye movements</a>, first to one target
and then to the other. In order to program the second
saccade accurately, the network was required to use the
remembered retinal location of the first target and update
it with the new eye position. This implies that an implicit
distributed representation of head-centerd location was
formed in the hidden layer.</p>


<h5>Algorithms for gain fields</h5>

<p>Multiplicative, additive and ceiling effects (which you can
see in NNs in terms of where on the sigmoidal activation
function the summed inputs lie) have all been observed in
the recording data.</p>


<h5>Cognitive intermediates in the sensory-motor transformation process</h5>

<p>The PPC also contains circuitries that appear to be
important for shifting <a href="attention.html">attention</a>, stimulus selection and
movement planning.</p>


<h5>Attention</h5>

<p>Patients with lesions to the PPC have difficulty shifting
their focus of <a href="attention.html">attention</a> (Posner et al, 1984). It now seems
that visual responsiveness of parietal neurons is actually
reduced at the focus of <a href="attention.html">attention</a> (Robinson et al, 1995),
while locations away from the focus of <a href="attention.html">attention</a> are more
responsive, apparently signaling novel events for the
shifting of <a href="attention.html">attention</a>.</p>


<h5>Intention</h5>

<p>Gnadt &amp; Andersen (1988) have shown that activity in cells
primarily in LIP (coding in oculomotor coordinates) precedes
<a href="saccades.html">saccades</a>. This activity is also <a href="memory.html">memory</a>-related,
e.g. lighting up when a monkey is remembering the location
of a briefly-flashed stimulus and, after a delay, made a
saccade to the remembered location. Glimcher &amp; Platt
required an animal to attend to a distractor target, which
was extinguished as a cue to saccade to the selected target,
thus separating the focus of <a href="attention.html">attention</a> from the selected
movement. For many of the cells, the activity reflected the
movement plan and not the attended location, although the
activity of some cells was influenced by the attended
location. Andersen thinks that these and other studies
suggest that a component of LIP activity is related to
movements that the animal intends to make.</p>

<p>Mazzoni et al (1996) used a delayed double-saccade
experiment to try and distinguish whether the <a href="memory.html">memory</a>
activity was primarily related to intentions to make eye
movements or to a sensory <a href="memory.html">memory</a> of the location of the
target. They found both types of cells, with the majority of
overall activity being related to the next intended saccade
and not to the remembered stimulus location. This did not
necessarily lead to execution of the movement, since the
animals could be asked to change their planned <a href="eye%20movements.html">eye movements</a>
during the delay period in a <a href="memory.html">memory</a> saccade task, and the
intended movement activity in LIP would change
correspondingly (Bracewell et al, 1996).</p>

<p>If it could be shown that the activity is related to the
type of movement being planned, it would be a strong
indication that the activity is intention-related.</p>

<blockquote>
<p class="quoted">&quot;Bushnell et al (1981) recorded from PPC neurons while the
animal programmed an eye or <a href="reaching.html">reaching</a> movement to a
retinotopically identical stimulus. They claimed that the
activity of the cells did not differentiate between these
two types of movements, indicating that the PPC is
concerned with sensory location and <a href="attention.html">attention</a> and not with
planning movements.&quot;</p>
</blockquote>

<p>However, when Andersen et al repeated the experiment, they
found that 2/3 of cells in the PPC were selective during the
<a href="memory.html">memory</a> period for whether the target requires an arm or eye
movement.</p>


<h5>Intention activity occurs when a monkey considers a movement</h5>

<p>They also added a control experiment, which involved both an
arm and eye movement (sometimes in opposite directions, and
so sometimes one within and one without the receptive
field), then they were able to conclude that plans for both
movements were represented by subpopulations of cells in the
PPC, even if only one movement would eventually be made(???
pg 24).</p>

<p>Andersen considers Duhamel et al, 1992 (similar to Gnadt &amp;
Andersen, 1988) and Kalaska &amp; Crammond, 1995 as studies in
which their theory that the <a href="memory.html">memory</a>-related activity in the
PPC signals the animal's plan to make a movement could
explain the results.</p>

<p>Thus, when stimulus-related activity comes into the parietal
cortex, it can sometimes invoke more than one potential
plan, e.g. both eye and limb movements, even if the limb
movement is not executed.</p>



<h4>Summary and conclusions</h4>

<p class="first">This coding of signals in the coordinates of movement is
consistent with the recent proposal of Goodale &amp; Milner
(1992) that <a href="posterior%20parietal%20cortex.html">posterior parietal cortex</a> is an action system
specifying how actions can be accomplished.</p>




<h3>Misc</h3>

<p class="first">viewer-centerd system - the system that identifies the
location, local orientation and <a href="motion.html">motion</a> of an object relative
to the viewer</p>

<p>many visual areas in the posterior parietal region</p>

<p>and also multiple projections to motor systems for eyes +
limbs</p>

<p>connections to prefrontal: has a role in the STM of location
of events in space</p>

<blockquote>
<p class="quoted">monkey neurons: activity dependent on concurrent behaviour
of the animal with respect to visual stimulation</p>
</blockquote>

<p>response of posterior parietal neurons during sensory input
and during movement</p>

<blockquote>
<p class="quoted">all respond to movements of the eyes and to the position
of the eye in its socket (some are most responsive to
behaviourally relevant stimulus, e.g. <a href="reward.html">reward</a>)</p>
</blockquote>

<blockquote>
<p class="quoted">some are barely activated by stationary visual stimuli but
respond strongly when <a href="attention.html">attention</a> is directed or eye/arm
movement made towards stimulus</p>
</blockquote>

<blockquote>
<p class="quoted">some respond to manipulation of the object, or its
structural features</p>
</blockquote>

<p>Stein (1992) - 2 characteristics of all posterior parietal
neurons:</p>

<ol>
<li>combinations of sensory, motivational and motor
information are received</li>

<li>response is greatest when the animal attends to, or
moves towards, a target

<blockquote>
<p class="quoted">we might then expect posterior parietal neurons to be
transforming sensory information into commands for
directing <a href="attention.html">attention</a> and guiding motor outputs</p>
</blockquote></li>
</ol>

<p>human posterior parietal lesions:</p>

<blockquote>
<p class="quoted">impaired in distinguishing left from right</p>
</blockquote>

<blockquote>
<p class="quoted">impaired mental manipulations of objects</p>
</blockquote>

<blockquote>
<p class="quoted">spatial deficits - perhaps due to damage to
temporal-parietal polysensory regions(Goodale &amp; Milner,
1993), rather than to the dorsal stream's role in
visuomotor guidance</p>
</blockquote>


<blockquote>
<p class="quoted">right hemisphere lesions (greater polysensory growth in</p>
</blockquote>

<blockquote>
<p class="quoted">the right hemisphere) =&gt; greater deficits on complex</p>
</blockquote>

<blockquote>
<p class="quoted">spatial tasks</p>
</blockquote>

<p>co-ordinate transformation for determining spatial location
and forming plans</p>

<blockquote>
<p class="quoted">Andersen - combine information from different modalities
to form 3 types of abstract representation:</p>
</blockquote>


<blockquote>
<p class="quoted">head-centerd - combines information about eye position +</p>
</blockquote>

<blockquote>
<p class="quoted">the location of a visual stimulus on the <a href="retina.html">retina</a></p>
</blockquote>


<blockquote>
<p class="quoted">body-centerd - combines information about head, eye and</p>
</blockquote>

<blockquote>
<p class="quoted">retinal position</p>
</blockquote>


<blockquote>
<p class="quoted">world-centerd - combines vestibular signals with eye</p>
</blockquote>

<blockquote>
<p class="quoted">poistion and retinal position</p>
</blockquote>




<!-- Page published by Emacs Muse ends here --> 

<hr>

<script language="javascript" src="footer.js">

  </body>
</html>
