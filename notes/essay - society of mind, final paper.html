<!DOCTYPE html>
<html lang="en">

<head>
    

    <title>

essay - society of mind, final paper -

Greg Detre
</title>
    <meta charset="utf-8" />
    <meta name="generator" content="gdwebgen" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <meta name="description" content="Greg Detre's personal website" />
    
    <link rel="stylesheet" href="/static/css/base.css">
    
<link rel="canonical" href="https://www.gregdetre.com/notes/essay - society of mind, final paper">

</head>

<body>

    
    <div id="wide-img-header">
        
        <a href="/wide_img_header/IMG_5253.jpg">
            <img src="/wide_img_header/IMG_5253_sm.jpg" />
        </a>
        
    </div>
    

    
<nav aria-label="Breadcrumb">
    <ol class="breadcrumb">
        
        
        <li class="breadcrumb-item" title="Greg Detre">
            
            <a href="/">Greg Detre</a>
            
        </li>
        
        
        
        <li class="breadcrumb-item" title="Notes">
            
            <a href="/notes/">...</a>
            
        </li>
        
        
        
        <li class="breadcrumb-item" title="essay - society of mind, final paper">
            
            <a href="/notes/essay - society of mind, final paper">Essay - Society Of Mind, Final Paper</a>
            
        </li>
        
        
    </ol>
</nav>


    
    <div class="nav_menu" <p>
        
        <a href="/about">About</a>
        
        <a href="/blog">Blog</a>
        
        </p>
    </div>
    

    <div id="main">
        

<h1>essay - society of mind, final paper</h1>




<h1 id="essay-society-of-mind-final-paper">Essay � Society of mind, final paper</h1>
<p>Greg Detre</p>
<p>Sunday, May 18, 2003</p>
<h2 id="introduction">Introduction</h2>
<h3 id="steves-intro">Steve�s intro</h3>
<p>[�]</p>
<p>The original way in which the C&amp;C framework was proposed implied a generality and lack of anthropomorphism which requires further argument. We are going to start by attempting to make a species-independent argument about intelligence as compression by decomposition. We bring in the M6 framework to show how a mind that doesn�t decompose the world into symbols must be limited to some variation of stimulus-response.</p>
<p>We take the argument further, and explore how the C&amp;C and M6 frameworks relate to each other, and the way that different situations are represented as different kinds of things, differences, causes and clauses at the various levels of the mind. This examination highlights the advantages, and the flaws of the C&amp;C framework.</p>
<p>Finally, we propose some amendments and extensions to the C&amp;C framework.</p>
<h3 id="what-can-we-say-with-confidence">What can we say with confidence?</h3>
<h5 id="intelligence-as-compression-by-decomposition">Intelligence as compression by decomposition</h5>
<p>We feel that we can argue with some confidence that any intelligence, no matter how alien, must </p>
<p>the compression is important because of the lookup-table problem � combinatorial explosion</p>
<p>decomposition into symbols is important because the world is structured and regular � the same symbols keep cropping up, and indeed predict the world well � admittedly, the symbols have probably been fixed a bit by language and the need to communicate</p>
<p>it�s worth saying that there is, of course, considerably more to intelligent behaviour than simply decomposing the world in a useful, predictive way � for starters, you want to decompose it in many different ways, according to need and to give flexibility � secondly, we have to recompose the symbols in different ways.</p>
<p>is there anything more to intelligence than decomposing and recomposing symbols??? not according to the AI people, right???</p>
<p>---</p>
<p>at the risk of adding to the canon of extant just-so stories, i am going to try and sketch an argument, resting on a minimum of assumptions, for why </p>
<p>the TDC framework should be arise in any thinking thing</p>
<p>where there�s life (process that maintains itself, transduces, reproduces) there�s scarcity</p>
<p>where�s there�s scarcity, there�s death</p>
<p>where there�s death, there�s selection (assuming that there is a source of variation)</p>
<p>where there�s selection, there�s pressure to exploit the world increasingly efficiently</p>
<p>doesn�t there also have to be heritance???</p>
<p>where there�s pressure to exploit the world increasingly efficiently, there will be an arms race � this will lead to increasingly close-fitting and complex models of the world</p>
<p>more complex models require longer and more memory to learn + remember � if you can represent them more compactly and generate all the data you need, then you can have a more complex model without the memory + learning-time costs</p>
<p>compression can be expressed in terms of the MDL � the more structure there is in the world, the more you benefit from really good, abstract, compact, generative models because you can really reduce the number of uninformative noisy exceptions that you need to capture the data</p>
<p>in other words, having good models helps you benefit, which means that you�re more likely to survive and propagate your genes, which means that those genes are more likely to proliferate, </p>
<p>can you not think of genes in non-genetic terms, but more as just something that gets passed to the next generation � could you think of memes as inherited???</p>
<p>more compact representations allow you to know more about the world � he�s arguing that the most compact representations are things, differences and causes � why though?</p>
<p>the most useful kind of knowledge that you can have is what to do in order to maximise reward</p>
<p>in order to do that, you need to have a sense of choice, or at least of alternatives � </p>
<p>I think we can see the first 3 levels of the model 6 as being a kind of obvious, obvious track for the intelligent arms race above</p>
<p>once you�ve got as far as level 3, you�ve got the ability to choose between alternatives</p>
<p>so then you can make use of knowledge to direct your actions� hmmm � this isn�t going anywhere</p>
<p>so, to backtrack: the most useful kind of knowledge that you can have is what to do in order to maximise reward</p>
<p>there are consequences you like � you need some sort of reward function then, right??? � these are goals???</p>
<p>anyway, causes are those things (or differences???) that are most richly generative of the consequences you like</p>
<p>as soon as you have a means of representing hypotheticals, you can compare between present, past + possible worlds</p>
<p>or to put it another way, as soon as you have a means of comparing� then what??? this doesn�t work � you need to put it the other way as above</p>
<p>as soon as you have a basic, first-order intentionality predictive model of others, it will get more complex</p>
<p>then, as before, you will try and break it into compositions because they�re better for storage (and we know that minds are structured, hence compressible and decomposable)</p>
<p>and what�s better for storage is better for generation</p>
<p>once you start to represent other people as decomposable, then what will you find to be the best way of decomposing them??? beliefs and desires??? their own things, differences + causes??? goals??? consequences??? things, differences and causes about the way they work</p>
<p>the thing is, I�ve justified TDC at some levels, but by no means at all � and indeed, the argument will become increasingly tenuous if I try to � the question is why TDC at <em>every</em> level??? you might be forgiven for thinking that it would be great for certain levels but not for others�</p>
<p>does level 3 (deliberative) have TDC??? of what kind??? see steve�s paper � it definitely has difference � and I suppose each complete hypothetical scenario is a thing � there�s a single cause, which is the best scenario which causes your behaviour � but I don�t think that there are component notions of cause within the different scenarios</p>
<h5 id="aunt-bertha">Aunt Bertha</h5>
<p>There is a knockdown argument, used by Ned Block to attack the validity of the Turing test as an infallible indicator of intelligence, that we initially worried might be applicable here. It might go something as follows: we might imagine an alien species with a very, very large head, that doesn�t do any learning whatsoever, but it is born with a huge list of rules for what to do in any of the situations it will ever encounter. We can see this as a huge instinctive-level only brain. Within the terms of the argument, such an alien would be intelligent, at least by any behavioural test we can devise, but would have no need to decompose the world into C&amp;C. This worried me briefly, but it needn�t, for two reasons.</p>
<p>Firstly, the combinatorial explosion involved in passing even a five-minute Turing test reliably and administered imaginatively would be altogether impossible to fit even in a brain the size of a planet � Block�s argument is intended to be an <em>a priori</em> one, and therefore not relevant to us, given that we�re only concerned with what is physically possible. No intelligent alien with just a massive lookup-table for a brain could actually exist.</p>
<p>Secondly, such an alien would never learn. If its environment was to change in even the slightest way from that for which it had evolved, its rules wouldn�t work. What if we were to try and add a second level, a learning level, which simply associated stimuli with responses, remembering them all and applying the ones that worked best? But it would be evident from the trial-and-error nature of such a being�s behaviour that it lacked true intelligence. No two situations will ever be identical � </p>
<p>need rules for judging similarity � rules are decompositions, focusing upon features</p>
<p>if we assume that the environment changes very slowly and doesn�t punish caution /failure � might a very long-lived such alien be able to manage by slowly learning and exploring the space of possible situations</p>
<p>Could we not add a third level? It becomes more difficult to imagine what such a level would look like if we refuse to allow decomposition. One way of seeing the deliberative level is as a predictive, feedforward model. Such a model could be trained using supervised learning over course of the alien�s lifetime, so that when faced with a new situation, it would try and generate the implications of a given action, in order to decide in advance whether to execute it. The question is whether it makes sense to talk of any sort of feedforward model that doesn�t decompose the situation into features of some description. The fact that we term the features �sub-symbolic� seems to make them somehow differentiable from real, symbolic features. But this is a mistake � all �sub-symbolic� means is that the features being detected are at a lower level or divide up the space in a different way from the features that we happen to be used to. Any sort of prediction requires figuring out what�s similar about the present case to the past case. Unless they�re identical, this means paying closer attention to some features at the expense of others. This is decomposition.</p>
<p>There were two reasons for setting up this staw man argument</p>
<p>show how powerful the idea of mental levels is � can�t do anything intelligent with just the first three levels</p>
<p>intelligence is decomposition � a straw man argument for the possibility of a lookup-table alien was to show from the opposite angle that you could not have intelligence without decomposition, having first made the positive argument for how decomposition necessarily comes about through evolution.</p>
<p>is there any way to differentiate between the kinds of sub-symbolic features that NNs detect, and the kinds of features we�re talking about??? and indeed, does it even make sense to talk of <em>sub</em> -symbolic � after all, that just means that they�re at a lower level than the features that we�re used to seeing � not that they aren�t in fact symbols in their own right, right???</p>
<p>so, do they have to be special or abstract features???</p>
<p>how do you represent actions without decomposition? a full-body posegraph, I suppose�??? :(</p>
<p>that�s not the question � the question is how you choose the shortlist of actions to run through the feedforward model</p>
<p>can you imagine the fourth level without decomposition???</p>
<p>could you imagine intelligence without the fourth/fifth levels and above???</p>
<p>this would need a static environment � the whole point is that if it changes, the rules don�t work, and so it doesn�t learn, which is the whole point of intelligence</p>
<h5 id="assumptions-required">Assumptions required</h5>
<p>scarcity � proved</p>
<p>evolution</p>
<p>natural selection � occurs whenever there�s scarcity</p>
<p>variation � assume this</p>
<p>inheritance � this is more problematic � either you can have creatures that don�t die, or you need some sort of inheritance mechanism � can you even assume reproduction</p>
<p>composition as following from an intellect arms race � proved</p>
<p>things/differences as the best way to decompose??? � not proved</p>
<p>need for consequences/goals in order to explain the need for causes???</p>
<p>causes as meta-analogies??? � take two pairs of situations (clauses), see the difference between them, </p>
<h3 id="steve-section">Steve section</h3>
<p>|  <em>Thing</em> |  <em>Difference</em> |  <em>Cause</em> |  <em>Clause</em> </p>
<p>---|---|---|---|---  </p>
<ol>
<li>
<p>Instinctive e.g. pulls hand away after touching a hot stove |  low-level sensory perception |  change in that perception i.e. the addition of heat/pain, right??? |    |     </p>
</li>
<li>
<p>Learned |    |    |    |     </p>
</li>
<li>
<p>Deliberative |    |    |    |     </p>
</li>
<li>
<p>Reflective |    |    |    |     </p>
</li>
<li>
<p>Self-reflective |    |    |    |     </p>
</li>
<li>
<p>Self-conscious  |    |    |    |     </p>
</li>
</ol>
<p>What we believe is a more profitable way think about these two ideas in combination is as a 6x4 grid, which allows all parts of the Causes and Clauses framework to exist within all levels of model six.</p>
<p>Let us imagine how the Causes and Clauses framework fit into each of the levels of model six.� Let us begin with the Instinctive level and take the example of a person who, by reflex, pulls their hand away after touching a hot stove.� We can identify the basic Thing to be low-level sensory perception, and the Difference to be a change in that perception.� In this case, the perception is that of the regular hand and the burning hand, and the Difference is between those two perceptions.� Here, the cause is roughly calculated by the body, as the hand is pulled away in the proper direction opposite from the source of pain.� While there may not be a detailed concept of what that cause was, the fact that the hand moves correctly to avoid the pain (rather than say, towards or along the stove), suggests that cause is known in some form.� The Clause in this domain can be thought of as the episodic memory of that reflex reaction that is saved in the nervous system.</p>
<p>Up on the level of Learned reactions, we find a different grouping of ideas under the Causes and Clauses framework can be applied.� Here, we can see Things as nouns and Differences as Verbs.� Causes can be thought of as the agent responsible for carrying out the action in verbs.� Clauses are the episodic memory of combining Things, Differences, and Causes.� This is probably the most natural level for thinking about the Causes and Clauses framework.� In this case, a clause like: �don�t touch the stove because it is hot� identifies the stove as a Thing, the difference between touching and not touching the stove as a Difference, and excessive heat as a Cause.</p>
<p>On the level of Deliberative thinking, it makes more sense to think about Things as transitions and Differences between transitions.� As before, Causes are the explanation for there being a Difference between Things.� We might think of the following Clause that illustrates these roles:� �When I bake a cake today, it is okay to touch the handle of the stove, but not the stove, because it is hot�.� In this case, the transitions concern such transitions as baking a cake (from not baked to baked), touching the handle (from not touched to touched), and touching the stove (from not touched to touched).� As these transitions are compared, their Differences become obvious (touching the handle versus touching the stove).� Here, the Cause implies a further transition which is not stated in the clause--that your hand will go from not painful to painful if you touch the stove.� This transition serves as an explanation for the difference between touching the stove and the handle.</p>
<p>As we move above the level of Deliberative thinking, we can begin to see a pattern where the Things one level up are the Clauses from the level below.� For Reflective thinking, a Thing can be thought of as a Clause from the Deliberative level, or a �deliberation�.� Differences are formed between these deliberations.� Causes are the rationale you have for coming up with those plans, and Clauses are the records of that process.� Here�s an example of a Clause in reflective thinking using this framework:� �When I planned to touch the handle of the stove rather than touch the stove itself, I still burned myself.�� I didn�t consider that the handle would be too hot, because it looked insulated�.� The deliberations are the plan to only touch the handle of the stove, as well as the consideration that the handle would be too hot.� The Cause in this case is the rationale that the handle looked insulated.</p>
<p>Self-reflective thinking follows the same pattern started below in Reflective thinking.� Its Things are �reflections�, its Differences are comparisons between those reflections, and its Causes are the rationalizations for those Differences.� An example Clause for this level would be: �How could I have failed to realize that the handle was going to burn me before touching it?� I must have been distracted.�</p>
<p>Finally, Self-conscious emotions are the crowning layer in this recursive process.� Here, Things are �self-reflections�, and differences are found by comparing the �self-reflections� that different people you know might place upon your actions.� An example clause might be: �If anyone else had seen me burn myself on the stove handle, they would have laughed at my ignorance.�</p>
<p>Now that we have outlined how these ideas might be more profitably connected, let us examine a few trends that this explanation appears to signal.� First of all,� one can see the chaining process suggested by the Causes and Clauses framework operate as one goes up the levels.� Because higher levels use the structures created on lower levels, an important feature of the framework is conserved in this combination.� Secondly, we notice that Clauses become more complicated sentences as one goes up.� Lastly we notice that this treatment demonstrates that Causes become less necessary as we move upwards through the levels. </p>
<h3 id="extensions-to-cc">Extensions to C&amp;C</h3>
<h5 id="reinforcement">Reinforcement</h5>
<p>We have tried to show how the C&amp;C framework might fit </p>
<p>We am going to argue that there is something missing from the C&amp;C, though it affects the way that it has been described all the same. The C&amp;C framework needs to take into account what could be variously termed �reward�, �value� or �salience�. We will argue that this explains the strange-seeming omission of goals from the C&amp;C framework, and how the idea of a goal drops out of the C&amp;C framework quite naturally if we have a notion of reward.</p>
<p>Reward (and punishment) are ultimately grounded in evolutionary adaptiveness. But it needs to be decomposed too, since we don�t know what will prove evolutionarily adaptive for every situation. The only way we can guess is by associating particular situations in the past with the reward received. Unfortunately, this is an enormously difficult problem, since rewards are not immediate, and because we need to know what about those situations elicited the reward.</p>
<p>�</p>
<p>add description/adjective to C&amp;C???</p>
<p>adjectives as <em>differences</em> between actualThings and prototypeThings, i.e. between what you�re seeing now and your abstract conception of them???</p>
<p>argue that TDC (things, differences, causes) are in some sense operations</p>
<p>argue that the notion of clause is underdetermined, nebulous and redundant</p>
<p>need for consequences/goals??? � show that causes + consequences are complements, both being a way of picking salient (i.e. rewarding) aspects of the situation out to focus on, causes being valuable/salient aspects before the action and consequences being valuable/salient aspects that happen after the action</p>
<p>goals and consequences are equivalent???</p>
<p>C&amp;C3 just-so story</p>


<br />
<br />
<br />
<hr />



        



        

<small>
    <h1>Belongs to these <a href="/tag">tags</a></h1>
    <ul>
        
        <li><a href="/tag/essay.html">Essay</a></li>
        
        <li><a href="/tag/intelligence-mind.html">Intelligence, mind</a></li>
        
        <li><a href="/tag/artificial-intelligence.html">Artificial Intelligence</a></li>
        
        <li><a href="/tag/cognitive-load.html">Cognitive load</a></li>
        
        <li><a href="/tag/philosophy.html">Philosophy</a></li>
        
    </ul>
</small>



        



        



    </div>
    <footer>
        <div class="bottom_nav">
            

            


<p>
    <i>
        Last updated: 2024-Oct-04
    </i>
</p>


        </div>
    </footer>
</body>

</html>